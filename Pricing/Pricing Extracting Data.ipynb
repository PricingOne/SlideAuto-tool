{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75e54828",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_path = 'C:\\\\Users\\\\Ali Salem\\\\Desktop\\\\App_Update\\\\static/files\\\\parameters.xlsx'\n",
    "slides_name = ['Price Positioning Summary Category', 'Price Positioning Summary Sector', 'Price Positioning Summary Segment', 'Price Positioning Summary SubSegment', 'Price Positioning Summary SubCategory', 'Price Positioning Analysis By Brand', 'Price Positioning Analysis By Manufacturer', 'Sector Share and Growth By Brands', 'Segment Share and Growth By Brands', 'SubSegment Share and Growth By Brands', 'SubCategory Share and Growth By Brands', 'Sector Share and Growth By Manufacturer', 'Segment Share and Growth By Manufacturer', 'SubSegment Share and Growth By Manufacturer', 'SubCategory Share and Growth By Manufacturer', 'Sec/Seg Value Sales Vs Avg Price By Manufacturer', 'Sector Sales Vs Avg Price', 'Segment Sales Vs Avg Price', 'SubSegment Sales Vs Avg Price', 'SubCategory Sales Vs Avg Price', 'Avg Price/Vol Comparison', 'Shelf Price/Vol Comparison', 'Price Point Distribution Analysis P3M', 'Price Point Distribution Analysis P12M', 'Price Point Distribution Analysis P3M ByManuf', 'Price Point Distribution Analysis P12M ByManuf', 'Price Point Distribution Scraped', 'Price Point Distribution Scraped ByManuf', 'Price Point Comparison Analysis by Manuf', 'Price Point Comparison Analysis', 'Price Point Comparison Analysis by Product Scraped', 'Price Point Comparison Analysis by Manuf Scraped', 'Price Point Distribution Analysis By Brand', 'Price Point Distribution Analysis By Brand By', 'Category Price Correlation Analysis P3Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd647d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "%run \"{os.path.dirname(os.getcwd())}\\general_functions\\generalFunctions.ipynb\" # container\n",
    "\n",
    "%run \"{os.path.dirname(os.getcwd())}\\general_functions\\Extracting Data Functions.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8130bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import adodbapi\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1983f646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ali Salem\\Desktop\\App_Update\\parameters.xlsx\n"
     ]
    }
   ],
   "source": [
    "filename = 'parameters.xlsx'\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.path.dirname(os.getcwd())\n",
    "\n",
    "# Construct the full path to the file\n",
    "f_path = os.path.join(current_dir, filename)\n",
    "print(f_path)\n",
    "#xls = pd.ExcelFile(f_path)\n",
    "parm = pd.read_excel(f_path, sheet_name='Pricing')\n",
    "fields = dict(zip(parm['Field'],parm['Value']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf0d1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "server = \"powerbi://api.powerbi.com/v1.0/myorg/\"+ fields['server']\n",
    "dataset_name = fields['f_name']\n",
    "f_name = os.getcwd()+\"/\"+fields['f_name']+\".xlsx\"\n",
    "\n",
    "client_manuf = list(set(fields['client_manuf'].split(','))-set(['']))\n",
    "client_brands = list(set(fields['client_brands'].split(','))-set(['']))\n",
    "prodORitem = fields['prodORitem']\n",
    "decimals = fields['decimals']\n",
    "sign = fields['sign']\n",
    "currency = fields['currency']\n",
    "currency = ' '+ currency if sign.lower() == 'after' else  currency + ' ' \n",
    "\n",
    "categories = list(set(fields['categories'].split(','))-set(['']))\n",
    "sectors=list(set(fields['sectors'].split(','))-set(['']))\n",
    "segments=list(set(fields['segments'].split(','))-set(['']))\n",
    "subsegments=list(set(fields['subsegments'].split(','))-set(['']))\n",
    "subcategories=list(set(fields['subcategories'].split(','))-set(['']))\n",
    "\n",
    "national=fields['national']\n",
    "customareas=fields['customareas']\n",
    "areas = list(set(fields['areas'].split(','))-set(['']))+[customareas]\n",
    "\n",
    "regions_RET = list(set(fields['regions_RET'].split(','))-set(['']))\n",
    "channels_RET = list(set(fields['channels_RET'].split(','))-set(['']))\n",
    "market_RET=list(set(fields['market_RET'].split(','))-set(['']))\n",
    "\n",
    "regions_CHAN=list(set(fields['regions_CHAN'].split(','))-set(['']))\n",
    "channels_CHAN=list(set(fields['channels_CHAN'].split(','))-set(['']))\n",
    "market_CHAN=list(set(fields['market_CHAN'].split(','))-set(['']))\n",
    "\n",
    "regions_CUST=list(set(fields['regions_CUST'].split(','))-set(['']))\n",
    "channels_CUST=list(set(fields['channels_CUST'].split(','))-set(['']))\n",
    "market_CUST=list(set(fields['market_CUST'].split(','))-set(['']))\n",
    "\n",
    "data_source=fields['data_source']\n",
    "years=list(set(fields['years'].split(','))-set(['']))\n",
    "years = {int(y) for y in fields['years'].split(',') if y}\n",
    "end_date=fields['end_date']\n",
    "\n",
    "ManufOrTopC = fields['ManufOrTopC']\n",
    "BrandOrTopB = fields['BrandOrTopB']\n",
    "\n",
    "National=[\"NATIONAL\"]if national else []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fabebc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "past_12_months = pd.date_range(end=end_date, periods=12, freq='ME').strftime('%b-%y').tolist()\n",
    "past_3_months = pd.date_range(end=end_date, periods=3, freq='ME').strftime('%b-%y').tolist()\n",
    "past_36_months = pd.date_range(end=end_date, periods=36, freq='ME').strftime('%b-%y').tolist()\n",
    "\n",
    "regions = regions_RET + regions_CHAN + regions_CUST\n",
    "channels = channels_RET + channels_CHAN + channels_CUST\n",
    "markets = market_RET + market_CHAN + market_CUST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a2e9f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_hierarchy = [\n",
    "    (\"Area\",National),\n",
    "    (\"Region\", regions),\n",
    "    (\"Channel\", channels),\n",
    "    (\"Market\", markets)\n",
    "]\n",
    "hierarchy_levels = [\n",
    "    (\"Category\", categories),\n",
    "    (\"Sector\", sectors),\n",
    "    (\"Segment\", segments),\n",
    "    (\"SubSegment\", subsegments),\n",
    "    (\"SubCategory\", subcategories)\n",
    " \n",
    "]\n",
    "direct_parent = {\"Sector\":\"Category\",\n",
    "                \"Segment\":\"Sector\",\n",
    "                \"SubSegment\":\"Segment\", \n",
    "                \"SubCategory\":\"Segment\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c639a1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "p12m_dax = \"{\" + \", \".join(f'\"{date}\"' for date in past_12_months) + \"}\"\n",
    "p3m_dax = \"{\" + \", \".join(f'\"{date}\"' for date in past_3_months) + \"}\"\n",
    "p36m_dax = \"{\" + \", \".join(f'\"{date}\"' for date in past_36_months) + \"}\"\n",
    "\n",
    "path=os.path.join(os.getcwd(),\"Pricing Datasets Test\")\n",
    "\n",
    "conn_str = f\"Provider=MSOLAP.8;Data Source={server};Initial Catalog={dataset_name};Timeout=900;\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d77d233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# client_manuf = [\"Edgewell Personal Care\"]\n",
    "# client_brands = [\"Schick\",\"Equate\",\"Cremo\"]\n",
    "# decimals = 2\n",
    "# sign = \"Before\"\n",
    "# currency = '$'\n",
    "# currency = ' '+ currency if sign.lower() == 'after' else  currency + ' '\n",
    "\n",
    "# prodORitem=\"SKU\"\n",
    "# categories =[\"Manual Shave Men\"]\n",
    "# sectors = [\"System\",\"Disposables\"]\n",
    "# segments = [\"Razors\",\"Refills\",\"Disposables\"]\n",
    "# subsegments= []\n",
    "# subcategories= []\n",
    "\n",
    "# customareas=''\n",
    "# national = True\n",
    "# areas = ['NATIONAL','RETAILER']\n",
    "\n",
    "# regions_RET  =[\"Bj's And Sam's\",\"Walmart\"]\n",
    "# channels_RET = []\n",
    "# market_RET = []\n",
    " \n",
    "# regions_CHAN = []\n",
    "# channels_CHAN = []\n",
    "# market_CHAN = []\n",
    " \n",
    "# regions_CUST = []\n",
    "# channels_CUST = []\n",
    "# market_CUST = []\n",
    " \n",
    "# data_source = \"DATA SOURCE: Trade Panel/Retailer Data | Ending March  2025\"\n",
    "# end_date = \"2025-04-01\"\n",
    "# years = {2023,2024,2025}\n",
    "\n",
    "# ManufOrTopC =\"Top Companies\"\n",
    "# BrandOrTopB = \"Brand\"\n",
    "\n",
    "# past_12_months = pd.date_range(end=end_date, periods=12, freq='ME').strftime('%b-%y').tolist()\n",
    "# past_3_months = pd.date_range(end=end_date, periods=3, freq='ME').strftime('%b-%y').tolist()\n",
    "# past_36_months = pd.date_range(end=end_date, periods=36, freq='ME').strftime('%b-%y').tolist()\n",
    "\n",
    "# National=[\"NATIONAL\"]if national else []\n",
    "# regions = regions_RET + regions_CHAN + regions_CUST\n",
    "# channels = channels_RET + channels_CHAN + channels_CUST\n",
    "# markets = market_RET + market_CHAN + market_CUST\n",
    "# brands_only = True  # Get the Data of SKU Share by brands level only\n",
    "\n",
    "# entity_hierarchy = [\n",
    "#     (\"Area\",National),\n",
    "#     (\"Region\", regions),\n",
    "#     (\"Channel\", channels),\n",
    "#     (\"Market\", markets)\n",
    "# ]\n",
    "# hierarchy_levels = [\n",
    "#     (\"Category\", categories),\n",
    "#     (\"Sector\", sectors),\n",
    "#     (\"Segment\", segments),\n",
    "#     (\"SubSegment\", subsegments),\n",
    "#     (\"SubCategory\", subcategories)\n",
    " \n",
    "# ]\n",
    "# direct_parent = {\"Sector\":\"Category\",\n",
    "#                 \"Segment\":\"Sector\",\n",
    "#                 \"SubSegment\":\"Segment\", \n",
    "#                 \"SubCategory\":\"Segment\"}\n",
    "# server = \"powerbi://api.powerbi.com/v1.0/myorg/Edgewell\"\n",
    "# dataset_name = \"Edgewell US Male Dataset\"\n",
    "# p12m_dax = \"{\" + \", \".join(f'\"{date}\"' for date in past_12_months) + \"}\"\n",
    "# p3m_dax = \"{\" + \", \".join(f'\"{date}\"' for date in past_3_months) + \"}\"\n",
    "# p36m_dax = \"{\" + \", \".join(f'\"{date}\"' for date in past_36_months) + \"}\"\n",
    "\n",
    "# print(p12m_dax)\n",
    "# path=os.path.join(os.getcwd(),\"Pricing Datasets NewEX\")\n",
    "\n",
    "# conn_str = f\"Provider=MSOLAP.8;Data Source={server};Initial Catalog={dataset_name};Timeout=900;\"\n",
    "# print(conn_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e9bcad",
   "metadata": {},
   "source": [
    "## Price Positioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e63c9c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "All DataFrames saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Pricing\\Pricing Datasets Test\\price_positioning_brands.pkl.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "All DataFrames saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Pricing\\Pricing Datasets Test\\price_positioning_manuf.pkl.\n"
     ]
    }
   ],
   "source": [
    "def execute_dax_query(BrandorManuf,entity_name,entity_type, area, hierby):\n",
    "    outputdic = {}\n",
    "    key =  f\"{entity_type} | {entity_name}\"\n",
    "\n",
    "\n",
    "    columns = [\n",
    "        \"Relative Price\",'Av Price/Unit','Value Sales','Value Share','Value Share DYA','Av Price/KG','IYA Price/KG'\n",
    "    ]\n",
    "    \n",
    "    column_exprs = \", \".join(f'\"{col}\", COALESCE([{col}], 0)' for col in columns)\n",
    "\n",
    "    # Main query\n",
    "    dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(\n",
    "                    Products,\n",
    "                    Products[{BrandorManuf}]\n",
    "                ),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            TREATAS({p12m_dax}, Calendar[MonthYear]),\n",
    "            Products[{hierby}] = \"{entity_type}\",\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"{hierby}\")\n",
    "        )\n",
    "    \"\"\"\n",
    "\n",
    "    # Grand total query (not grouped by BrandOrTopB, just Category)\n",
    "    grandtotal_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                VALUES(Products[Category]),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            TREATAS({p12m_dax}, Calendar[MonthYear]),\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"{hierby}\")\n",
    "        )\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(dax_query)\n",
    "            columns = [desc[0] for desc in cursor.description]\n",
    "            data = cursor.fetchall()\n",
    "            \n",
    "\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(grandtotal_query)\n",
    "            grandtotal_columns = [desc[0] for desc in cursor.description]\n",
    "            grandtotal_data = cursor.fetchall()\n",
    "            \n",
    "            df = pd.DataFrame(data, columns=columns)\n",
    "            df.columns = df.columns.str.replace(r'.*\\[|\\]', '', regex=True)         \n",
    "            df = df.loc[~(df.select_dtypes(include='number') == 0).all(axis=1)]  # Remove zero rows\n",
    "\n",
    "            grand_tot = pd.DataFrame(grandtotal_data, columns=grandtotal_columns)\n",
    "            grand_tot.columns = grand_tot.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "            grand_tot = grand_tot.loc[~(grand_tot.select_dtypes(include='number') == 0).all(axis=1)]  # Remove zero rows\n",
    "\n",
    "            grand_tot[df.columns[0]] = 'Grand Total'\n",
    "\n",
    "            # Reorder columns if necessary\n",
    "            grand_tot = grand_tot[df.columns]\n",
    "\n",
    "            # Concatenate the two\n",
    "            df = pd.concat([df, grand_tot], ignore_index=True)\n",
    "            outputdic[key] = df  \n",
    "\n",
    "            \n",
    "            print(f\"Query executed successfully for {entity_name}.\")\n",
    "    except adodbapi.DatabaseError as db_error:\n",
    "        print(f\"Database error for {entity_name} in {area}: {db_error}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error for {entity_name} in {area}: {e}\")\n",
    "\n",
    "    return outputdic\n",
    "\n",
    "\n",
    "\n",
    "def process_dax_queries(BrandorManuf,entity_hierarchy, hierarchy_levels):\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        dfs_results = {} \n",
    "        futures = {}\n",
    "        ordered_keys=[]\n",
    "        for hierby, hier_values in hierarchy_levels:\n",
    "            if isinstance(hier_values, list):\n",
    "                for value in hier_values:\n",
    "                    for area, entity_list in entity_hierarchy:\n",
    "                        for entity in entity_list:\n",
    "                            # print(hierby,value,entity)                                    \n",
    "                            key = f\"{value} | {entity}\"\n",
    "                            ordered_keys.append(key)\n",
    "                            future = executor.submit(execute_dax_query,BrandorManuf, entity,value,area, hierby)\n",
    "                            futures[future] = key\n",
    "       \n",
    "        temp_results = {}\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            temp_results.update(result)\n",
    "\n",
    "        # Insert results in original order\n",
    "        for key in ordered_keys:\n",
    "            if key in temp_results:\n",
    "                dfs_results[key] = temp_results[key]\n",
    "        if BrandorManuf==f'{BrandOrTopB}':\n",
    "            filename =  f\"price_positioning_brands.pkl\"\n",
    "        else:\n",
    "            filename =  f\"price_positioning_manuf.pkl\"\n",
    "        output_file = f\"{path}\\\\{filename}\"\n",
    "        with open(output_file, \"wb\") as f:\n",
    "            pd.to_pickle(dfs_results, f)\n",
    "        \n",
    "        print(f\"All DataFrames saved to {output_file}.\")\n",
    "\n",
    "process_dax_queries(f'{BrandOrTopB}',entity_hierarchy,hierarchy_levels) \n",
    "process_dax_queries(f'{ManufOrTopC}',entity_hierarchy,hierarchy_levels) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4fc172",
   "metadata": {},
   "source": [
    "## Sector/Segment/SubSegment Leaderships (SINGLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25e87c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "All DataFrames for Category saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Pricing\\Pricing Datasets Test\\Category_leadership.pkl.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "All DataFrames for Sector saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Pricing\\Pricing Datasets Test\\Sector_leadership.pkl.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "All DataFrames for Segment saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Pricing\\Pricing Datasets Test\\Segment_leadership.pkl.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "All DataFrames for SubSegment saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Pricing\\Pricing Datasets Test\\SubSegment_leadership.pkl.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "All DataFrames for SubCategory saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Pricing\\Pricing Datasets Test\\SubCategory_leadership.pkl.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "All DataFrames for Category saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Pricing\\Pricing Datasets Test\\Category_total_leadership.pkl.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "All DataFrames for Sector saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Pricing\\Pricing Datasets Test\\Sector_total_leadership.pkl.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for Segment saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Pricing\\Pricing Datasets Test\\Segment_total_leadership.pkl.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "All DataFrames for SubSegment saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Pricing\\Pricing Datasets Test\\SubSegment_total_leadership.pkl.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "All DataFrames for SubCategory saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Pricing\\Pricing Datasets Test\\SubCategory_total_leadership.pkl.\n"
     ]
    }
   ],
   "source": [
    "def execute_dax_query(entity_name, area, hierby, client, total=True):\n",
    "    outputdic = {}\n",
    "    key = f\"{client} | {entity_name}\" if client else f\"{entity_name}\"\n",
    "\n",
    "    columns = [\n",
    "        \"Value Share\", \"Value Sales\", \"Av Price/KG\", \"WoB %\", \"Gross Margin %\"\n",
    "    ]\n",
    "    column_exprs = \", \".join(f'\"{col}\", COALESCE([{col}], 0)' for col in columns)\n",
    "\n",
    "    # Prepare client brand filter conditionally\n",
    "    client_filter = \"\"\n",
    "    if not total and client:\n",
    "        client_filter = f'''\n",
    "            FILTER(\n",
    "                Products,\n",
    "                Products[Top Brands] = \"{client}\"\n",
    "            ),\n",
    "        '''\n",
    "\n",
    "    # Main query\n",
    "    dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(\n",
    "                    Products,\n",
    "                    Products[{hierby}]\n",
    "                ),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            TREATAS({p12m_dax}, Calendar[MonthYear]),\n",
    "            {client_filter}\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"Category\")\n",
    "        )\n",
    "    \"\"\"\n",
    "    # Grand total query\n",
    "    grandtotal_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                VALUES(Products[Category]),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            TREATAS({p12m_dax}, Calendar[MonthYear]),\n",
    "            {client_filter}\n",
    "\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}])\n",
    "        )\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(dax_query)\n",
    "            columns_result = [desc[0] for desc in cursor.description]\n",
    "            data = cursor.fetchall()\n",
    "\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(grandtotal_query)\n",
    "            grandtotal_columns = [desc[0] for desc in cursor.description]\n",
    "            grandtotal_data = cursor.fetchall()\n",
    "\n",
    "        df = pd.DataFrame(data, columns=columns_result)\n",
    "        df.columns = df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        df = df.loc[~(df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "\n",
    "        grand_tot = pd.DataFrame(grandtotal_data, columns=grandtotal_columns)\n",
    "        grand_tot.columns = grand_tot.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        grand_tot = grand_tot.loc[~(grand_tot.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "        grand_tot[df.columns[0]] = 'Grand Total'\n",
    "        grand_tot = grand_tot[df.columns]\n",
    "        df = pd.concat([df, grand_tot], ignore_index=True)\n",
    "        outputdic[key] = df\n",
    "\n",
    "        print(f\"Query executed successfully for {entity_name}.\")\n",
    "    except adodbapi.DatabaseError as db_error:\n",
    "        print(f\"Database error for {entity_name} in {area}: {db_error}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error for {entity_name} in {area}: {e}\")\n",
    "\n",
    "    return outputdic\n",
    "\n",
    "\n",
    "def process_dax_queries(entity_hierarchy, hierarchy_levels, client_brands=None, total=False):\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        \n",
    "        dfs_results = {} \n",
    "        futures = {}\n",
    "        ordered_keys = []\n",
    "        for hierby, hier_values in hierarchy_levels:\n",
    "            if isinstance(hier_values, list):\n",
    "                if client_brands: \n",
    "                    for area, entity_list in entity_hierarchy:\n",
    "                        for entity in entity_list:\n",
    "                            for client in client_brands:\n",
    "                                key = f\"{client} | {entity}\"\n",
    "                                ordered_keys.append(key)\n",
    "                                future = executor.submit(\n",
    "                                    execute_dax_query, entity, area, hierby, client, total\n",
    "                                )\n",
    "                                futures[future] = key\n",
    "                else:\n",
    "                    for area, entity_list in entity_hierarchy:\n",
    "                        for entity in entity_list:\n",
    "                            key = f\"{entity}\"\n",
    "                            ordered_keys.append(key)\n",
    "                            future = executor.submit(\n",
    "                                execute_dax_query, entity, area, hierby, '', total\n",
    "                            )\n",
    "                            futures[future] = key\n",
    "\n",
    "            temp_results = {}\n",
    "            for future in as_completed(futures):\n",
    "                result = future.result()\n",
    "                temp_results.update(result)\n",
    "\n",
    "            for key in ordered_keys:\n",
    "                if key in temp_results:\n",
    "                    dfs_results[key] = temp_results[key]\n",
    "\n",
    "            if client_brands:\n",
    "                filename = f\"{hierby}_leadership.pkl\"\n",
    "            else:\n",
    "                filename = f\"{hierby}_total_leadership.pkl\"\n",
    "\n",
    "            output_file = f\"{path}\\\\{filename}\"\n",
    "            with open(output_file, \"wb\") as f:\n",
    "                pd.to_pickle(dfs_results, f)\n",
    "            \n",
    "            print(f\"All DataFrames for {hierby} saved to {output_file}.\")\n",
    "\n",
    "# Execute\n",
    "if client_brands:\n",
    "    process_dax_queries(entity_hierarchy, hierarchy_levels, client_brands=client_brands)\n",
    "\n",
    "process_dax_queries(entity_hierarchy, hierarchy_levels, total=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b263f3c8",
   "metadata": {},
   "source": [
    "## Sector/Segment Leaderships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "362fe3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for Segment saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Pricing\\Pricing Datasets Test\\brand_Segment.pkl.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "All DataFrames for SubSegment saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Pricing\\Pricing Datasets Test\\brand_SubSegment.pkl.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for SubCategory saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Pricing\\Pricing Datasets Test\\brand_SubCategory.pkl.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "All DataFrames for Segment saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Pricing\\Pricing Datasets Test\\manuf_Segment.pkl.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for SubSegment saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Pricing\\Pricing Datasets Test\\manuf_SubSegment.pkl.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "All DataFrames for SubCategory saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Pricing\\Pricing Datasets Test\\manuf_SubCategory.pkl.\n",
      "SALMA YA SALAMA\n",
      "SALMA YA SALAMA\n",
      "SALMA YA SALAMA\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "All DataFrames for Segment saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Pricing\\Pricing Datasets Test\\total_Segment.pkl.\n",
      "SALMA YA SALAMA\n",
      "SALMA YA SALAMA\n",
      "SALMA YA SALAMA\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for SubSegment saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Pricing\\Pricing Datasets Test\\total_SubSegment.pkl.\n",
      "SALMA YA SALAMA\n",
      "SALMA YA SALAMA\n",
      "SALMA YA SALAMA\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for SubCategory saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Pricing\\Pricing Datasets Test\\total_SubCategory.pkl.\n"
     ]
    }
   ],
   "source": [
    "def execute_dax_query(entity_name, area, hierby,direct_parent, client,manuf, total=False):\n",
    "    outputdic = {}\n",
    "    key = f\"{client} | {entity_name}\" if client else (f\"{manuf} | {entity_name}\" if manuf else f\"{entity_name}\")\n",
    "\n",
    "    columns = [\n",
    "        \"Value Share\", \"Value Sales\", \"Av Price/KG\", \"WoB %\", \"Gross Margin %\"\n",
    "    ]\n",
    "    column_exprs = \", \".join(f'\"{col}\", COALESCE([{col}], 0)' for col in columns)\n",
    "\n",
    "    # Prepare client brand filter conditionally\n",
    "    client_filter = \"\"\n",
    "    if not total and client:\n",
    "        client_filter = f'''\n",
    "            FILTER(\n",
    "                Products,\n",
    "                Products[Top Brands] = \"{client}\"\n",
    "            ),\n",
    "        '''    \n",
    "    manuf_filter=''\n",
    "    if not total and manuf:\n",
    "        manuf_filter = f'''\n",
    "            FILTER(\n",
    "                Products,\n",
    "                Products[Top Companies] = \"{manuf}\"\n",
    "            ),\n",
    "        '''\n",
    "\n",
    "    # Main query\n",
    "    dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(\n",
    "                    Products,\n",
    "                    Products[{direct_parent[hierby]}],\n",
    "                    Products[{hierby}]\n",
    "                    \n",
    "                ),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            TREATAS({p12m_dax}, Calendar[MonthYear]),\n",
    "            {client_filter}\n",
    "            {manuf_filter}\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"Category\")\n",
    "        )\n",
    "    \"\"\"\n",
    "    parenttotal_dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(\n",
    "                    Products,\n",
    "                    Products[{direct_parent[hierby]}]                   \n",
    "                ),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            TREATAS({p12m_dax}, Calendar[MonthYear]),\n",
    "            {client_filter}\n",
    "            {manuf_filter}\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"Category\")\n",
    "        )\n",
    "    \"\"\"\n",
    "        # Grand total query\n",
    "    grandtotal_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                VALUES(Products[Category]),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            TREATAS({p12m_dax}, Calendar[MonthYear]),\n",
    "            {client_filter}\n",
    "            {manuf_filter}\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}])\n",
    "        )\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(dax_query)\n",
    "            columns_result = [desc[0] for desc in cursor.description]\n",
    "            data = cursor.fetchall()\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(parenttotal_dax_query)\n",
    "            maintotal_columns = [desc[0] for desc in cursor.description]\n",
    "            maintotal_data = cursor.fetchall()\n",
    "            \n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(grandtotal_query)\n",
    "            grandtotal_columns = [desc[0] for desc in cursor.description]\n",
    "            grandtotal_data = cursor.fetchall()\n",
    "\n",
    "        df = pd.DataFrame(data, columns=columns_result)\n",
    "        df.columns = df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        df = df.loc[~(df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "        maintotal_df = pd.DataFrame(maintotal_data, columns=maintotal_columns)\n",
    "        maintotal_df.columns = maintotal_df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        maintotal_df = maintotal_df.loc[~(maintotal_df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "        \n",
    "        grand_tot = pd.DataFrame(grandtotal_data, columns=grandtotal_columns)\n",
    "        grand_tot.columns = grand_tot.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        grand_tot = grand_tot.loc[~(grand_tot.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "        \n",
    "        if maintotal_df.shape[1] > 1:\n",
    "            maintotal_df.iloc[:, 0] = maintotal_df.iloc[:, 0].astype(str) + \" Total\"\n",
    "\n",
    "        if maintotal_df.empty:\n",
    "            outputdic[key] = maintotal_df\n",
    "            return outputdic\n",
    "        if not maintotal_df.empty:\n",
    "            df_with_totals = pd.concat([df,maintotal_df], ignore_index=True)\n",
    "    \n",
    "        if not grand_tot.empty:\n",
    "            # Create a dict for the first two columns\n",
    "            grand_tot[df.columns[0]] = 'Grand Total'\n",
    "            grand_tot[df.columns[1]] = np.nan  # or pd.NA\n",
    "\n",
    "            # Ensure all required columns exist, fill missing ones with NaN\n",
    "            for col in df.columns:\n",
    "                if col not in grand_tot.columns:\n",
    "                    grand_tot[col] = np.nan\n",
    "\n",
    "            # Reorder columns exactly as in df\n",
    "            grand_tot = grand_tot[df.columns]\n",
    "            df = pd.concat([df_with_totals, grand_tot], ignore_index=True)\n",
    "\n",
    "        outputdic[key] = df\n",
    "\n",
    "        print(f\"Query executed successfully for {entity_name}.\")\n",
    "    except adodbapi.DatabaseError as db_error:\n",
    "        print(f\"Database error for {entity_name} in {area}: {db_error}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error for {entity_name} in {area}: {e}\")\n",
    "\n",
    "    return outputdic\n",
    "\n",
    "\n",
    "def process_dax_queries(entity_hierarchy, hierarchy_levels,direct_parent, client_brands=None,client_manuf=None, total=False):\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        dfs_results = {} \n",
    "        futures = {}\n",
    "        ordered_keys = []\n",
    "        for hierby, hier_values in hierarchy_levels:\n",
    "            if hierby == \"Category\" or direct_parent[hierby]==\"Category\":\n",
    "                    continue\n",
    "            if isinstance(hier_values, list):\n",
    "                if client_brands and not total: \n",
    "                    for area, entity_list in entity_hierarchy:\n",
    "                        for entity in entity_list:\n",
    "                            for client in client_brands:\n",
    "                                key = f\"{client} | {entity}\"\n",
    "                                ordered_keys.append(key)\n",
    "                                future = executor.submit(\n",
    "                                    execute_dax_query, entity, area, hierby,direct_parent, client,'', ''\n",
    "                                )\n",
    "                                futures[future] = key\n",
    "                if client_manuf and not total:\n",
    "                    for area, entity_list in entity_hierarchy:\n",
    "                        for entity in entity_list:\n",
    "                            for manuf in client_manuf:\n",
    "                                key = f\"{manuf} | {entity}\"\n",
    "                                ordered_keys.append(key)\n",
    "                                future = executor.submit(\n",
    "                                    execute_dax_query, entity, area, hierby,direct_parent, '',manuf,''\n",
    "                                )\n",
    "                                futures[future] = key               \n",
    "                elif total==True:\n",
    "                    for area, entity_list in entity_hierarchy:\n",
    "                        for entity in entity_list:\n",
    "                            key = f\"{entity}\"\n",
    "                            print(\"SALMA YA SALAMA\")\n",
    "                            ordered_keys.append(key)\n",
    "                            future = executor.submit(\n",
    "                                execute_dax_query, entity, area, hierby, direct_parent,'','', total\n",
    "                            )\n",
    "                            futures[future] = key\n",
    "\n",
    "            temp_results = {}\n",
    "            for future in as_completed(futures):\n",
    "                result = future.result()\n",
    "                temp_results.update(result)\n",
    "\n",
    "            for key in ordered_keys:\n",
    "                if key in temp_results:\n",
    "                    dfs_results[key] = temp_results[key]\n",
    "\n",
    "            if client_brands:\n",
    "                filename = f\"brand_{hierby}.pkl\"\n",
    "            elif client_manuf:\n",
    "                filename = f\"manuf_{hierby}.pkl\"\n",
    "            else:\n",
    "                filename = f\"total_{hierby}.pkl\"\n",
    "\n",
    "            output_file = f\"{path}\\\\{filename}\"\n",
    "            with open(output_file, \"wb\") as f:\n",
    "                pd.to_pickle(dfs_results, f)\n",
    "            \n",
    "            print(f\"All DataFrames for {hierby} saved to {output_file}.\")\n",
    "\n",
    "if client_brands:\n",
    "    process_dax_queries(entity_hierarchy, hierarchy_levels,direct_parent, client_brands=client_brands)\n",
    "if client_manuf:\n",
    "    process_dax_queries(entity_hierarchy, hierarchy_levels,direct_parent, client_manuf=client_manuf)   \n",
    "process_dax_queries(entity_hierarchy, hierarchy_levels,direct_parent, total=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0800646",
   "metadata": {},
   "source": [
    "## Shelf & Avg Price / KG By Brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3eb2ca95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "All DataFrames for Sector saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Pricing\\Pricing Datasets Test\\shelf_Sector_top_brands.pkl.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "All DataFrames for Segment saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Pricing\\Pricing Datasets Test\\shelf_Segment_top_brands.pkl.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "All DataFrames for SubSegment saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Pricing\\Pricing Datasets Test\\shelf_SubSegment_top_brands.pkl.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "All DataFrames for SubCategory saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Pricing\\Pricing Datasets Test\\shelf_SubCategory_top_brands.pkl.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for Sector saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Pricing\\Pricing Datasets Test\\shelf_Sector_all_brands.pkl.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "All DataFrames for Segment saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Pricing\\Pricing Datasets Test\\shelf_Segment_all_brands.pkl.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "All DataFrames for SubSegment saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Pricing\\Pricing Datasets Test\\shelf_SubSegment_all_brands.pkl.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "All DataFrames for SubCategory saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Pricing\\Pricing Datasets Test\\shelf_SubCategory_all_brands.pkl.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for Sector saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Pricing\\Pricing Datasets Test\\shelf_Sector_top_manuf.pkl.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for Segment saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Pricing\\Pricing Datasets Test\\shelf_Segment_top_manuf.pkl.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "All DataFrames for SubSegment saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Pricing\\Pricing Datasets Test\\shelf_SubSegment_top_manuf.pkl.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "All DataFrames for SubCategory saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Pricing\\Pricing Datasets Test\\shelf_SubCategory_top_manuf.pkl.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for Sector saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Pricing\\Pricing Datasets Test\\shelf_Sector_all_manuf.pkl.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "All DataFrames for Segment saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Pricing\\Pricing Datasets Test\\shelf_Segment_all_manuf.pkl.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "All DataFrames for SubSegment saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Pricing\\Pricing Datasets Test\\shelf_SubSegment_all_manuf.pkl.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "All DataFrames for SubCategory saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Pricing\\Pricing Datasets Test\\shelf_SubCategory_all_manuf.pkl.\n"
     ]
    }
   ],
   "source": [
    "def execute_dax_query(manuforbrand,entity_name,entity_type, area, hierby, incall=True):\n",
    "    outputdic = {}\n",
    "    key =  f\"{entity_type} | {entity_name}\"\n",
    "    if manuforbrand==f'{BrandOrTopB}':\n",
    "        columns = [\n",
    "            \"Relative Price\", \"Av Price/Unit\", \"Value Sales\", \"IYA Price/KG\", \"Base Price/KG\",\"Av Price/KG\",\"Value Share\",\"WoB %\",\"Value Share DYA\"\n",
    "        ]\n",
    "    else:\n",
    "        columns = [\"Base Price/KG\",\"Av Price/KG\",\"Value Share\",\"WoB %\",\"Value Share DYA\"]\n",
    " \n",
    " \n",
    "    column_exprs = \", \".join(f'\"{col}\", COALESCE([{col}], 0)' for col in columns)\n",
    " \n",
    "    # Prepare client brand filter conditionally\n",
    "    scope_filter = \"\"\n",
    "    if incall:\n",
    "        scope_filter = f'''\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"Category\"),\n",
    "            Products[{direct_parent[hierby]}] = \"{entity_type}\"\n",
    "            \n",
    "        '''\n",
    "    else:\n",
    "        scope_filter = f'''\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"{hierby}\"),\n",
    "            Products[{direct_parent[hierby]}] = \"{entity_type}\"\n",
    "        '''\n",
    " \n",
    "    # Main query\n",
    "    dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(\n",
    "                    Products,\n",
    "                    Products[{hierby}],\n",
    "                    Products[{manuforbrand}]\n",
    "                ),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            TREATAS({p12m_dax}, Calendar[MonthYear]),\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "            {scope_filter}\n",
    "        )\n",
    "    \"\"\"\n",
    "    parenttotal_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(\n",
    "                    Products,\n",
    "                    Products[{hierby}]\n",
    " \n",
    "                ),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            TREATAS({p12m_dax}, Calendar[MonthYear]),\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "            {scope_filter}\n",
    "        )\n",
    "    \"\"\"\n",
    "   \n",
    "    # Grand total query\n",
    "    grandtotal_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                VALUES(Products[Category]),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            TREATAS({p12m_dax}, Calendar[MonthYear]),\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "            {scope_filter}\n",
    " \n",
    "        )\n",
    "    \"\"\"\n",
    " \n",
    "    try:\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(dax_query)\n",
    "            columns_result = [desc[0] for desc in cursor.description]\n",
    "            data = cursor.fetchall()\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(parenttotal_query)\n",
    "            maintotal_columns = [desc[0] for desc in cursor.description]\n",
    "            maintotal_data = cursor.fetchall()\n",
    " \n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(grandtotal_query)\n",
    "            grandtotal_columns = [desc[0] for desc in cursor.description]\n",
    "            grandtotal_data = cursor.fetchall()\n",
    " \n",
    "        df = pd.DataFrame(data, columns=columns_result)\n",
    "        df.columns = df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        df = df.loc[~(df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    " \n",
    "        maintotal_df = pd.DataFrame(maintotal_data, columns=maintotal_columns)\n",
    "        maintotal_df.columns = maintotal_df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        maintotal_df = maintotal_df.loc[~(maintotal_df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "       \n",
    "        grand_tot = pd.DataFrame(grandtotal_data, columns=grandtotal_columns)\n",
    "        grand_tot.columns = grand_tot.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        grand_tot = grand_tot.loc[~(grand_tot.select_dtypes(include='number') == 0).all(axis=1)]\n",
    " \n",
    "        if maintotal_df.shape[1] > 1:\n",
    "            maintotal_df.iloc[:, 0] = maintotal_df.iloc[:, 0].astype(str) + \" Total\"\n",
    " \n",
    "        if maintotal_df.empty:\n",
    "            outputdic[key] = maintotal_df\n",
    "            return outputdic\n",
    "        if not maintotal_df.empty:\n",
    "            df_with_totals = pd.concat([df,maintotal_df], ignore_index=True)\n",
    " \n",
    "        if not grand_tot.empty:\n",
    "            # Ensure 'Grand Total' label is added to the first column of grand_tot\n",
    "            grand_tot[df_with_totals.columns[0]] = 'Grand Total'\n",
    " \n",
    "            # Keep only columns that exist in df_with_totals\n",
    "            common_columns = [col for col in df_with_totals.columns if col in grand_tot.columns]\n",
    "            grand_tot = grand_tot[common_columns]\n",
    " \n",
    "            # Align grand_tot with df_with_totals in case column order matters\n",
    "            grand_tot = grand_tot.reindex(columns=df_with_totals.columns)\n",
    " \n",
    "        df = pd.concat([df_with_totals, grand_tot], ignore_index=True)\n",
    "        outputdic[key] = df\n",
    " \n",
    " \n",
    "        print(f\"Query executed successfully for {entity_name}.\")\n",
    "    except adodbapi.DatabaseError as db_error:\n",
    "        print(f\"Database error for {entity_name} in {area}: {db_error}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error for {entity_name} in {area}: {e}\")\n",
    " \n",
    "    return outputdic\n",
    " \n",
    " \n",
    "def process_dax_queries(manuforbrand,entity_hierarchy, hierarchy_levels, incall=False):\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        hierarchy_dict = dict(hierarchy_levels)\n",
    " \n",
    "        for hierby, _ in hierarchy_levels:\n",
    "            if hierby == \"Category\":\n",
    "                continue\n",
    "                   \n",
    "            dfs_results = {}\n",
    "            futures = {}\n",
    "            ordered_keys = []\n",
    "            # Get the parent level name (e.g., 'Segment')\n",
    "            parent_level = direct_parent[hierby]\n",
    "            parent_values = hierarchy_dict.get(parent_level, [])\n",
    "            if isinstance(parent_values, list):\n",
    "                for value in parent_values:\n",
    "                    for area, entity_list in entity_hierarchy:\n",
    "                        for entity in entity_list:\n",
    "                            key = f\"{value} | {entity}\"                          \n",
    "                            ordered_keys.append(key)\n",
    "                            future = executor.submit(\n",
    "                                execute_dax_query,manuforbrand, entity, value, area, hierby,incall\n",
    "                            )\n",
    "                            futures[future] = key\n",
    "            temp_results = {}\n",
    "            for future in as_completed(futures):\n",
    "                result = future.result()\n",
    "                temp_results.update(result)\n",
    " \n",
    "            for key in ordered_keys:\n",
    "                if key in temp_results:\n",
    "                    dfs_results[key] = temp_results[key]\n",
    "            if manuforbrand==f'{BrandOrTopB}':\n",
    "                if incall==False:\n",
    "                    filename = f\"shelf_{hierby}_top_brands.pkl\"\n",
    "                else:\n",
    "                    filename = f\"shelf_{hierby}_all_brands.pkl\"\n",
    "            else:\n",
    "                if incall==False:\n",
    "                    filename = f\"shelf_{hierby}_top_manuf.pkl\"\n",
    "                else:\n",
    "                    filename = f\"shelf_{hierby}_all_manuf.pkl\"\n",
    " \n",
    "            output_file = f\"{path}\\\\{filename}\"\n",
    "            with open(output_file, \"wb\") as f:\n",
    "                pd.to_pickle(dfs_results, f)\n",
    "           \n",
    "            print(f\"All DataFrames for {hierby} saved to {output_file}.\")\n",
    " \n",
    " \n",
    " \n",
    "# Execute\n",
    "process_dax_queries(f'{BrandOrTopB}',entity_hierarchy, hierarchy_levels,incall=False)\n",
    "process_dax_queries(f'{BrandOrTopB}',entity_hierarchy, hierarchy_levels, incall=True)\n",
    "process_dax_queries(f'{ManufOrTopC}',entity_hierarchy, hierarchy_levels,incall=False)\n",
    "process_dax_queries(f'{ManufOrTopC}',entity_hierarchy, hierarchy_levels, incall=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c5b360",
   "metadata": {},
   "source": [
    "## Price Point Distribution By Product (Item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0e76e696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "All DataFrames for Sector saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Pricing\\Pricing Datasets Test\\price_distribution_Sector_p12m.pkl.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "All DataFrames for Segment saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Pricing\\Pricing Datasets Test\\price_distribution_Segment_p12m.pkl.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.Query executed successfully for Bj's And Sam's.\n",
      "\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "All DataFrames for SubSegment saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Pricing\\Pricing Datasets Test\\price_distribution_SubSegment_p12m.pkl.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "All DataFrames for SubCategory saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Pricing\\Pricing Datasets Test\\price_distribution_SubCategory_p12m.pkl.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "All DataFrames for Sector saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Pricing\\Pricing Datasets Test\\price_distribution_Sector_p3m.pkl.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "All DataFrames for Segment saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Pricing\\Pricing Datasets Test\\price_distribution_Segment_p3m.pkl.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "All DataFrames for SubSegment saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Pricing\\Pricing Datasets Test\\price_distribution_SubSegment_p3m.pkl.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "All DataFrames for SubCategory saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Pricing\\Pricing Datasets Test\\price_distribution_SubCategory_p3m.pkl.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "All DataFrames for Sector saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Pricing\\Pricing Datasets Test\\price_distribution_Sector_manuf_p12m.pkl.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "All DataFrames for Segment saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Pricing\\Pricing Datasets Test\\price_distribution_Segment_manuf_p12m.pkl.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "All DataFrames for SubSegment saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Pricing\\Pricing Datasets Test\\price_distribution_SubSegment_manuf_p12m.pkl.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "All DataFrames for SubCategory saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Pricing\\Pricing Datasets Test\\price_distribution_SubCategory_manuf_p12m.pkl.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "All DataFrames for Sector saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Pricing\\Pricing Datasets Test\\price_distribution_Sector_manuf_p3m.pkl.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for Segment saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Pricing\\Pricing Datasets Test\\price_distribution_Segment_manuf_p3m.pkl.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "All DataFrames for SubSegment saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Pricing\\Pricing Datasets Test\\price_distribution_SubSegment_manuf_p3m.pkl.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "All DataFrames for SubCategory saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Pricing\\Pricing Datasets Test\\price_distribution_SubCategory_manuf_p3m.pkl.\n"
     ]
    }
   ],
   "source": [
    "def execute_dax_query(entity_name, entity_type, area, hierby, client, manuf,p12m=True):\n",
    "    outputdic = {}\n",
    "\n",
    "    key = f\"{client} | {entity_name} | {entity_type}\" if client else f\"{manuf} | {entity_name} | {entity_type}\" \n",
    "    if p12m :\n",
    "        timeper=p12m_dax\n",
    "    else:\n",
    "        timeper=p3m_dax    \n",
    "    columns = [\"Base Price/Unit\", \"Base Price/KG\", \"Value Sales\", \"Gross Margin %\"]\n",
    "    column_exprs = \", \".join(f'\"{col}\", COALESCE([{col}], 0)' for col in columns)\n",
    "    client_filter=''\n",
    "    if client:\n",
    "        client_filter = f'''\n",
    "                FILTER(\n",
    "                    Products,\n",
    "                    Products[{BrandOrTopB}] = \"{client}\" &&\n",
    "                    Products[{ManufOrTopC}] = \"{manuf}\"\n",
    "                ),\n",
    "            '''\n",
    "   \n",
    "    manuf_filter = f'FILTER(Products, Products[Top Companies] = \"{manuf}\"),' if manuf else \"\"\n",
    "\n",
    "    dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(\n",
    "                    Products,\n",
    "                    Products[{hierby}],\n",
    "                    Products[{prodORitem}],\n",
    "                    Products[Total Size]\n",
    "                ),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            Products[{direct_parent[hierby]}] = \"{entity_type}\",\n",
    "            TREATAS({timeper}, Calendar[MonthYear]),\n",
    "            {client_filter}\n",
    "            {manuf_filter}\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"{direct_parent[hierby]}\")\n",
    "        )\n",
    "    \"\"\"\n",
    "\n",
    "    # Parent total query\n",
    "    parenttotal_dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(Products, Products[{hierby}]),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            TREATAS({timeper}, Calendar[MonthYear]),\n",
    "            {client_filter}\n",
    "            {manuf_filter}\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"Category\")\n",
    "        )\n",
    "    \"\"\"\n",
    "    itemtotal_dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(Products,Products[{prodORitem}]),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            TREATAS({timeper}, Calendar[MonthYear]),\n",
    "            {client_filter}\n",
    "            {manuf_filter}\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"Category\")\n",
    "        )\n",
    "    \"\"\"\n",
    "\n",
    "    grandtotal_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(VALUES(Products[Category]), {column_exprs}),\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            TREATAS({timeper}, Calendar[MonthYear]),\n",
    "            {client_filter}\n",
    "            {manuf_filter}\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}])\n",
    "        )\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        with adodbapi.connect(conn_str) as conn:\n",
    "            with conn.cursor() as cursor:\n",
    "                cursor.execute(dax_query)\n",
    "                df = pd.DataFrame(cursor.fetchall(), columns=[desc[0] for desc in cursor.description])\n",
    "\n",
    "            with conn.cursor() as cursor:\n",
    "                cursor.execute(parenttotal_dax_query)\n",
    "                maintotal_df = pd.DataFrame(cursor.fetchall(), columns=[desc[0] for desc in cursor.description])\n",
    "          \n",
    "            with conn.cursor() as cursor:\n",
    "                cursor.execute(itemtotal_dax_query)\n",
    "                itemtotal_df = pd.DataFrame(cursor.fetchall(), columns=[desc[0] for desc in cursor.description])\n",
    "\n",
    "\n",
    "            with conn.cursor() as cursor:\n",
    "                cursor.execute(grandtotal_query)\n",
    "                grand_tot = pd.DataFrame(cursor.fetchall(), columns=[desc[0] for desc in cursor.description])\n",
    "\n",
    "        for dataframe in [df,maintotal_df,itemtotal_df, grand_tot]:\n",
    "            dataframe.columns = dataframe.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "            dataframe.dropna(how='all', inplace=True)\n",
    "\n",
    "        if not maintotal_df.empty:\n",
    "            maintotal_df.iloc[:, 0] = maintotal_df.iloc[:, 0].astype(str) + \" Total\"\n",
    "            df = pd.concat([df, maintotal_df], ignore_index=True)\n",
    "      \n",
    "        if not itemtotal_df.empty:\n",
    "            itemtotal_df.iloc[:, 0] = itemtotal_df.iloc[:, 0].astype(str) + \" Total\"\n",
    "            df = pd.concat([df, itemtotal_df], ignore_index=True)\n",
    "            \n",
    "        if not grand_tot.empty:\n",
    "            grand_tot[df.columns[0]] = 'Grand Total'\n",
    "            grand_tot[df.columns[1]] = np.nan\n",
    "            grand_tot = grand_tot.reindex(columns=df.columns)\n",
    "            df = pd.concat([df, grand_tot], ignore_index=True)\n",
    "\n",
    "        outputdic[key] = df\n",
    "        print(f\"Query executed successfully for {entity_name}.\")\n",
    "\n",
    "    except adodbapi.DatabaseError as db_error:\n",
    "        print(f\"Database error for {entity_name} in {area}: {db_error}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error for {entity_name} in {area}: {e}\")\n",
    "\n",
    "    return outputdic\n",
    "\n",
    "\n",
    "def process_dax_queries(entity_hierarchy, hierarchy_levels, client_brands=None, client_manuf=None,p12m=True):\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        for hierby, hier_values in hierarchy_levels:\n",
    "            if hierby == \"Category\":\n",
    "                continue\n",
    "\n",
    "            hierarchy_dict = dict(hierarchy_levels)\n",
    "            parent_level = direct_parent.get(hierby)\n",
    "            parent_values = hierarchy_dict.get(parent_level, [])\n",
    "\n",
    "            if not parent_values:\n",
    "                continue\n",
    "\n",
    "            futures = {}\n",
    "            ordered_keys = []\n",
    "\n",
    "            # Determine prodORitem per level\n",
    "\n",
    "            for value in parent_values:\n",
    "                for area, entity_list in entity_hierarchy:\n",
    "                    for entity in entity_list:\n",
    "                        if client_brands:\n",
    "                            for client in client_brands:\n",
    "                                for manuf in client_manuf:\n",
    "                                    key = f\"{client} | {entity} | {value}\"\n",
    "                                    ordered_keys.append(key)\n",
    "                                    futures[executor.submit(\n",
    "                                        execute_dax_query, entity, value, area, hierby, client, manuf,p12m\n",
    "                                    )] = key\n",
    "                        elif client_manuf:\n",
    "                            for manuf in client_manuf:\n",
    "                                key = f\"{manuf} | {entity} | {value}\"\n",
    "                                ordered_keys.append(key)\n",
    "                                futures[executor.submit(\n",
    "                                    execute_dax_query, entity, value, area, hierby, None, manuf, p12m\n",
    "                                )] = key\n",
    "\n",
    "            dfs_results = {}\n",
    "            for future in as_completed(futures):\n",
    "                result = future.result()\n",
    "                dfs_results.update(result)\n",
    "\n",
    "            dfs_results = {key: dfs_results[key] for key in ordered_keys if key in dfs_results}\n",
    "            if p12m:\n",
    "                 filename = f\"price_distribution_{hierby}_{'p12m' if client_brands else 'manuf_p12m'}.pkl\"\n",
    "            else:\n",
    "                 filename = f\"price_distribution_{hierby}_{'p3m' if client_brands else 'manuf_p3m'}.pkl\"\n",
    "                     \n",
    "            output_file = f\"{path}\\\\{filename}\"\n",
    "            with open(output_file, \"wb\") as f:\n",
    "                pd.to_pickle(dfs_results, f)\n",
    "            print(f\"All DataFrames for {hierby} saved to {output_file}.\")\n",
    "\n",
    "\n",
    "# Example Invocation\n",
    "if client_brands:\n",
    "    process_dax_queries(entity_hierarchy, hierarchy_levels, client_brands=client_brands,client_manuf=client_manuf,p12m=True)\n",
    "    process_dax_queries(entity_hierarchy, hierarchy_levels, client_brands=client_brands,client_manuf=client_manuf,p12m=False)\n",
    "\n",
    "process_dax_queries(entity_hierarchy, hierarchy_levels, client_manuf=client_manuf,p12m=True)\n",
    "process_dax_queries(entity_hierarchy, hierarchy_levels, client_manuf=client_manuf,p12m=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c703548",
   "metadata": {},
   "source": [
    "## Price Point Distribution By Product Scraped(Item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1671bc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def execute_dax_query(entity_name, area, hierby, client, manuf):\n",
    "#     outputdic={}\n",
    "#     if not categories or len(categories) == 0:\n",
    "#         raise ValueError(\"categories list must have at least one element\")\n",
    "\n",
    "#     columns = [\n",
    "#         \"Scraped Av. Price/Unit\", \"Scraped Av. Price/KG\"\n",
    "#     ]\n",
    "#     column_exprs = \", \".join(f'\"{col}\", COALESCE([{col}], 0)' for col in columns)\n",
    "\n",
    "#     client_filter = ''\n",
    "#     if client:\n",
    "#         client_filter = f'FILTER(Products, Products[{BrandOrTopB}] = \"{client}\")'\n",
    "\n",
    "#     manuf_filter = ''\n",
    "#     if manuf:\n",
    "#         manuf_filter = f'FILTER(Products, Products[Top Companies] = \"{manuf}\")'\n",
    "\n",
    "#     # Compose CALCULATETABLE filter arguments list (only non-empty filters)\n",
    "#     filter_args = [f'FILTER(Products, Products[Category] = \"{categories[0]}\")']\n",
    "#     if client_filter:\n",
    "#         filter_args.append(client_filter)\n",
    "#     if manuf_filter:\n",
    "#         filter_args.append(manuf_filter)\n",
    "#     filter_args.append(f'TREATAS({{ \"{entity_name}\" }}, Market[{area}])')\n",
    "#     filter_args.append('FILTER(\\'Scope\\', \\'Scope\\'[Scope] = \"Category\")')\n",
    "\n",
    "#     filters_str = \",\\n            \".join(filter_args)\n",
    "\n",
    "#     dax_query = f\"\"\"\n",
    "#         EVALUATE\n",
    "#         VAR BaseTable =\n",
    "#             CROSSJOIN(\n",
    "#                 VALUES(Calendar[End of Week]),\n",
    "#                 VALUES(Products[{prodORitem}]),\n",
    "#                 VALUES(Products[Total Size])\n",
    "#             )\n",
    "#         VAR FilteredTable =\n",
    "#             CALCULATETABLE(\n",
    "#                 BaseTable,\n",
    "#                 {filters_str}\n",
    "#             )\n",
    "#         RETURN\n",
    "#             ADDCOLUMNS(\n",
    "#                 FilteredTable,\n",
    "#                 {column_exprs}\n",
    "#             )\n",
    "#         \"\"\"\n",
    "\n",
    "#     parenttotal_dax_query = f\"\"\"\n",
    "#             EVALUATE\n",
    "#             CALCULATETABLE(\n",
    "#                 ADDCOLUMNS(\n",
    "#                     SUMMARIZECOLUMNS(\n",
    "#                         Products[{hierby}]\n",
    "\n",
    "#                     ),\n",
    "#                 {column_exprs}\n",
    "#             ),\n",
    "#             Products[Category] = \"{categories[0]}\",\n",
    "#             {client_filter}\n",
    "#             {manuf_filter}\n",
    "#             TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "#             FILTER('Scope', 'Scope'[Scope] = \"Category\")\n",
    "#         )\n",
    "#     \"\"\"\n",
    "#         # Grand total query\n",
    "#     grandtotal_query = f\"\"\"\n",
    "#         EVALUATE\n",
    "#         CALCULATETABLE(\n",
    "#             ADDCOLUMNS(\n",
    "#                 VALUES(Products[Category]),\n",
    "#                 {column_exprs}\n",
    "#             ),\n",
    "#             Products[Category] = \"{categories[0]}\",\n",
    "#             {client_filter}\n",
    "#             {manuf_filter}\n",
    "#             TREATAS({{ \"{entity_name}\" }}, Market[{area}])\n",
    "#         )\n",
    "#     \"\"\"\n",
    "\n",
    "#     try:\n",
    "#         with adodbapi.connect(conn_str) as conn:\n",
    "#             conn.timeout = 500  # Increase timeout to 300 seconds\n",
    "#             with conn.cursor() as cursor:\n",
    "#                 cursor.execute(dax_query)\n",
    "#                 columns_result = [desc[0] for desc in cursor.description]\n",
    "#                 data = cursor.fetchall()\n",
    "                \n",
    "#         with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "#             cursor.execute(parenttotal_dax_query)\n",
    "#             maintotal_columns = [desc[0] for desc in cursor.description]\n",
    "#             maintotal_data = cursor.fetchall()\n",
    "            \n",
    "#         with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "#             cursor.execute(grandtotal_query)\n",
    "#             grandtotal_columns = [desc[0] for desc in cursor.description]\n",
    "#             grandtotal_data = cursor.fetchall()\n",
    "\n",
    "#         df = pd.DataFrame(data, columns=columns_result)\n",
    "#         df.columns = df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "#         df = df.loc[~(df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "#         maintotal_df = pd.DataFrame(maintotal_data, columns=maintotal_columns)\n",
    "#         maintotal_df.columns = maintotal_df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "#         maintotal_df = maintotal_df.loc[~(maintotal_df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "        \n",
    "#         grand_tot = pd.DataFrame(grandtotal_data, columns=grandtotal_columns)\n",
    "#         grand_tot.columns = grand_tot.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "#         grand_tot = grand_tot.loc[~(grand_tot.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "        \n",
    "#         if maintotal_df.shape[1] > 1:\n",
    "#             maintotal_df.iloc[:, 0] = maintotal_df.iloc[:, 0].astype(str) + \" Total\"\n",
    "\n",
    "#         if maintotal_df.empty:\n",
    "#             outputdic[key] = maintotal_df\n",
    "#             return outputdic\n",
    "#         if not maintotal_df.empty:\n",
    "#             df_with_totals = pd.concat([df,maintotal_df], ignore_index=True)\n",
    "    \n",
    "#         if not grand_tot.empty:\n",
    "#             # Create a dict for the first two columns\n",
    "#             grand_tot[df.columns[0]] = 'Grand Total'\n",
    "#             grand_tot[df.columns[1]] = np.nan  # or pd.NA\n",
    "\n",
    "#             # Ensure all required columns exist, fill missing ones with NaN\n",
    "#             for col in df.columns:\n",
    "#                 if col not in grand_tot.columns:\n",
    "#                     grand_tot[col] = np.nan\n",
    "\n",
    "#             # Reorder columns exactly as in df\n",
    "#             grand_tot = grand_tot[df.columns]\n",
    "#             df = pd.concat([df_with_totals, grand_tot], ignore_index=True)\n",
    "\n",
    "#         outputdic[key] = df\n",
    "\n",
    "#         print(f\"Query executed successfully for {entity_name}.\")\n",
    "#     except adodbapi.DatabaseError as db_error:\n",
    "#         print(f\"Database error for {entity_name} in {area}: {db_error}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Unexpected error for {entity_name} in {area}: {e}\")\n",
    "\n",
    "#     return outputdic\n",
    "\n",
    "\n",
    "# def process_dax_queries(entity_hierarchy, hierarchy_levels, client_brands=None,client_manuf=None):\n",
    "#     with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "#         dfs_results = {} \n",
    "#         futures = {}\n",
    "#         ordered_keys = []\n",
    "#         for hierby, hier_values in hierarchy_levels:\n",
    "#             if hierby == \"Category\":\n",
    "#                     continue\n",
    "#             if isinstance(hier_values, list):\n",
    "#                 if client_brands: \n",
    "#                     for area, entity_list in entity_hierarchy:\n",
    "#                         for entity in entity_list:\n",
    "#                             for manuf in client_manuf:\n",
    "#                                 for client in client_brands: \n",
    "#                                     key = f\"{client} | {entity}\"\n",
    "#                                     ordered_keys.append(key)\n",
    "#                                     future = executor.submit(\n",
    "#                                         execute_dax_query, entity, area, hierby, client,manuf\n",
    "#                                     )\n",
    "#                                     futures[future] = key\n",
    "#                 else:\n",
    "#                     for area, entity_list in entity_hierarchy:\n",
    "#                         for entity in entity_list:\n",
    "#                             for manuf in client_manuf:\n",
    "#                                 key = f\"{manuf} | {entity}\"\n",
    "#                                 ordered_keys.append(key)\n",
    "#                                 future = executor.submit(\n",
    "#                                     execute_dax_query, entity, area, hierby, '',manuf\n",
    "#                                 )\n",
    "#                                 futures[future] = key               \n",
    "\n",
    "\n",
    "#             temp_results = {}\n",
    "#             for future in as_completed(futures):\n",
    "#                 result = future.result()\n",
    "#                 temp_results.update(result)\n",
    "\n",
    "#             for key in ordered_keys:\n",
    "#                 if key in temp_results:\n",
    "#                     dfs_results[key] = temp_results[key]\n",
    "\n",
    "#             if client_brands:\n",
    "#                 filename = f\"price_distribution_scraped_{hierby}.pkl\"\n",
    "#             else:\n",
    "#                 filename = f\"price_distribution_scraped_{hierby}_manuf.pkl\"\n",
    "\n",
    "\n",
    "#             output_file = f\"{path}\\\\{filename}\"\n",
    "#             with open(output_file, \"wb\") as f:\n",
    "#                 pd.to_pickle(dfs_results, f)\n",
    "            \n",
    "#             print(f\"All DataFrames for {hierby} saved to {output_file}.\")\n",
    "\n",
    "# if client_brands:\n",
    "#     process_dax_queries(entity_hierarchy, hierarchy_levels, client_brands=client_brands,client_manuf=client_manuf)\n",
    "# if client_manuf:\n",
    "#     process_dax_queries(entity_hierarchy, hierarchy_levels, client_manuf=client_manuf)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45219ecd",
   "metadata": {},
   "source": [
    "## Price Point Distribution By Brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cdf5d184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Pricing\\Pricing Datasets Test\\price_distribution_by_brands_category.pkl.\n"
     ]
    }
   ],
   "source": [
    "def execute_dax_query(entity_name, entity_type, area, hierby):\n",
    "    outputdic = {}\n",
    "\n",
    "    key = f\"{entity_name} | {entity_type}\" \n",
    "\n",
    "\n",
    "    columns = \"Av Price/Unit\", \"Value Share\"\n",
    "    column_exprs = \", \".join(f'\"{col}\", COALESCE([{col}], 0)' for col in columns)\n",
    "\n",
    "    dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(\n",
    "                    Products,\n",
    "                    Products[{BrandOrTopB}],\n",
    "                    Products[Pack Size]\n",
    "                ),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            Products[{hierby}] = \"{entity_type}\",\n",
    "            TREATAS({p12m_dax}, Calendar[MonthYear]),\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"{hierby}\")\n",
    "        )\n",
    "    \"\"\"\n",
    "\n",
    "    # Parent total query\n",
    "    parenttotal_dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(Products, Products[{BrandOrTopB}]),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            Products[{hierby}] = \"{entity_type}\",\n",
    "        \n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            TREATAS({p12m_dax}, Calendar[MonthYear]),\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"{hierby}\")\n",
    "        )\n",
    "    \"\"\"\n",
    "\n",
    "    grandtotal_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(VALUES(Products[Category]), {column_exprs}),\n",
    "            Products[{hierby}] = \"{entity_type}\",\n",
    "            \n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            TREATAS({p12m_dax}, Calendar[MonthYear]),\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"{hierby}\")\n",
    "\n",
    "        )\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # 1. Main query\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(dax_query)\n",
    "            data_columns = [desc[0] for desc in cursor.description]\n",
    "            data = cursor.fetchall()\n",
    "\n",
    "        # 2. Parent total query\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(parenttotal_dax_query)\n",
    "            maintotal_columns = [desc[0] for desc in cursor.description]\n",
    "            maintotal_data = cursor.fetchall()\n",
    "\n",
    "        # 3. Grand total query\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(grandtotal_query)\n",
    "            grandtotal_columns = [desc[0] for desc in cursor.description]\n",
    "            grandtotal_data = cursor.fetchall()\n",
    "\n",
    "        # Now build DataFrames safely\n",
    "        df = pd.DataFrame(data, columns=data_columns)\n",
    "        df.columns = df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        df = df.loc[~(df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "\n",
    "        maintotal_df = pd.DataFrame(maintotal_data, columns=maintotal_columns)\n",
    "        maintotal_df.columns = maintotal_df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        maintotal_df = maintotal_df.loc[~(maintotal_df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "        \n",
    "\n",
    "        grand_tot = pd.DataFrame(grandtotal_data, columns=grandtotal_columns)\n",
    "        grand_tot.columns = grand_tot.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        grand_tot = grand_tot.loc[~(grand_tot.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        if not maintotal_df.empty:\n",
    "            maintotal_df.iloc[:, 0] = maintotal_df.iloc[:, 0].astype(str) + \" Total\"\n",
    "            df = pd.concat([df,maintotal_df], ignore_index=True)\n",
    "  \n",
    "        if not grand_tot.empty:\n",
    "            grand_tot[df.columns[0]] = 'Grand Total'\n",
    "            grand_tot[df.columns[1]] = np.nan\n",
    "            grand_tot = grand_tot.reindex(columns=df.columns)\n",
    "            df = pd.concat([df, grand_tot], ignore_index=True)\n",
    "\n",
    "        outputdic[key] = df\n",
    "        print(f\"Query executed successfully for {entity_name}.\")\n",
    "\n",
    "    except adodbapi.DatabaseError as db_error:\n",
    "        print(f\"Database error for {entity_name} in {area}: {db_error}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error for {entity_name} in {area}: {e}\")\n",
    "\n",
    "    return outputdic\n",
    "\n",
    "\n",
    "def process_dax_queries(entity_hierarchy, hierarchy_levels):\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        dfs_results = {} \n",
    "        futures = {}\n",
    "        ordered_keys=[]\n",
    "        for hierby, hier_values in hierarchy_levels:\n",
    "            if isinstance(hier_values, list):\n",
    "                for value in hier_values:\n",
    "                    for area, entity_list in entity_hierarchy:\n",
    "                        for entity in entity_list:\n",
    "                            # print(hierby,value,entity)                                    \n",
    "                            key = f\"{entity} | {value}\"\n",
    "                            ordered_keys.append(key)\n",
    "                            future = executor.submit(execute_dax_query, entity,value,area, hierby)\n",
    "                            futures[future] = key\n",
    "       \n",
    "        temp_results = {}\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            temp_results.update(result)\n",
    "\n",
    "        # Insert results in original order\n",
    "        for key in ordered_keys:\n",
    "            if key in temp_results:\n",
    "                dfs_results[key] = temp_results[key]\n",
    "        filename = f\"price_distribution_by_brands_category.pkl\"\n",
    "        output_file = f\"{path}\\\\{filename}\"\n",
    "        with open(output_file, \"wb\") as f:\n",
    "            pd.to_pickle(dfs_results, f)\n",
    "        \n",
    "        print(f\"All DataFrames saved to {output_file}.\")\n",
    "\n",
    "process_dax_queries(entity_hierarchy,hierarchy_levels) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b617cd93",
   "metadata": {},
   "source": [
    "## By brand For Sec,Seg,SubSeg... (Slide 7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "10d078e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "All DataFrames for Sector saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Pricing\\Pricing Datasets Test\\price_distribution_by_brands_Sector.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.Query executed successfully for NATIONAL.\n",
      "\n",
      "Query executed successfully for NATIONAL.\n",
      "All DataFrames for Segment saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Pricing\\Pricing Datasets Test\\price_distribution_by_brands_Segment.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "All DataFrames for SubSegment saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Pricing\\Pricing Datasets Test\\price_distribution_by_brands_SubSegment.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "All DataFrames for SubCategory saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Pricing\\Pricing Datasets Test\\price_distribution_by_brands_SubCategory.\n"
     ]
    }
   ],
   "source": [
    "def execute_dax_query(entity_name, entity_type, area, hierby):\n",
    "    outputdic = {}\n",
    " \n",
    "    key = f\"{entity_name} | {entity_type}\"\n",
    "    columns = \"Av Price/Unit\", \"Value Share\"\n",
    "    column_exprs = \", \".join(f'\"{col}\", COALESCE([{col}], 0)' for col in columns)\n",
    "             \n",
    "    dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(\n",
    "                    Products,\n",
    "                    Products[{BrandOrTopB}],\n",
    "                    Products[{hierby}],\n",
    "                    Products[Pack Size]\n",
    "                ),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            Products[{direct_parent[hierby]}] = \"{entity_type}\",\n",
    "            TREATAS({p12m_dax}, Calendar[MonthYear]),\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"{direct_parent[hierby]}\")\n",
    "        )\n",
    "    \"\"\"\n",
    " \n",
    "    # Parent total query\n",
    "    parenttotal_dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(Products, Products[{BrandOrTopB}]),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            Products[{direct_parent[hierby]}] = \"{entity_type}\",\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            TREATAS({p12m_dax}, Calendar[MonthYear]),\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"Category\")\n",
    "        )\n",
    "    \"\"\"\n",
    "    itemtotal_dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(Products,Products[{BrandOrTopB}],\n",
    "                Products[{hierby}]),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            Products[{direct_parent[hierby]}] = \"{entity_type}\",\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            TREATAS({p12m_dax}, Calendar[MonthYear]),\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"Category\")\n",
    "        )\n",
    "    \"\"\"\n",
    " \n",
    "    grandtotal_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(VALUES(Products[Category]), {column_exprs}),\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            TREATAS({p12m_dax}, Calendar[MonthYear]),\n",
    " \n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}])\n",
    "        )\n",
    "    \"\"\"\n",
    " \n",
    "    try:\n",
    "        with adodbapi.connect(conn_str) as conn:\n",
    "            with conn.cursor() as cursor:\n",
    "                cursor.execute(dax_query)\n",
    "                df = pd.DataFrame(cursor.fetchall(), columns=[desc[0] for desc in cursor.description])\n",
    " \n",
    "            with conn.cursor() as cursor:\n",
    "                cursor.execute(parenttotal_dax_query)\n",
    "                maintotal_df = pd.DataFrame(cursor.fetchall(), columns=[desc[0] for desc in cursor.description])\n",
    "         \n",
    "            with conn.cursor() as cursor:\n",
    "                cursor.execute(itemtotal_dax_query)\n",
    "                itemtotal_df = pd.DataFrame(cursor.fetchall(), columns=[desc[0] for desc in cursor.description])\n",
    " \n",
    " \n",
    "            with conn.cursor() as cursor:\n",
    "                cursor.execute(grandtotal_query)\n",
    "                grand_tot = pd.DataFrame(cursor.fetchall(), columns=[desc[0] for desc in cursor.description])\n",
    " \n",
    "        for dataframe in [df,maintotal_df,itemtotal_df, grand_tot]:\n",
    "            dataframe.columns = dataframe.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "            dataframe.dropna(how='all', inplace=True)\n",
    "       \n",
    "            cols_to_check = [\"Av Price/Unit\", \"Value Share\"]\n",
    "            dataframe.drop(\n",
    "                index=dataframe[\n",
    "                    (dataframe[cols_to_check].abs() < 1e-6).all(axis=1)\n",
    "                ].index,\n",
    "                inplace=True\n",
    "            )    \n",
    "           \n",
    " \n",
    "        if not maintotal_df.empty:\n",
    "            maintotal_df.iloc[:, 0] = maintotal_df.iloc[:, 0].astype(str) + \" Total\"\n",
    "            df = pd.concat([df, maintotal_df], ignore_index=True)\n",
    "     \n",
    "        if not itemtotal_df.empty:\n",
    "            itemtotal_df.iloc[:, 1] = itemtotal_df.iloc[:, 1].astype(str) + \" Total\"\n",
    "            df = pd.concat([df, itemtotal_df], ignore_index=True)\n",
    "           \n",
    "        if not grand_tot.empty:\n",
    "            grand_tot[df.columns[0]] = 'Grand Total'\n",
    "            grand_tot[df.columns[1]] = np.nan\n",
    "            grand_tot = grand_tot.reindex(columns=df.columns)\n",
    "            df = pd.concat([df, grand_tot], ignore_index=True)\n",
    " \n",
    "        outputdic[key] = df\n",
    "        print(f\"Query executed successfully for {entity_name}.\")\n",
    " \n",
    "    except adodbapi.DatabaseError as db_error:\n",
    "        print(f\"Database error for {entity_name} in {area}: {db_error}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error for {entity_name} in {area}: {e}\")\n",
    " \n",
    "    return outputdic\n",
    " \n",
    " \n",
    "def process_dax_queries(entity_hierarchy, hierarchy_levels):\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        for hierby, hier_values in hierarchy_levels:\n",
    "            if hierby == \"Category\":\n",
    "                continue\n",
    " \n",
    "            hierarchy_dict = dict(hierarchy_levels)\n",
    "            parent_level = direct_parent.get(hierby)\n",
    "            parent_values = hierarchy_dict.get(parent_level, [])\n",
    " \n",
    "            if not parent_values:\n",
    "                continue\n",
    " \n",
    "            futures = {}\n",
    "            ordered_keys = []\n",
    " \n",
    "            # Determine prodORitem per level\n",
    " \n",
    "            for value in parent_values:\n",
    "                for area, entity_list in entity_hierarchy:\n",
    "                    for entity in entity_list:\n",
    "                        for manuf in client_manuf:\n",
    "                            key = f\"{entity} | {value}\"\n",
    "                            ordered_keys.append(key)\n",
    "                            futures[executor.submit(\n",
    "                                execute_dax_query, entity, value, area, hierby\n",
    "                            )] = key\n",
    " \n",
    "            dfs_results = {}\n",
    "            for future in as_completed(futures):\n",
    "                result = future.result()\n",
    "                dfs_results.update(result)\n",
    " \n",
    "            dfs_results = {key: dfs_results[key] for key in ordered_keys if key in dfs_results}\n",
    "           \n",
    "            filename = f\"price_distribution_by_brands_{hierby}\"\n",
    "                     \n",
    "            output_file = f\"{path}\\\\{filename}\"\n",
    "            with open(output_file, \"wb\") as f:\n",
    "                pd.to_pickle(dfs_results, f)\n",
    "            print(f\"All DataFrames for {hierby} saved to {output_file}.\")\n",
    " \n",
    " \n",
    "process_dax_queries(entity_hierarchy, hierarchy_levels)\n",
    " \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43544eb7",
   "metadata": {},
   "source": [
    "## Price Point Comparison by Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6d08f455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "All DataFrames for SubCategory saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Pricing\\Pricing Datasets Test\\price_point_by_brands_items_P12M.pkl.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for SubCategory saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Pricing\\Pricing Datasets Test\\price_point_by_brands_items_P3M.pkl.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "All DataFrames for SubCategory saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Pricing\\Pricing Datasets Test\\price_point_by_manuf_items_P12M.pkl.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for SubCategory saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Pricing\\Pricing Datasets Test\\price_point_by_manuf_items_P3M.pkl.\n"
     ]
    }
   ],
   "source": [
    "def execute_dax_query(ManuforBrand,entity_name, entity_type, area, hierby, p12m=True):\n",
    "    outputdic = {}\n",
    "\n",
    "    key = f\"{entity_name} | {entity_type}\"\n",
    "\n",
    "    if p12m:\n",
    "        timeper = p12m_dax\n",
    "        columns = [\"Value Share\", \"Gross Margin %\"]\n",
    "        row_fields = [\n",
    "            f\"Products[{ManuforBrand}]\",\n",
    "            f\"Products[{prodORitem}]\"\n",
    "        ]\n",
    "    else:\n",
    "        timeper = p3m_dax\n",
    "        columns = [\"Base Price/Unit\", \"Base Price/KG\", \"Value Sales\", \"Value Share\"]\n",
    "        row_fields = [\n",
    "            f\"Products[{ManuforBrand}]\",\n",
    "            f\"Products[{prodORitem}]\",\n",
    "            \"Products[Total Size]\"\n",
    "        ]\n",
    "\n",
    "    summarize_fields = \",\\n                        \".join(row_fields)\n",
    "    column_exprs = \",\\n                \".join(f'\"{col}\", COALESCE([{col}], 0)' for col in columns)\n",
    "\n",
    "    dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(\n",
    "                    Products,\n",
    "                    {summarize_fields}\n",
    "                ),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            Products[{hierby}] = \"{entity_type}\",\n",
    "            TREATAS({timeper}, Calendar[MonthYear]),\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"{hierby}\")\n",
    "        )\n",
    "    \"\"\"\n",
    "\n",
    "    # Parent total query\n",
    "    parenttotal_dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(Products, Products[{ManuforBrand}]),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            TREATAS({timeper}, Calendar[MonthYear]),\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "            Products[{hierby}] = \"{entity_type}\",\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"{hierby}\")\n",
    "        )\n",
    "    \"\"\"\n",
    "    itemtotal_dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(Products,Products[{prodORitem}]),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            TREATAS({timeper}, Calendar[MonthYear]),\n",
    "            Products[{hierby}] = \"{entity_type}\",\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"{hierby}\")\n",
    "        )\n",
    "    \"\"\"\n",
    "\n",
    "    grandtotal_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE( \n",
    "            ADDCOLUMNS(VALUES(Products[Category]), {column_exprs}),\n",
    "            TREATAS({timeper}, Calendar[MonthYear]),\n",
    "            Products[{hierby}] = \"{entity_type}\",\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"{hierby}\")\n",
    "            \n",
    "        )\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        \n",
    "        \n",
    "        with adodbapi.connect(conn_str) as conn:\n",
    "            with conn.cursor() as cursor:\n",
    "                cursor.execute(dax_query)\n",
    "                df = pd.DataFrame(cursor.fetchall(), columns=[desc[0] for desc in cursor.description])\n",
    "\n",
    "            with conn.cursor() as cursor:\n",
    "                cursor.execute(parenttotal_dax_query)\n",
    "                maintotal_df = pd.DataFrame(cursor.fetchall(), columns=[desc[0] for desc in cursor.description])\n",
    "          \n",
    "            with conn.cursor() as cursor:\n",
    "                cursor.execute(itemtotal_dax_query)\n",
    "                itemtotal_df = pd.DataFrame(cursor.fetchall(), columns=[desc[0] for desc in cursor.description])\n",
    "\n",
    "\n",
    "            with conn.cursor() as cursor:\n",
    "                cursor.execute(grandtotal_query)\n",
    "                grand_tot = pd.DataFrame(cursor.fetchall(), columns=[desc[0] for desc in cursor.description])\n",
    "          \n",
    "            numeric_columns = columns\n",
    "            for i, dataframe in enumerate([df, maintotal_df, itemtotal_df, grand_tot]):\n",
    "                dataframe.columns = dataframe.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "                dataframe.dropna(how='all', inplace=True)\n",
    "\n",
    "                if not dataframe.empty:\n",
    "                    # Force conversion to numeric in case some values are strings\n",
    "                    dataframe[numeric_columns] = dataframe[numeric_columns].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "                    # Now drop rows where all numeric values are 0.0\n",
    "                    mask_all_zero = (dataframe[numeric_columns].fillna(0.0) == 0.0).all(axis=1)\n",
    "                    dataframe = dataframe[~mask_all_zero]\n",
    "\n",
    "                if i == 0:\n",
    "                    df = dataframe\n",
    "                elif i == 1:\n",
    "                    maintotal_df = dataframe\n",
    "                elif i == 2:\n",
    "                    itemtotal_df = dataframe\n",
    "                else:\n",
    "                    grand_tot = dataframe\n",
    "\n",
    "\n",
    "\n",
    "        if not maintotal_df.empty:\n",
    "            maintotal_df.iloc[:, 0] = maintotal_df.iloc[:, 0].astype(str) + \" Total\"\n",
    "            df = pd.concat([df, maintotal_df], ignore_index=True)\n",
    "            \n",
    "        if not p12m:\n",
    "            if not itemtotal_df.empty:\n",
    "                itemtotal_df.iloc[:, 0] = itemtotal_df.iloc[:, 0].astype(str) + \" Total\"\n",
    "                df = pd.concat([df, itemtotal_df], ignore_index=True)\n",
    "            \n",
    "        if not grand_tot.empty:\n",
    "            grand_tot[df.columns[0]] = 'Grand Total'\n",
    "            grand_tot[df.columns[1]] = np.nan\n",
    "            grand_tot = grand_tot.reindex(columns=df.columns)\n",
    "            df = pd.concat([df, grand_tot], ignore_index=True)\n",
    "\n",
    "        outputdic[key] = df\n",
    "        print(f\"Query executed successfully for {entity_name}.\")\n",
    "\n",
    "    except adodbapi.DatabaseError as db_error:\n",
    "        print(f\"Database error for {entity_name} in {area}: {db_error}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error for {entity_name} in {area}: {e}\")\n",
    "\n",
    "    return outputdic\n",
    "\n",
    "\n",
    "def process_dax_queries(ManuforBrand,entity_hierarchy, hierarchy_levels,p12m):\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        dfs_results = {} \n",
    "        futures = {}\n",
    "        ordered_keys=[]\n",
    "        for hierby, hier_values in hierarchy_levels:\n",
    "            if isinstance(hier_values, list):\n",
    "                for value in hier_values:\n",
    "                    for area, entity_list in entity_hierarchy:\n",
    "                        for entity in entity_list:\n",
    "                            # print(hierby,value,entity)                                    \n",
    "                            key = f\"{entity} | {value}\"\n",
    "                            ordered_keys.append(key)\n",
    "                            future = executor.submit(execute_dax_query,ManuforBrand, entity,value,area, hierby,p12m)\n",
    "                            futures[future] = key\n",
    "\n",
    "        dfs_results = {}\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            dfs_results.update(result)\n",
    "\n",
    "        dfs_results = {key: dfs_results[key] for key in ordered_keys if key in dfs_results}\n",
    "        if ManuforBrand ==f'{BrandOrTopB}' :\n",
    "            if p12m:\n",
    "                    filename = f\"price_point_by_brands_items_P12M.pkl\"\n",
    "            else:\n",
    "                    filename = f\"price_point_by_brands_items_P3M.pkl\"\n",
    "        else:\n",
    "            if p12m:\n",
    "                    filename = f\"price_point_by_manuf_items_P12M.pkl\"\n",
    "            else:\n",
    "                    filename = f\"price_point_by_manuf_items_P3M.pkl\"           \n",
    "        output_file = f\"{path}\\\\{filename}\"\n",
    "        with open(output_file, \"wb\") as f:\n",
    "            pd.to_pickle(dfs_results, f)\n",
    "        print(f\"All DataFrames for {hierby} saved to {output_file}.\")\n",
    "\n",
    "\n",
    "# Example Invocation\n",
    "process_dax_queries(f'{BrandOrTopB}',entity_hierarchy, hierarchy_levels,p12m=True)\n",
    "process_dax_queries(f'{BrandOrTopB}',entity_hierarchy, hierarchy_levels,p12m=False)\n",
    "process_dax_queries(f'{ManufOrTopC}',entity_hierarchy, hierarchy_levels,p12m=True)\n",
    "process_dax_queries(f'{ManufOrTopC}',entity_hierarchy, hierarchy_levels,p12m=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50029e3",
   "metadata": {},
   "source": [
    "## Price Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "88086e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.Query executed successfully for Walmart.\n",
      "\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Bj's And Sam's.\n",
      "All DataFrames saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Pricing\\Pricing Datasets Test\\price_correlation_P3Y.pkl.\n"
     ]
    }
   ],
   "source": [
    "def execute_dax_query(entity_name,entity_type, area, hierby):\n",
    "    outputdic = {}\n",
    "    key =  f\"{entity_type} | {entity_name}\"\n",
    "\n",
    "    columns = [\n",
    "        \"Volume Share\",'Av Price/KG','Value Share'\n",
    "    ]\n",
    "    \n",
    "    column_exprs = \", \".join(f'\"{col}\", COALESCE([{col}], 0)' for col in columns)\n",
    "\n",
    "    # Main query\n",
    "    dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                CROSSJOIN(\n",
    "                    DISTINCT(Products[{BrandOrTopB}]),\n",
    "                    DISTINCT(Calendar[QuarterStart]),\n",
    "                    DISTINCT(Calendar[End of Week])\n",
    "                ),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            TREATAS({p36m_dax}, Calendar[MonthYear]),\n",
    "            Products[{hierby}] = \"{entity_type}\",\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"{hierby}\")\n",
    "        )\n",
    "\n",
    "    \"\"\"\n",
    "    fristcoltot_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(\n",
    "                    products,\n",
    "                    Products[{BrandOrTopB}]\n",
    "                ),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            TREATAS({p36m_dax}, Calendar[MonthYear]),\n",
    "            Products[{hierby}] = \"{entity_type}\",\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"{hierby}\")\n",
    "        )\n",
    "\n",
    "    \"\"\"\n",
    "    secondcoltot_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(\n",
    "                    Calendar,\n",
    "                    Calendar[QuarterStart]\n",
    "                ),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            TREATAS({p36m_dax}, Calendar[MonthYear]),\n",
    "            Products[{hierby}] = \"{entity_type}\",\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"Category\")\n",
    "        )\n",
    "    \"\"\"\n",
    "    # Grand total query (not grouped by BrandOrTopB, just Category)\n",
    "    grandtotal_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(\n",
    "                    Products,\n",
    "                    Products[Category]\n",
    "                ),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            TREATAS({p36m_dax}, Calendar[MonthYear]),\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"Category\")\n",
    "        )\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(dax_query)\n",
    "            columns = [desc[0] for desc in cursor.description]\n",
    "            data = cursor.fetchall()\n",
    "            \n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(fristcoltot_query)\n",
    "            fristttotal_columns = [desc[0] for desc in cursor.description]\n",
    "            fristtotal_data = cursor.fetchall()       \n",
    "             \n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(secondcoltot_query)\n",
    "            sectotal_columns = [desc[0] for desc in cursor.description]\n",
    "            sectotal_data = cursor.fetchall()  \n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(grandtotal_query)\n",
    "            grandtotal_columns = [desc[0] for desc in cursor.description]\n",
    "            grandtotal_data = cursor.fetchall()\n",
    "            \n",
    "            df = pd.DataFrame(data, columns=columns)\n",
    "            df.columns = df.columns.str.replace(r'.*\\[|\\]', '', regex=True)         \n",
    "            df = df.loc[~(df.select_dtypes(include='number') == 0).all(axis=1)]  # Remove zero rows\n",
    "\n",
    "            maintotal_df = pd.DataFrame(fristtotal_data, columns=fristttotal_columns)\n",
    "            maintotal_df.columns = maintotal_df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "            maintotal_df = maintotal_df.loc[~(maintotal_df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "            \n",
    "            sectotal_df = pd.DataFrame(sectotal_data, columns=sectotal_columns)\n",
    "            sectotal_df.columns = sectotal_df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "            sectotal_df = sectotal_df.loc[~(sectotal_df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "        \n",
    "            grand_tot = pd.DataFrame(grandtotal_data, columns=grandtotal_columns)\n",
    "            grand_tot.columns = grand_tot.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "            grand_tot = grand_tot.loc[~(grand_tot.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "\n",
    "            if maintotal_df.shape[1] > 1:\n",
    "                maintotal_df.iloc[:, 0] = maintotal_df.iloc[:, 0].astype(str) + \" Total\"\n",
    "            \n",
    "            if sectotal_df.shape[1] > 1:\n",
    "                first_col = sectotal_df.columns[0]\n",
    "                # Ensure datetime\n",
    "                sectotal_df[first_col] = pd.to_datetime(sectotal_df[first_col], errors='coerce')\n",
    "\n",
    "                # Format date and append ' total'\n",
    "                sectotal_df[first_col] = sectotal_df[first_col].dt.strftime('%Y-%m-%d') + \" Total\"\n",
    "\n",
    "\n",
    "\n",
    "            if sectotal_df.empty:\n",
    "                outputdic[key] = sectotal_df\n",
    "                return outputdic\n",
    "\n",
    "            if maintotal_df.empty:\n",
    "                outputdic[key] = maintotal_df\n",
    "                return outputdic\n",
    "        \n",
    "            \n",
    "            if not maintotal_df.empty:\n",
    "                df_with_totals = pd.concat([df,maintotal_df], ignore_index=True)\n",
    "                \n",
    "            df_with_totals['QuarterStart'] = pd.to_datetime(df_with_totals['QuarterStart']).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "            if not sectotal_df.empty:\n",
    "                dfsec_with_totals = pd.concat([df_with_totals,sectotal_df], ignore_index=True)\n",
    "\n",
    "            \n",
    "\n",
    "            grand_tot = pd.DataFrame(grandtotal_data, columns=grandtotal_columns)\n",
    "            grand_tot.columns = grand_tot.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "            grand_tot = grand_tot.loc[~(grand_tot.select_dtypes(include='number') == 0).all(axis=1)]  # Remove zero rows\n",
    "          \n",
    "            if not grand_tot.empty:\n",
    "                # Set first column value to 'Grand Total', second column to NaN\n",
    "                grand_tot[df.columns[0]] = 'Grand Total'\n",
    "                grand_tot[df.columns[1]] = np.nan  # or pd.NA\n",
    "\n",
    "                # Restrict grand_tot to only columns that appear in df\n",
    "                grand_tot = grand_tot[df.columns.intersection(grand_tot.columns)]\n",
    "\n",
    "            # Concatenate all together\n",
    "            df = pd.concat([dfsec_with_totals, grand_tot], ignore_index=True)\n",
    "\n",
    "            outputdic[key] = df\n",
    "\n",
    "\n",
    "            \n",
    "            print(f\"Query executed successfully for {entity_name}.\")\n",
    "    except adodbapi.DatabaseError as db_error:\n",
    "        print(f\"Database error for {entity_name} in {area}: {db_error}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error for {entity_name} in {area}: {e}\")\n",
    "\n",
    "    return outputdic\n",
    "\n",
    "\n",
    "\n",
    "def process_dax_queries(entity_hierarchy, hierarchy_levels):\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        dfs_results = {} \n",
    "        futures = {}\n",
    "        ordered_keys=[]\n",
    "        for hierby, hier_values in hierarchy_levels:\n",
    "            if isinstance(hier_values, list):\n",
    "                for value in hier_values:\n",
    "                    for area, entity_list in entity_hierarchy:\n",
    "                        for entity in entity_list:\n",
    "                            # print(hierby,value,entity)                                    \n",
    "                            key = f\"{value} | {entity}\"\n",
    "                            ordered_keys.append(key)\n",
    "                            future = executor.submit(execute_dax_query, entity,value,area, hierby)\n",
    "                            futures[future] = key\n",
    "       \n",
    "        temp_results = {}\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            temp_results.update(result)\n",
    "\n",
    "        # Insert results in original order\n",
    "        for key in ordered_keys:\n",
    "            if key in temp_results:\n",
    "                dfs_results[key] = temp_results[key]\n",
    "        filename =  f\"price_correlation_P3Y.pkl\"\n",
    "\n",
    "        output_file = f\"{path}\\\\{filename}\"\n",
    "        with open(output_file, \"wb\") as f:\n",
    "            pd.to_pickle(dfs_results, f)\n",
    "        \n",
    "        print(f\"All DataFrames saved to {output_file}.\")\n",
    "\n",
    "process_dax_queries(entity_hierarchy,hierarchy_levels) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f1314c",
   "metadata": {},
   "source": [
    "# Read "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4f13d2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# loaded_data = {}\n",
    "# datasets_path =r\"C:\\Users\\aleaa\\Desktop\\Slide-Automate Extraction\\Pricing\\Pricing Datasets NewEX\\\\\"\n",
    "# datasets = os.listdir(datasets_path)    \n",
    "# for d in datasets:\n",
    "#     with open(datasets_path+d, 'rb') as handle:\n",
    "#         globals()[d.split('.')[0]] = pd.read_pickle(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f50239",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
