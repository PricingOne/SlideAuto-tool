{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_path = 'C:\\\\Users\\\\Ali Salem\\\\Desktop\\\\App_Update\\\\static/files\\\\parameters.xlsx'\n",
    "pricing_cbc_path = 'C:\\\\Users\\\\Ali Salem\\\\Desktop\\\\App_Update\\\\Pricing CBC/Pricing CBC Datasets Test\\\\Walmart CBC Extract incl BrandxSector Sourcing.xlsx'\n",
    "slides_name = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "%run \"{os.path.dirname(os.getcwd())}\\general_functions\\generalFunctions.ipynb\" #container\n",
    "\n",
    "%run \"{os.getcwd()}\\Pricing CBC Replacement Function.ipynb\" #container\n",
    "%run \"{os.path.dirname(os.getcwd())}\\general_functions\\Extracting Data Functions.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pptx import Presentation\n",
    "import win32com.client as win32\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re\n",
    "import sys \n",
    "win32c = win32.constants\n",
    "import shutil\n",
    "import os\n",
    "from win32com import client\n",
    "from pptx.util import Pt\n",
    "from pptx.enum.text import PP_ALIGN\n",
    "from pptx.chart.data import CategoryChartData,XyChartData,BubbleChartData\n",
    "import win32com.client\n",
    "from pptx.dml.color import RGBColor\n",
    "from pptx.util import Inches,Cm\n",
    "from pptx.chart.data import ChartData\n",
    "from pptx.enum.chart import XL_TICK_LABEL_POSITION\n",
    "from pptx.enum.chart import XL_LABEL_POSITION\n",
    "from win32com.client import constants as xl\n",
    "from pptx.enum.chart import XL_CHART_TYPE\n",
    "import pickle\n",
    "from pptx.enum.dml import MSO_LINE,MSO_LINE_DASH_STYLE\n",
    "import time\n",
    "import itertools\n",
    "from pptx.enum.chart import XL_AXIS_CROSSES,XL_LEGEND_POSITION\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from collections import defaultdict \n",
    "from io import BytesIO \n",
    "import math\n",
    "from pptx.enum.shapes import MSO_SHAPE\n",
    "from pptx.oxml.xmlchemy import OxmlElement\n",
    "from pptx.enum.text import PP_ALIGN, MSO_ANCHOR\n",
    "from pptx.oxml.ns import qn\n",
    "import adodbapi\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ali Salem\\Desktop\\App_Update\\parameters.xlsx\n"
     ]
    }
   ],
   "source": [
    "filename = 'parameters.xlsx'\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.path.dirname(os.getcwd())\n",
    "\n",
    "# Construct the full path to the file\n",
    "f_path = os.path.join(current_dir, filename)\n",
    "print(f_path)\n",
    "#xls = pd.ExcelFile(f_path)\n",
    "parm = pd.read_excel(f_path, sheet_name='Pricing_CBC')\n",
    "fields = dict(zip(parm['Field'],parm['Value']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "server = \"powerbi://api.powerbi.com/v1.0/myorg/\"+ fields['server']\n",
    "dataset_name = fields['dataset_name']\n",
    "f_name = fields['f_name']\n",
    "directory = r\"Pricing CBC Datasets Test\"\n",
    "f_name = os.path.join(directory, f_name) \n",
    "file_path = os.path.join(os.getcwd(), f_name)\n",
    "\n",
    "market =  list(set(fields['market'].split(','))-set(['']))\n",
    "channel =  list(set(fields['channel'].split(','))-set(['']))\n",
    "region = list(set(fields['region'].split(','))-set(['']))\n",
    "client_manuf = list(set(fields['client_manuf'].split(','))-set(['']))\n",
    "categories = list(set(fields['categories'].split(','))-set(['']))\n",
    "\n",
    "pricingPlus = fields['pricingPlus'].replace(\"'\", \"\")\n",
    "pricingMinus = fields['pricingMinus'].replace(\"'\", \"\")\n",
    "\n",
    "currency = fields['currency']\n",
    "decimals = fields['decimals']\n",
    "sign = fields['sign']\n",
    "data_source = fields['data_source']\n",
    "marketsub= \" \".join(market+channel+region)\n",
    "\n",
    "end_date= fields['end_date']\n",
    "past_12_months = pd.date_range(end=end_date, periods=12, freq='ME').strftime('%b-%y').tolist()\n",
    "p12m_dax = \"{\" + \", \".join(f'\"{date}\"' for date in past_12_months) + \"}\"\n",
    "\n",
    "entity_hierarchy = [\n",
    "    (\"Region\", region),\n",
    "    (\"Channel\", channel),\n",
    "    (\"Market\", market)\n",
    "]\n",
    "hierarchy_levels = [\n",
    "    (\"Category\", categories),\n",
    "]\n",
    "\n",
    "colorList = [\n",
    "    RGBColor(0, 160, 151),\n",
    "    RGBColor(126, 202, 196),\n",
    "    RGBColor(0, 108, 109),\n",
    "    RGBColor(146, 208, 80),\n",
    "    RGBColor(0, 176, 80),\n",
    "    RGBColor(184, 182, 13),\n",
    "    RGBColor(0, 142, 135),   # New color close to the first and third colors\n",
    "    RGBColor(131, 218, 212)  # New color close to the second color\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ### Parameters -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "BrandelRun= fields['BrandelRun']\n",
    "BrandSourcingRun= fields['BrandSourcingRun']\n",
    "ProductSourcingRun= fields['ProductSourcingRun']\n",
    "PERun= fields['PERun']\n",
    "RevenueResponseRun= fields['RevenueResponseRun']\n",
    "SERun= fields['SERun']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = r\"C:\\Users\\Salma Hany\\Documents\\Slide-Automate\\Pricing CBC Slide Duplicate\\Pricing CBC Datasets\\Edgewell Mexico CBC Extract.xlsx\"\n",
    "\n",
    "# market = []\n",
    "# channel=[]\n",
    "# region=['Walmart']\n",
    "# marketsub=\" \".join(market+channel+region)\n",
    "# client_manuf = [\"Edgewell Personal Care\", \"PBG\"]\n",
    "# categories = [\"Manual Shave Men\"]\n",
    "\n",
    "# pricingPlus = \"+15%\"\n",
    "# pricingMinus = \"-10%\"\n",
    "\n",
    "# colorList = [\n",
    "#     RGBColor(0, 160, 151),\n",
    "#     RGBColor(126, 202, 196),\n",
    "#     RGBColor(0, 108, 109),\n",
    "#     RGBColor(146, 208, 80),\n",
    "#     RGBColor(0, 176, 80),\n",
    "#     RGBColor(184, 182, 13),\n",
    "#     RGBColor(0, 142, 135),   # New color close to the first and third colors\n",
    "#     RGBColor(131, 218, 212)  # New color close to the second color\n",
    "# ]\n",
    "# currency = '$' \n",
    "# sign = \"before\"\n",
    "# decimals = 2\n",
    "# data_source = \"DATA SOURCE: Consumer Test | July 2025\"\n",
    "\n",
    "# server = \"powerbi://api.powerbi.com/v1.0/myorg/Edgewell\"\n",
    "# dataset_name = \"Edgewell US Male Dataset\"\n",
    "# entity_hierarchy = [\n",
    "#     (\"Region\", region),\n",
    "#     (\"Channel\", channel),\n",
    "#     (\"Market\", market)\n",
    "# ]\n",
    "# hierarchy_levels = [\n",
    "#     (\"Category\", categories),\n",
    "# ]\n",
    "\n",
    "# path=os.path.join(os.getcwd(),\"Pricing CBC Datasets\")\n",
    "\n",
    "# Format months for DAX\n",
    "# end_date = \"2025-05-01\"\n",
    "# past_12_months = pd.date_range(end=end_date, periods=12, freq='ME').strftime('%b-%y').tolist()\n",
    "# p12m_dax = \"{\" + \", \".join(f'\"{date}\"' for date in past_12_months) + \"}\"\n",
    "\n",
    "# BrandelRun= True\n",
    "# BrandSourcingRun=True\n",
    "# ProductSourcingRun=True\n",
    "# PERun=True\n",
    "# RevenueResponseRun=True\n",
    "# SERun=True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ### PBI extract for WoB and GM -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PERun: \n",
    "    client_manuf_dax = \", \".join(f'\"{x}\"' for x in client_manuf)\n",
    "    conn_str = f\"Provider=MSOLAP.8;Data Source={server};Initial Catalog={dataset_name};Timeout=600;\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Walmart.\n",
      "All DataFrames saved to Pricing CBC Datasets Test/Pricing_CBC.pkl.\n"
     ]
    }
   ],
   "source": [
    "def execute_dax_query(entity_name, area):\n",
    "    outputdic = {}\n",
    "    # Construct filter condition dynamically\n",
    "    columns = [\"Product Names WoB %\",\"Gross Margin %\"]\n",
    "    column_exprs = \", \".join(f'\"{col}\", COALESCE([{col}], 0)' for col in columns)\n",
    "    dax_query = f\"\"\"\n",
    "    EVALUATE\n",
    "    CALCULATETABLE(\n",
    "        ADDCOLUMNS(\n",
    "            SUMMARIZE(\n",
    "                ProductNames,\n",
    "                ProductNames[ProductName]\n",
    "                ),\n",
    "            {column_exprs}\n",
    "        ),\n",
    "            ProductNames[SourceName]= \"Consumerresearchname\",\n",
    "            TREATAS(\n",
    "                {{\"{categories[0]}\"}} , \n",
    "                Products[Category]\n",
    "            ),\n",
    "            TREATAS(\n",
    "                {{{client_manuf_dax}}},\n",
    "                Products[Top Companies]\n",
    "            ),\n",
    "            TREATAS(\n",
    "                {p12m_dax},\n",
    "                Calendar[MonthYear]\n",
    "            ),\n",
    "            TREATAS(\n",
    "                {{\"{entity_name}\"}},\n",
    "                Market[{area}]\n",
    "            )\n",
    "    )  \n",
    "    \"\"\"\n",
    " \n",
    "    try:\n",
    "        with adodbapi.connect(conn_str) as conn:\n",
    "            with conn.cursor() as cursor:\n",
    "                cursor.execute(dax_query)\n",
    "                columns = [desc[0] for desc in cursor.description]\n",
    "                data = cursor.fetchall()\n",
    " \n",
    "                df = pd.DataFrame(data, columns=columns)\n",
    "                df.columns = df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "                df = df.loc[~(df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "                outputdic = df\n",
    "                print(f\"Query executed successfully for {entity_name}.\")\n",
    "    except adodbapi.DatabaseError as db_error:\n",
    "        print(f\"Database error for {entity_name}: {db_error}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error for {entity_name}: {e}\")\n",
    "   \n",
    "    return outputdic\n",
    " \n",
    "# Process data concurrently\n",
    "def process_dax_queries(entity_hierarchy):\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "       \n",
    "        dfs_results = {}\n",
    "        futures = []\n",
    "        \n",
    "        for area, entity_list in entity_hierarchy:\n",
    "            for entity in entity_list:\n",
    "                future = executor.submit(execute_dax_query, entity, area)\n",
    "                futures.append((future, entity, area))\n",
    "\n",
    "        for future, entity, area in futures:\n",
    "            df = future.result()\n",
    "            dfs_results[(entity)] = df\n",
    "\n",
    "        # Save results\n",
    "        output_path = \"Pricing CBC Datasets Test/Pricing_CBC.pkl\"\n",
    "        pd.to_pickle(dfs_results, output_path)\n",
    "        print(f\"All DataFrames saved to {output_path}.\")\n",
    " \n",
    "\n",
    "if PERun:\n",
    "    process_dax_queries(entity_hierarchy) \n",
    "    pbi = pd.read_pickle(\"Pricing CBC Datasets Test/Pricing_CBC.pkl\")\n",
    "    for key, df in pbi.items():\n",
    "        #if not df.empty:\n",
    "        df.rename(columns={'Product Names WoB %':'WOB%', 'Gross Margin %':'GM%'}, inplace=True)\n",
    "        #pbi[key] = df\n",
    "        #else:\n",
    "        #    print(f\"Warning: DataFrame for {key} is empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ### Read Data in Excel -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "if BrandelRun:\n",
    "    sheet_name='Brand elasticity'\n",
    "    brandElasticity_ori=pd.read_excel(file_path,sheet_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_columns(df):\n",
    "    column_mapping = {\n",
    "        'product': 'Product',\n",
    "        'Product': 'Product',\n",
    "        'SKU': 'Product',\n",
    "        # Add more mappings as necessary\n",
    "    }\n",
    "    \n",
    "    # Apply the column mapping\n",
    "    df.rename(columns=column_mapping, inplace=True)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "if BrandSourcingRun:\n",
    "    plusbranding = pd.read_excel(file_path,\"Plus Brand Sourcing\").rename(columns={'Unnamed: 0': 'Product'})\n",
    "    Minusbranding = pd.read_excel(file_path,\"Minus Brand Sourcing\").rename(columns={'Unnamed: 0': 'Product'})\n",
    "    plusbrandingfair = pd.read_excel(file_path,\"Plus Brand Sourcing Fair share\").rename(columns={'Unnamed: 0': 'Product'})\n",
    "    Minusbrandingfair = pd.read_excel(file_path,\"Minus Brand Sourcing Fair share\").rename(columns={'Unnamed: 0': 'Product'})\n",
    "    Minusbrandingfair = Minusbrandingfair[[col for col in Minusbrandingfair.columns if 'Unnamed' not in col]]\n",
    "    plusbranding = normalize_columns(plusbranding)\n",
    "    Minusbranding = normalize_columns(Minusbranding)\n",
    "    plusbrandingfair = normalize_columns(plusbrandingfair)\n",
    "    Minusbrandingfair = normalize_columns(Minusbrandingfair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ProductSourcingRun: \n",
    "    plus = pd.read_excel(file_path,\"Plus Sourcing\")\n",
    "    Minus = pd.read_excel(file_path,\"Minus Sourcing\")\n",
    "    plusfair = pd.read_excel(file_path,\"Plus Sourcing Fair share\")\n",
    "    Minusfair = pd.read_excel(file_path,\"Minus Sourcing Fair share\")\n",
    "    plus = normalize_columns(plus)\n",
    "    Minus = normalize_columns(Minus)\n",
    "    plusfair = normalize_columns(plusfair)\n",
    "    Minusfair = normalize_columns(Minusfair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PERun: \n",
    "    pe = pd.read_excel(file_path,\"PE\")\n",
    "    group_list = pe['Grouping'].unique().tolist()\n",
    "    pe = normalize_columns(pe) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ### Data Cleaning -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PERun:\n",
    "    from openpyxl import load_workbook\n",
    "    from openpyxl.utils import get_column_letter\n",
    "\n",
    "    # Load the workbook and select the sheet\n",
    "    wb = load_workbook(file_path, data_only=True)\n",
    "    sheet = wb['PE']\n",
    "    pe[\"Share P5 - Base Share\"] = pd.Series([float(0)] * len(pe))\n",
    "    pe[\"PE P5-P6\"] = pd.Series([float(0)] * len(pe))\n",
    "    # Check for yellow cells and adjust shares\n",
    "    for row in range(1, len(pe)+2):\n",
    "        for col in range(1, pe.shape[1]+ 2):\n",
    "            cell = sheet.cell(row=row, column=col)\n",
    "            if cell.fill.fgColor.rgb == 'FFFFFF00':\n",
    "                column_name = sheet.cell(1,col).value \n",
    "                col_num=int(column_name[1])\n",
    "                \n",
    "                for i in range(5,col_num-1,-1):\n",
    "                    if i == col_num:\n",
    "                        pe[f\"Share P{i} - Base Share\"][row-2] =0\n",
    "                        pe[f\"PE P{i}-P{i+1}\"][row-2]=0\n",
    "                    else:\n",
    "                        pe[f\"Share P{i} - Base Share\"][row-2] = pe[f\"Share P{i-1} - Base Share\"][row-2]\n",
    "                        pe[f\"PE P{i}-P{i+1}\"][row-2] =  pe[f\"PE P{i-1}-P{i}\"][row-2]\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PERun:\n",
    "    # Create the cleaned column\n",
    "    pbi[marketsub][\"ProductNameUpp\"] = pbi[marketsub][\"ProductName\"].str.upper().apply(lambda x: ' '.join(str(x).split()))\n",
    "\n",
    "    # Also create in pe\n",
    "    pe[\"ProductUpp\"] = pe[\"Product\"].str.upper().apply(lambda x: ' '.join(str(x).split()))\n",
    "\n",
    "    # Now merge on these cleaned columns\n",
    "    pepbijoin = pd.merge(\n",
    "        pe,\n",
    "        pbi[marketsub][['ProductNameUpp', 'WOB%', 'GM%']],\n",
    "        how='left',\n",
    "        left_on='ProductUpp',\n",
    "        right_on='ProductNameUpp'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RevenueResponseRun: \n",
    "    from openpyxl import load_workbook\n",
    "\n",
    "    # Load the workbook and select the specific sheet\n",
    "    wb = load_workbook(file_path, data_only=True)\n",
    "    sheet = wb['Revenue Response']\n",
    "\n",
    "    # Create an empty color mask matrix\n",
    "    bgcol = np.empty([sheet.max_row, sheet.max_column], dtype=object, order='C')\n",
    "\n",
    "    # Cycle through all cells to get colors\n",
    "    for row in range(sheet.max_row):\n",
    "        for column in range(sheet.max_column):\n",
    "            cell = sheet.cell(row + 1, column + 1)\n",
    "            \n",
    "            # Check if the cell font color is not red (hex 'FFFF0000'), if it is, set the value to empty\n",
    "            color_in_hex_index = cell.value if cell.font.color.rgb != 'FFFF0000' else ''\n",
    "            \n",
    "            # Assign the value to the bgcol array\n",
    "            bgcol[row, column] = str(color_in_hex_index)\n",
    "\n",
    "    # Convert the bgcol array to a pandas DataFrame\n",
    "    colormask = pd.DataFrame(bgcol)\n",
    "\n",
    "    # Print the shape of the DataFrame\n",
    "    #print(colormask.shape)\n",
    "\n",
    "    # Adjust columns names and content\n",
    "    # Define ranges for slicing the DataFrame\n",
    "    if len([]) == 0:\n",
    "        SheetContentRanges = range(0, colormask.shape[0]), range(0, colormask.shape[1])\n",
    "\n",
    "    # Slice the DataFrame according to defined ranges\n",
    "    colormask = colormask.iloc[SheetContentRanges]\n",
    "\n",
    "    # Reset index and update column names\n",
    "    colormask.reset_index(drop=True, inplace=True)\n",
    "    colormask.columns = colormask.iloc[0]  # Set the first row as the header\n",
    "    colormask = colormask.iloc[1:, :]  # Remove the first row (header) and the first column\n",
    "\n",
    "    # Drop specific columns\n",
    "    colormask = colormask.drop(columns=['Select SKUs\\n(with \"x\")', 'Possible steps (d/u)', 'Base case share'])\n",
    "\n",
    "    # Display the final DataFrame\n",
    "    #print(colormask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_revenue(colormask):\n",
    "    \"\"\"\n",
    "    Clean revenue data from a DataFrame.\n",
    " \n",
    "    Parameters:\n",
    "    - colormask: The DataFrame containing revenue data.\n",
    " \n",
    "    Returns:\n",
    "    - dfList: A list of cleaned DataFrames.\n",
    "    \"\"\"\n",
    "    dfList = []  # Initialize an empty list to store cleaned DataFrames\n",
    "    colormask = colormask.loc[:, ~colormask.columns.duplicated(keep='last')]  # Remove duplicate columns\n",
    "    for i in range(1, colormask.shape[0] + 1):\n",
    "        df = colormask.iloc[i - 1:i]  # Extract a single row DataFrame\n",
    "        df = df.replace(\"\", np.nan)  # Replace empty cells with NaN\n",
    "        df.dropna(axis=1, inplace=True)  # Drop columns with NaN values\n",
    "        dfList.append(df)  # Append cleaned DataFrame to the list\n",
    "    return dfList  # Return the list of cleaned DataFrames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RevenueResponseRun: dfList = cleaning_revenue(colormask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SERun: \n",
    "    sheet_name='SE'\n",
    "    SE=pd.read_excel(file_path,sheet_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SE_clean(data):\n",
    "    new_rows = []\n",
    "\n",
    "    for _, row in data.iterrows():\n",
    "        # First row: s1 and s2 \n",
    "        new_rows.append({'Product':row['Product'],\n",
    "        'Base Size': row['S1 total'], \n",
    "        'New Size': row['S2 total'],\n",
    "        'Size Change':row['size S2 - Base size'],\n",
    "        'Volume Index': row['Volume Index 2'],\n",
    "        'Value Index': row['Value Index 2'],\n",
    "        'Gross Profit':row['Gross Profit 2'],\n",
    "        'Size Elasticity': row['SE 2'], \n",
    "        'SCD PE':row['SCD PE 2'], \n",
    "        'PCD PE':row['PCD PE 2']})\n",
    "        # Second row: s1 and s3 \n",
    "        new_rows.append({'Product':row['Product'],\n",
    "        'Base Size': row['S1 total'],\n",
    "        'New Size': row['S3 total'], \n",
    "        'Size Change':row['size S3 - Base size'],\n",
    "        'Volume Index': row['Volume Index 3'],\n",
    "        'Value Index': row['Value Index 3'],\n",
    "        'Gross Profit':row['Gross Profit 3'],\n",
    "        'Size Elasticity': row['SE 3'], \n",
    "        'SCD PE':row['SCD PE 3'], \n",
    "        'PCD PE':row['PCD PE 3']})\n",
    "        \n",
    "    # Convert the list of new rows back into a DataFrame\n",
    "    new_df = pd.DataFrame(new_rows)\n",
    "    new_df = new_df.dropna(thresh=new_df.shape[1] - 7).reset_index(drop=True)\n",
    "    return new_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SERun: SE_cleaned = SE_clean(SE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ## Duplication Stage -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5]\n",
      "[4, 16, 48, 17, 48, 5]\n",
      "['Brand Elasticity', 'Brand Sourcing Analysis', 'Product Sourcing Analysis', 'Price Elasticity Curve', 'Revenue Response Analysis', 'Size Elasticity']\n",
      "6\n",
      "6\n",
      "6\n",
      "138\n"
     ]
    }
   ],
   "source": [
    "index = [0,1 ,2,3,4,5 ]\n",
    "\n",
    "duplication = [(len(brandElasticity_ori.columns)+3)//6 if BrandelRun else 0,len(plusbranding.keys())-1 if BrandSourcingRun else 0,len(plus.keys())-1 if ProductSourcingRun else 0, len(group_list) if PERun else 0, colormask.shape[0] if RevenueResponseRun else 0, math.ceil(len(SE_cleaned)/20) if SERun else 0]\n",
    "section_names = [\"Brand Elasticity\" if BrandelRun else \"\",\"Brand Sourcing Analysis\" if BrandSourcingRun else 0,\"Product Sourcing Analysis\" if ProductSourcingRun else 0,\"Price Elasticity Curve\" if PERun else 0,\"Revenue Response Analysis\" if RevenueResponseRun else 0,\"Size Elasticity\" if SERun else 0]\n",
    "\n",
    "path = os.getcwd() + '//Pricing CBC base Oct 2024.pptx'\n",
    "new_pre = os.getcwd() + '//slide duplicated.pptx'\n",
    "\n",
    "print(index)\n",
    "print(duplication)\n",
    "print(section_names)\n",
    "print(len(index))\n",
    "print(len(duplication))\n",
    "print(len(section_names))\n",
    "print(sum(duplication))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slideDuplication(index,duplication,section_names,path,new_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(slides_name) >0:\n",
    "    valid_sections = [(i, s) for i, s in enumerate(section_names) if s is not None]\n",
    "    indices = [i for i, s in valid_sections if any(sub.lower() == s.lower() for sub in slides_name)]\n",
    "    filtered_section_names = [section_names[i] for i in indices]\n",
    "    filtered_duplication = [duplication[i] for i in indices]\n",
    "    filtered_index = [index[i] for i in indices]\n",
    "    slideDuplication(filtered_index,filtered_duplication,filtered_section_names,path,new_pre)\n",
    "else:\n",
    "    slideDuplication(index,duplication,section_names,path,new_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "prs = Presentation(new_pre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ### Slide 1 -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "if BrandelRun:\n",
    "    try:\n",
    "        if 0 in filtered_index:\n",
    "            dup_list = filtered_duplication\n",
    "            run_slide = True\n",
    "        else:\n",
    "            run_slide = False\n",
    "    except NameError:\n",
    "        dup_list = duplication\n",
    "        run_slide = True \n",
    "\n",
    "    if run_slide:\n",
    "        for sli in range(dup_list[0]):\n",
    "            start=sli*6\n",
    "            end=start+3\n",
    "            brandElasticity= brandElasticity_ori[brandElasticity_ori.columns[start:end]]\n",
    "            brandElasticity=brandElasticity.dropna()\n",
    "            \n",
    "            BrandElasticity(prs,brandElasticity,1,categories[0],position=0+sli) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ### Slide 2 -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "if BrandSourcingRun:\n",
    "    try:\n",
    "        if 1 in filtered_index:\n",
    "            dup_list = filtered_duplication\n",
    "            run_slide = True\n",
    "        else:\n",
    "            run_slide = False\n",
    "    except NameError:\n",
    "        dup_list = duplication\n",
    "        run_slide = True \n",
    "\n",
    "    if run_slide:\n",
    "        BrandSourcingAnalysis(prs,plusbranding,Minusbranding,plusbrandingfair,Minusbrandingfair,dup_list[1],position=sum(dup_list[:1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ### Slide 3 -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ProductSourcingRun:\n",
    "    try:\n",
    "        if 2 in filtered_index:\n",
    "            dup_list = filtered_duplication\n",
    "            run_slide = True\n",
    "        else:\n",
    "            run_slide = False\n",
    "    except NameError:\n",
    "        dup_list = duplication\n",
    "        run_slide = True \n",
    "\n",
    "    if run_slide:\n",
    "        ProductSourcingAnalysis(prs,plus,Minus,plusfair,Minusfair,dup_list[2],position=sum(dup_list[:2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ### Slide 4 -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PERun:\n",
    "    try:\n",
    "        if 3 in filtered_index:\n",
    "            dup_list = filtered_duplication\n",
    "            run_slide = True\n",
    "        else:\n",
    "            run_slide = False\n",
    "    except NameError:\n",
    "        dup_list = duplication\n",
    "        run_slide = True \n",
    "\n",
    "    if run_slide:\n",
    "        PriceElasticityCurve(prs, pepbijoin, group_list, dup_list[3],position=sum(dup_list[:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ### Slide 5 -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated value axis to min: 60.0, max: 130.0\n",
      "Updated value axis to min: 60.0, max: 140.0\n",
      "Updated value axis to min: 50.0, max: 130.0\n",
      "Updated value axis to min: 60.0, max: 150.0\n",
      "Updated value axis to min: 70.0, max: 150.0\n",
      "Updated value axis to min: 50.0, max: 130.0\n",
      "Updated value axis to min: 60.0, max: 150.0\n",
      "Updated value axis to min: 50.0, max: 160.0\n",
      "Updated value axis to min: 60.0, max: 140.0\n",
      "Updated value axis to min: 60.0, max: 130.0\n",
      "Updated value axis to min: 50.0, max: 140.0\n",
      "Updated value axis to min: 50.0, max: 140.0\n",
      "Updated value axis to min: 60.0, max: 130.0\n",
      "Updated value axis to min: 60.0, max: 130.0\n",
      "Updated value axis to min: 60.0, max: 130.0\n",
      "Updated value axis to min: 60.0, max: 140.0\n",
      "Updated value axis to min: 70.0, max: 160.0\n",
      "Updated value axis to min: 60.0, max: 160.0\n",
      "Updated value axis to min: 60.0, max: 170.0\n",
      "Updated value axis to min: 70.0, max: 160.0\n",
      "Updated value axis to min: 60.0, max: 130.0\n",
      "Updated value axis to min: 70.0, max: 140.0\n",
      "Updated value axis to min: -1800.0, max: 1900.0\n",
      "Updated value axis to min: -3600.0, max: 3700.0\n",
      "Updated value axis to min: -20.0, max: 240.0\n",
      "Updated value axis to min: 60.0, max: 150.0\n",
      "Updated value axis to min: 50.0, max: 150.0\n",
      "Updated value axis to min: 60.0, max: 130.0\n",
      "Updated value axis to min: 60.0, max: 160.0\n",
      "Updated value axis to min: 60.0, max: 170.0\n",
      "Updated value axis to min: 50.0, max: 140.0\n",
      "Updated value axis to min: 60.0, max: 160.0\n",
      "Updated value axis to min: 60.0, max: 170.0\n",
      "Updated value axis to min: 60.0, max: 160.0\n",
      "Updated value axis to min: 60.0, max: 130.0\n",
      "Updated value axis to min: 60.0, max: 130.0\n",
      "Updated value axis to min: 60.0, max: 140.0\n",
      "Updated value axis to min: 70.0, max: 130.0\n",
      "Updated value axis to min: 70.0, max: 130.0\n",
      "Updated value axis to min: 70.0, max: 140.0\n",
      "Updated value axis to min: 60.0, max: 140.0\n",
      "Updated value axis to min: 70.0, max: 130.0\n",
      "Updated value axis to min: 50.0, max: 130.0\n",
      "Updated value axis to min: 60.0, max: 130.0\n",
      "Updated value axis to min: 70.0, max: 150.0\n",
      "Updated value axis to min: 70.0, max: 140.0\n",
      "Updated value axis to min: 60.0, max: 130.0\n",
      "Updated value axis to min: 70.0, max: 140.0\n"
     ]
    }
   ],
   "source": [
    "if RevenueResponseRun: \n",
    "    try:\n",
    "        if 4 in filtered_index:\n",
    "            dup_list = filtered_duplication\n",
    "            run_slide = True\n",
    "        else:\n",
    "            run_slide = False\n",
    "    except NameError:\n",
    "        dup_list = duplication\n",
    "        run_slide = True \n",
    "\n",
    "    if run_slide:\n",
    "        revenue_response(prs,dfList, dup_list[4],position=sum(dup_list[:4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ### Slide 6 -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Slide 1\n",
      "Processing Slide 2\n",
      "Processing Slide 3\n",
      "Processing Slide 4\n",
      "Processing Slide 5\n"
     ]
    }
   ],
   "source": [
    "if SERun: \n",
    "    try:\n",
    "        if 5 in filtered_index:\n",
    "            dup_list = filtered_duplication\n",
    "            run_slide = True\n",
    "        else:\n",
    "            run_slide = False\n",
    "    except NameError:\n",
    "        dup_list = duplication\n",
    "        run_slide = True \n",
    "\n",
    "    if run_slide:\n",
    "        SE_Slide(prs,SE_cleaned,dup_list[5],position=sum(dup_list[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ### Final -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "today = str(date.today())\n",
    "outputPath=os.getcwd() + \"\\\\Pricing CBC \"+marketsub+\" output \"+today+\".pptx\"\n",
    "prs.save(outputPath)\n",
    "app = win32.Dispatch(\"PowerPoint.Application\")\n",
    "#presentation = app.Presentations.Open(outputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shape_number_partial(shapes, text):\n",
    "    \"\"\"\n",
    "    Get the index of a shape with specific text.\n",
    "\n",
    "    Parameters:\n",
    "    - shapes (list): A list of shapes in a slide.\n",
    "    - text (str): The text to search for in the shapes.\n",
    "\n",
    "    Returns:\n",
    "    - int or None: The index of the shape if found, None otherwise.\n",
    "    \"\"\"\n",
    "    for shape_index in range(len(shapes)):\n",
    "        if shapes[shape_index].has_text_frame:  # Check if the shape has a text frame\n",
    "            if text in shapes[shape_index].text:  # Compare the shape's text with the given text\n",
    "                return shape_index  # Return the index of the shape if text matches\n",
    "    return None  # Return None if the shape with the given text is not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getChartNum(shapes):\n",
    "    \"\"\"\n",
    "    Extractcharts from a collection of shapes in a PowerPoint slide.\n",
    "\n",
    "    Parameters:\n",
    "    - shapes: A collection of shapes in a PowerPoint slide.\n",
    "\n",
    "    Returns:\n",
    "    - shape_index: shape number in selecton pane\n",
    "    \"\"\"\n",
    "    shape_index = []\n",
    "    for shape_index in range(len(shapes)):\n",
    "        if shapes[shape_index].has_chart:\n",
    "            return shape_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "if RevenueResponseRun:  \n",
    "    sls = [sl for sl in prs.slides][sum(duplication[:4]):sum(duplication[:5])]\n",
    "    shapes = prs.slides[sum(duplication[:4])].shapes\n",
    "    subtitleNumber = get_shape_number_partial(shapes, \"Revenue Response Curve by Price Point\")\n",
    "    chartNumber = getChartNum(shapes)\n",
    "    print(chartNumber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RevenueResponseRun:  \n",
    "    prod_names=[i.shapes[subtitleNumber].text.split(\" | \")[1] for i in sls]\n",
    "    prod_names=list(set(prod_names))\n",
    "    len(prod_names)\n",
    "    filter_dps=pd.read_excel(file_path, sheet_name=\"PE\")\n",
    "    filter_dps=filter_dps.melt(id_vars=[\"Product\"],value_vars=[\"P1\",\"P2\",\"P3\",\"P4\",\"P5\"])\n",
    "    filter_dps=filter_dps.groupby(\"Product\")[\"value\"].unique().reset_index()\n",
    "    filter_dps=dict(zip(filter_dps[\"Product\"],filter_dps[\"value\"]))\n",
    "    len(filter_dps.keys())\n",
    "    set(prod_names)==set(filter_dps.keys())\n",
    "    charts=[s.shapes[chartNumber] for s in sls]\n",
    "    [float(i[0].replace(currency,\"\")) for i in charts[0].chart.plots[0].categories.flattened_labels]\n",
    "    dfs=[]\n",
    "    for num,chart in enumerate(charts):\n",
    "        temp=pd.DataFrame()\n",
    "        for i in chart.chart.series:\n",
    "            temp[i.name]=i.values\n",
    "        temp[\"x_axis\"]=[float(j[0].replace(currency,\"\")) for j in chart.chart.plots[0].categories.flattened_labels]\n",
    "        prod_name=sls[num].shapes[subtitleNumber].text.split(\" | \")[1]\n",
    "        temp=temp[(temp[\"x_axis\"].isin(filter_dps[prod_name]))|(temp[\"Volume Ix\"]==100)]\n",
    "        dfs.append(temp)\n",
    "    len(dfs)       \n",
    "\n",
    "    org_dfs=[]\n",
    "    for num,chart in enumerate(charts):\n",
    "        temp=pd.DataFrame()\n",
    "        for i in chart.chart.series:\n",
    "            temp[i.name]=i.values\n",
    "        temp[\"x_axis\"]=[float(j[0].replace(currency,\"\")) for j in chart.chart.plots[0].categories.flattened_labels]\n",
    "        prod_name=sls[num].shapes[subtitleNumber].text.split(\" | \")[1]\n",
    "        org_dfs.append(temp)\n",
    "    len(org_dfs)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RevenueResponseRun:  \n",
    "  final_res=[]\n",
    "  dfs=org_dfs\n",
    "  for num_index,df in enumerate(dfs) :\n",
    "    df[\"ZeroPoint\"]=np.where((df[\"Volume Ix\"]==100),-1,np.nan)\n",
    "    df[\"ZeroPoint\"]=df[\"ZeroPoint\"].bfill()\n",
    "    df[\"ZeroPoint\"]=df[\"ZeroPoint\"].fillna(1)\n",
    "  \n",
    "\n",
    "    df[\"Value Ix delta\"]=df[\"Value Ix\"].diff() \n",
    "    df[\"Gross Profit Ix delta\"]=df[\"Gross Profit Ix\"].diff()\n",
    "\n",
    "    df[\"Value Ix delta\"]=df[\"Value Ix delta\"]*df[\"ZeroPoint\"]\n",
    "    df[\"Gross Profit Ix delta\"]=df[\"Gross Profit Ix delta\"]*df[\"ZeroPoint\"]\n",
    "\n",
    "\n",
    "\n",
    "    df[\"color\"]=np.select([(df[\"Value Ix delta\"]>0)&(df[\"Gross Profit Ix delta\"]>0)&(df[\"Value Ix\"]>100),\n",
    "                          (df[\"Value Ix delta\"]>0)&(df[\"Gross Profit Ix delta\"]<0)&(df[\"Value Ix\"]>100),\n",
    "                          (df[\"Value Ix delta\"]>0)&(df[\"Gross Profit Ix delta\"]==0)&(df[\"Value Ix\"]>100),\n",
    "                          (df[\"Value Ix delta\"]<0)&(df[\"Gross Profit Ix delta\"]>0)&(df[\"Value Ix\"]>100),\n",
    "                          (df[\"Value Ix delta\"]<0)&(df[\"Gross Profit Ix delta\"]<0)&(df[\"Value Ix\"]>100),\n",
    "                          (df[\"Value Ix delta\"]<0)&(df[\"Gross Profit Ix delta\"]==0)&(df[\"Value Ix\"]>100),\n",
    "                          (df[\"Value Ix delta\"]==0)&(df[\"Gross Profit Ix delta\"]>0)&(df[\"Value Ix\"]>100),\n",
    "                          (df[\"Value Ix delta\"]==0)&(df[\"Gross Profit Ix delta\"]<0)&(df[\"Value Ix\"]>100),\n",
    "                          (df[\"Value Ix delta\"]==0)&(df[\"Gross Profit Ix delta\"]==0)&(df[\"Value Ix\"]>100),\n",
    "                          (df[\"Value Ix delta\"]>0)&(df[\"Gross Profit Ix delta\"]>0)&(df[\"Value Ix\"]<=100),\n",
    "                          (df[\"Value Ix delta\"]>0)&(df[\"Gross Profit Ix delta\"]<0)&(df[\"Value Ix\"]<=100),\n",
    "                          (df[\"Value Ix delta\"]>0)&(df[\"Gross Profit Ix delta\"]==0)&(df[\"Value Ix\"]<=100),\n",
    "                          (df[\"Value Ix delta\"]<0)&(df[\"Gross Profit Ix delta\"]>0)&(df[\"Value Ix\"]<=100),\n",
    "                          (df[\"Value Ix delta\"]<0)&(df[\"Gross Profit Ix delta\"]<0)&(df[\"Value Ix\"]<=100),\n",
    "                          (df[\"Value Ix delta\"]<0)&(df[\"Gross Profit Ix delta\"]==0)&(df[\"Value Ix\"]<=100),\n",
    "                          (df[\"Value Ix delta\"]==0)&(df[\"Gross Profit Ix delta\"]>0)&(df[\"Value Ix\"]<=100),\n",
    "                          (df[\"Value Ix delta\"]==0)&(df[\"Gross Profit Ix delta\"]<0)&(df[\"Value Ix\"]<=100),\n",
    "                          (df[\"Value Ix delta\"]==0)&(df[\"Gross Profit Ix delta\"]==0)&(df[\"Value Ix\"]<=100)],\n",
    "                          [\n",
    "  \"Green|Profit & Revenue Increase\",\n",
    "  \"Yellow|Revenue Increase & Profit Dilution\",\n",
    "  \"Yellow|Revenue Increase & Flat Profit\",\n",
    "  \"Yellow|Revenue Dilution & Profit Increase\",\n",
    "  \"Red|Revenue & Profit Dilution\",\n",
    "  \"Red|Revenue Dilution & Flat Profit\",\n",
    "  \"Yellow|Flat Revenue & Profit Increase\",\n",
    "  \"Red|Flat Revenue & Profit Dilution\",\n",
    "  \"Yellow|Flat Revenue & Profit\",\n",
    "  \"Yellow|Profit & Revenue Increase\",\n",
    "  \"Yellow|Revenue Increase & Profit Dilution\",\n",
    "  \"Yellow|Revenue Increase & Flat Profit\",\n",
    "  \"Yellow|Revenue Dilution & Profit Increase\",\n",
    "  \"Red|Revenue & Profit Dilution\",\n",
    "  \"Red|Revenue Dilution & Flat Profit\",\n",
    "  \"Yellow|Flat Revenue & Profit Increase\",\n",
    "  \"Red|Flat Revenue & Profit Dilution\",\n",
    "  \"Yellow|Flat Revenue & Profit\",\n",
    "  ],\"\")\n",
    "    \n",
    "\n",
    "    df=org_dfs[num_index].merge(df,how=\"left\")\n",
    "\n",
    "    df=df.reset_index(drop=True)\n",
    "    df=df.iloc[1:,:]\n",
    "    df[\"color\"]=df[\"color\"].bfill()\n",
    "    df[\"grouping_same_color\"]=DetermineShapeNumber(df[\"color\"].apply(lambda x : x.split(\"|\")[0]).to_list())\n",
    "    groups=df.groupby(\"grouping_same_color\")[\"color\"].apply(lambda x : pd.Series(x).mode()[0]).reset_index()\n",
    "    df=df.drop(columns=[\"color\"]).merge(groups)\n",
    "    \n",
    "    df[\"shape_number\"]=DetermineShapeNumber(df[\"color\"].to_list())\n",
    "    df[\"color_num\"]=df[\"color\"]+\"_\"+df[\"shape_number\"].astype(str)\n",
    "    dist=df[\"color_num\"].to_list()\n",
    "    full_lenght=len(dist)\n",
    "    dist_list=[[0,0,dist[0] ]]\n",
    "    current=dist[0]\n",
    "    for i in range(len(dist)):\n",
    "      if current == dist[i]:\n",
    "        dist_list[-1][1]+=1\n",
    "      else :\n",
    "        dist_list.append([i,i+1,dist[i]])\n",
    "        current=dist[i]\n",
    "    res=[(int(round(i[0]/len(dist),2)*100),int(round(i[1]/len(dist),2)*100),i[2].split(\"_\")[0]) for i in dist_list ]  \n",
    "    final_res.append(res)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RevenueResponseRun:  \n",
    "    full_width=Cm(15.7)\n",
    "    left_start=Cm(2.7)\n",
    "    for num,slide in enumerate(sls):\n",
    "        for sh in final_res[num]:\n",
    "            added_shape=AddRectangle(slide,left_start + (sh[0]/100) * full_width, ((sh[1]-sh[0])/100) * full_width,sh[2] )\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RevenueResponseRun:  \n",
    "    prs.save(outputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_order = []\n",
    "current_max = len(new_order)\n",
    "group = pepbijoin['Product'][pepbijoin['Grouping'] == group_list[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = win32.Dispatch(\"PowerPoint.Application\")\n",
    "presentation = app.Presentations.Open(outputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New slide order (1-based): [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 134, 135, 136, 137, 138, 69, 86, 21, 91, 26, 70, 87, 22, 88, 23, 89, 24, 90, 25, 94, 29, 95, 30, 96, 31, 71, 92, 27, 93, 28, 72, 97, 32, 101, 36, 73, 98, 33, 99, 34, 100, 35, 106, 41, 74, 102, 37, 103, 38, 104, 39, 105, 40, 107, 42, 75, 108, 43, 109, 44, 76, 110, 45, 112, 47, 114, 49, 77, 111, 46, 113, 48, 78, 115, 50, 116, 51, 79, 117, 52, 119, 54, 80, 118, 53, 81, 120, 55, 124, 59, 125, 60, 82, 121, 56, 123, 58, 128, 63, 129, 64, 83, 122, 57, 126, 61, 127, 62, 84, 130, 65, 85, 131, 66, 132, 67, 133, 68]\n",
      "Reordered 138 slides successfully\n",
      "Removing 6 sections...\n",
      "All sections removed.\n",
      "Reordered presentation saved as: c:\\Users\\Ali Salem\\Desktop\\App_Update\\Pricing CBC\\Pricing CBC Walmart reordered output 2025-08-31.pptx\n",
      "Slide reordering complete! All formatting preserved.\n"
     ]
    }
   ],
   "source": [
    "# app = win32.Dispatch(\"PowerPoint.Application\")\n",
    "# presentation = app.Presentations.Open(outputPath)\n",
    "\n",
    "new_order = []\n",
    "current_max = len(new_order)\n",
    "\n",
    "if BrandelRun: \n",
    "    current_max += duplication[0]\n",
    "    new_order.extend(range(len(new_order) + 1, current_max + 1))\n",
    "if BrandSourcingRun: \n",
    "    current_max += duplication[1]\n",
    "    new_order.extend(range(len(new_order) + 1, current_max + 1))\n",
    "if SERun: \n",
    "    total = sum(duplication)\n",
    "    count = duplication[5]\n",
    "    new_order.extend(range(total - count + 1, total + 1))\n",
    "\n",
    "#Reordering PE, RR, Sourcing\n",
    "if PERun:\n",
    "    startPE = sum([duplication[0],duplication[1],duplication[2]])\n",
    "    startSou= sum([duplication[0],duplication[1]])\n",
    "    startRR= sum([duplication[0],duplication[1],duplication[2],duplication[3]])\n",
    "    for group_num,group_name in enumerate(group_list):\n",
    "        new_order.append(startPE+group_num+1)\n",
    "        group = pepbijoin['Product'][pepbijoin['Grouping'] == group_name]\n",
    "        for i, name in enumerate(group):\n",
    "            row_numbers = []\n",
    "            for i, df in enumerate(dfList):\n",
    "                if (df['Product'] == name).any():\n",
    "                    row_numbers.append(i) \n",
    "            if RevenueResponseRun: new_order.append(startRR + row_numbers[0] + 1)\n",
    "            if ProductSourcingRun: new_order.append(startSou + row_numbers[0] + 1)\n",
    "if PERun== False:\n",
    "    startSou=sum([duplication[0],duplication[1]])\n",
    "    startRR= sum([duplication[0],duplication[1],duplication[2],duplication[3]])\n",
    "    if RevenueResponseRun: new_order.extend(range(startRR, duplication[4]))\n",
    "    if ProductSourcingRun: new_order.extend(range(startRR, duplication[2]))\n",
    "\n",
    "\n",
    "print(f\"New slide order (1-based): {new_order}\")\n",
    "\n",
    "def reorder_slides_in_place(presentation, new_order):\n",
    "    total_slides = presentation.Slides.Count\n",
    "    \n",
    "    slides_temp = []\n",
    "    \n",
    "    for slide_index in new_order:\n",
    "        if slide_index <= total_slides:\n",
    "            original_slide = presentation.Slides(slide_index)\n",
    "            original_slide.Copy()\n",
    "            time.sleep(0.5) \n",
    "            new_slide = presentation.Slides.Paste(presentation.Slides.Count + 1)\n",
    "            slides_temp.append(new_slide)\n",
    "    \n",
    "    for i in range(total_slides, 0, -1):\n",
    "        presentation.Slides(i).Delete()\n",
    "    \n",
    "    print(f\"Reordered {len(slides_temp)} slides successfully\")\n",
    "    return presentation\n",
    "\n",
    "# Reorder the slides\n",
    "presentation = reorder_slides_in_place(presentation, new_order)\n",
    "\n",
    "reordered_outputPath = os.getcwd() + \"\\\\Pricing CBC \"+marketsub+\" reordered output \"+today+\".pptx\"\n",
    "def remove_all_sections(presentation):\n",
    "    try:\n",
    "        section_count = presentation.SectionProperties.Count\n",
    "        print(f\"Removing {section_count} sections...\")\n",
    "        for i in range(section_count, 0, -1):\n",
    "            presentation.SectionProperties.Delete(i, False)  # False = keep slides\n",
    "        print(\"All sections removed.\")\n",
    "        return presentation\n",
    "    except Exception as e:\n",
    "        print(f\"Error removing sections: {e}\")\n",
    "\n",
    "presentation = remove_all_sections(presentation)\n",
    "\n",
    "# Add Section names\n",
    "presentation.SectionProperties.AddBeforeSlide(1,section_names[0])\n",
    "presentation.SectionProperties.AddBeforeSlide(duplication[0]+1,section_names[1] )\n",
    "presentation.SectionProperties.AddBeforeSlide(duplication[0]+duplication[1]+1,section_names[5] )\n",
    "presentation.SectionProperties.AddBeforeSlide(duplication[0]+duplication[1]+duplication[5]+1,\"PE, Revenue Responses, Sourcing by product\")\n",
    "\n",
    "\n",
    "presentation.SaveAs(reordered_outputPath)\n",
    "\n",
    "print(f\"Reordered presentation saved as: {reordered_outputPath}\")\n",
    "print(\"Slide reordering complete! All formatting preserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
