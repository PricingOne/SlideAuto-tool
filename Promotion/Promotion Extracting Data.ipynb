{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d78169e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_path = 'C:\\\\Users\\\\Ali Salem\\\\Desktop\\\\App_Update\\\\static/files\\\\parameters.xlsx'\n",
    "slides_name = ['Top 20 promotions', 'Top 20 promotions CLIENT ONLY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4a04a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import adodbapi\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b7cc9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ali Salem\\Desktop\\App_Update\\parameters.xlsx\n"
     ]
    }
   ],
   "source": [
    "filename = 'parameters.xlsx'\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.path.dirname(os.getcwd())\n",
    "\n",
    "# Construct the full path to the file\n",
    "f_path = os.path.join(current_dir, filename)\n",
    "print(f_path)\n",
    "#xls = pd.ExcelFile(f_path)\n",
    "parm = pd.read_excel(f_path, sheet_name='Promotion')\n",
    "fields = dict(zip(parm['Field'],parm['Value']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9b27bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "server = \"powerbi://api.powerbi.com/v1.0/myorg/\"+ fields['server']\n",
    "dataset_name = fields['f_name']\n",
    "f_name = os.getcwd()+\"/\"+fields['f_name']+\".xlsx\"\n",
    "\n",
    "client_manuf = list(set(fields['client_manuf'].split(','))-set(['']))\n",
    "client_brands = list(set(fields['client_brands'].split(','))-set(['']))\n",
    "\n",
    "decimals = fields['decimals']\n",
    "sign = fields['sign']\n",
    "currency = fields['currency']\n",
    "currency = ' '+ currency if sign.lower() == 'after' else  currency + ' ' \n",
    "\n",
    "categories = list(set(fields['categories'].split(','))-set(['']))\n",
    "sectors=list(set(fields['sectors'].split(','))-set(['']))\n",
    "segments=list(set(fields['segments'].split(','))-set(['']))\n",
    "subsegments=list(set(fields['subsegments'].split(','))-set(['']))\n",
    "subcategories=list(set(fields['subcategories'].split(','))-set(['']))\n",
    "\n",
    "national=fields['national']\n",
    "customareas=fields['customareas']\n",
    "areas = list(set(fields['areas'].split(','))-set(['']))+[customareas]\n",
    "\n",
    "regions_RET = list(set(fields['regions_RET'].split(','))-set(['']))\n",
    "channels_RET = list(set(fields['channels_RET'].split(','))-set(['']))\n",
    "market_RET=list(set(fields['market_RET'].split(','))-set(['']))\n",
    "\n",
    "regions_CHAN=list(set(fields['regions_CHAN'].split(','))-set(['']))\n",
    "channels_CHAN=list(set(fields['channels_CHAN'].split(','))-set(['']))\n",
    "market_CHAN=list(set(fields['market_CHAN'].split(','))-set(['']))\n",
    "\n",
    "regions_CUST=list(set(fields['regions_CUST'].split(','))-set(['']))\n",
    "channels_CUST=list(set(fields['channels_CUST'].split(','))-set(['']))\n",
    "market_CUST=list(set(fields['market_CUST'].split(','))-set(['']))\n",
    "\n",
    "data_source=fields['data_source']\n",
    "years=list(set(fields['years'].split(','))-set(['']))\n",
    "years = {int(y) for y in fields['years'].split(',') if y}\n",
    "end_date=fields['end_date']\n",
    "\n",
    "ManufOrTopC = fields['ManufOrTopC']\n",
    "BrandOrTopB = fields['BrandOrTopB']\n",
    "prodORitem = fields['prodORitem']\n",
    "\n",
    "National=[\"NATIONAL\"]if national else []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b15af1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "past_12_months = pd.date_range(end=end_date, periods=12, freq='ME').strftime('%b-%y').tolist()\n",
    "past_3_months = pd.date_range(end=end_date, periods=3, freq='ME').strftime('%b-%y').tolist()\n",
    "past_36_months = pd.date_range(end=end_date, periods=36, freq='ME').strftime('%b-%y').tolist()\n",
    "\n",
    "regions = regions_RET + regions_CHAN + regions_CUST\n",
    "channels = channels_RET + channels_CHAN + channels_CUST\n",
    "markets = market_RET + market_CHAN + market_CUST\n",
    "\n",
    "entity_hierarchy = [\n",
    "    (\"Area\",National),\n",
    "    (\"Region\", regions),\n",
    "    (\"Channel\", channels),\n",
    "    (\"Market\", markets)\n",
    "]\n",
    "hierarchy_levels = [\n",
    "    (\"Category\", categories),\n",
    "    (\"Sector\", sectors),\n",
    "    (\"Segment\", segments),\n",
    "    (\"SubSegment\", subsegments),\n",
    "    (\"SubCategory\", subcategories)\n",
    " \n",
    "]\n",
    "direct_parent = {\"Sector\":\"Category\",\n",
    "                \"Segment\":\"Sector\",\n",
    "                \"SubSegment\":\"Segment\", \n",
    "                \"SubCategory\":\"Segment\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f3690b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "p12m_dax = \"{\" + \", \".join(f'\"{date}\"' for date in past_12_months) + \"}\"\n",
    "p3m_dax = \"{\" + \", \".join(f'\"{date}\"' for date in past_3_months) + \"}\"\n",
    "p36m_dax = \"{\" + \", \".join(f'\"{date}\"' for date in past_36_months) + \"}\"\n",
    "normalized = fields['normalized']\n",
    "promo_type = fields['promo_type']\n",
    "display_share = fields['display_share']\n",
    "feature_share = fields['feature_share']\n",
    "\n",
    "path=os.path.join(os.getcwd(),\"Promotion Datasets Test\")\n",
    "\n",
    "conn_str = f\"Provider=MSOLAP.8;Data Source={server};Initial Catalog={dataset_name};Timeout=900;\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4f0d903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ManufOrTopC =\"Top Companies\"\n",
    "# BrandOrTopB = \"Top Brands\"\n",
    "# prodORitem=\"SKU\"\n",
    "\n",
    "# client_manuf = [\"Edgewell Personal Care\"]\n",
    "# client_brands = [\"Schick\", \"Equate\", \"Cremo\"]\n",
    "\n",
    "# categories = [\"Manual Shave Men\"]\n",
    "# sectors = [\"System\",\"Disposables\"]\n",
    "# segments = [\"Razors\", \"Refills\", \"Disposables\"]\n",
    "# subsegments= []\n",
    "# subcategories= []\n",
    "\n",
    "# decimals = 2\n",
    "# sign = \"Before\"\n",
    "# currency = '$'\n",
    "# currency = ' '+ currency if sign.lower() == 'after' else  currency + ' '\n",
    "\n",
    "# customareas=''\n",
    "# national = False\n",
    "# areas = [\"RETAILER\"]\n",
    "# regions_RET  =[\"Bj's And Sam's\",\"Walmart\"]\n",
    "# channels_RET = []\n",
    "# market_RET = []\n",
    "\n",
    "# regions_CHAN = []\n",
    "# channels_CHAN = []\n",
    "# market_CHAN = []\n",
    "\n",
    "# regions_CUST = []\n",
    "# channels_CUST = []\n",
    "# market_CUST = []\n",
    " \n",
    "# data_source = \"DATA SOURCE: Trade Panel/Retailer Data | Ending March  2025\"\n",
    "# end_date = \"2025-04-01\"\n",
    "# years = {2023,2024,2025}\n",
    "\n",
    "# past_12_months = pd.date_range(end=end_date, periods=12, freq='ME').strftime('%b-%y').tolist()\n",
    "# past_3_months = pd.date_range(end=end_date, periods=3, freq='ME').strftime('%b-%y').tolist()\n",
    "# past_36_months = pd.date_range(end=end_date, periods=36, freq='ME').strftime('%b-%y').tolist()\n",
    "\n",
    "# National=[\"NATIONAL\"]if national else []\n",
    "# regions = regions_RET + regions_CHAN + regions_CUST\n",
    "# channels = channels_RET + channels_CHAN + channels_CUST\n",
    "# markets = market_RET + market_CHAN + market_CUST\n",
    "# brands_only = True  # Get the Data of SKU Share by brands level only\n",
    "\n",
    "# entity_hierarchy = [\n",
    "#     (\"Area\",National),\n",
    "#     (\"Region\", regions),\n",
    "#     (\"Channel\", channels),\n",
    "#     (\"Market\", markets)\n",
    "# ]\n",
    "# hierarchy_levels = [\n",
    "#     (\"Category\", categories),\n",
    "#     (\"Sector\", sectors),\n",
    "#     (\"Segment\", segments),\n",
    "#     (\"SubSegment\", subsegments),\n",
    "#     (\"SubCategory\", subcategories)\n",
    " \n",
    "# ]\n",
    "# direct_parent = {\"Sector\":\"Category\",\n",
    "#                 \"Segment\":\"Sector\",\n",
    "#                 \"SubSegment\":\"Segment\", \n",
    "#                 \"SubCategory\":\"Segment\"}\n",
    "# server = \"powerbi://api.powerbi.com/v1.0/myorg/Edgewell\"\n",
    "# dataset_name = \"Edgewell US Male Dataset\"\n",
    "# p12m_dax = \"{\" + \", \".join(f'\"{date}\"' for date in past_12_months) + \"}\"\n",
    "# p3m_dax = \"{\" + \", \".join(f'\"{date}\"' for date in past_3_months) + \"}\"\n",
    "# p36m_dax = \"{\" + \", \".join(f'\"{date}\"' for date in past_36_months) + \"}\"\n",
    "# normalized = True\n",
    "# promo_type = True\n",
    "# display_share = False  # True if Available\n",
    "# feature_share = False\n",
    "\n",
    "# print(p12m_dax)\n",
    "# path=os.path.join(os.getcwd(),\"Promotion Datasets NewEX\")\n",
    "\n",
    "# conn_str = f\"Provider=MSOLAP.8;Data Source={server};Initial Catalog={dataset_name};Timeout=900;\"\n",
    "# print(conn_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50f7c6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ali Salem\\AppData\\Local\\Temp\\ipykernel_13492\\2956073908.py:1: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  month_years =  pd.date_range(end=end_date , periods=12, freq='M').strftime('%b-%y').tolist()\n"
     ]
    }
   ],
   "source": [
    "month_years =  pd.date_range(end=end_date , periods=12, freq='M').strftime('%b-%y').tolist()\n",
    "regions = regions_RET + regions_CHAN + regions_CUST\n",
    "channels = channels_RET + channels_CHAN + channels_CUST\n",
    "markets = market_RET + market_CHAN + market_CUST\n",
    "promo_col = []\n",
    "promo_col = promo_col+['Display Share'] if display_share else promo_col\n",
    "promo_col = promo_col+['Feature Share'] if feature_share else promo_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8dabf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5580c82",
   "metadata": {},
   "source": [
    "## By Promo Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a84f2b",
   "metadata": {},
   "source": [
    "#### Brands and Promo Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa533ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Manual Shave Men Sam's Corp.\n",
      "Query executed successfully for Disposables Sam's Corp.\n",
      "Query executed successfully for Refills NATIONAL.\n",
      "Query executed successfully for Manual Shave Men NATIONAL.\n",
      "Query executed successfully for Disposables NATIONAL.\n",
      "Query executed successfully for Razors NATIONAL.\n",
      "Query executed successfully for System NATIONAL.\n",
      "Query executed successfully for Disposables NATIONAL.\n",
      "Query executed successfully for Refills Sam's Corp.\n",
      "Query executed successfully for System Sam's Corp.\n",
      "Query executed successfully for Manual Shave Men Walmart.\n",
      "Query executed successfully for Razors Sam's Corp.\n",
      "Query executed successfully for Disposables Sam's Corp.\n",
      "Query executed successfully for Refills Walmart.\n",
      "Query executed successfully for Disposables Walmart.\n",
      "Query executed successfully for System Walmart.\n",
      "Query executed successfully for Razors Walmart.\n",
      "Query executed successfully for Disposables Walmart.\n"
     ]
    }
   ],
   "source": [
    "def execute_dax_query(BrandorManuf, entity_name, area, hierby, entity_type):\n",
    "    outputdic = {}\n",
    "    columns = [\"Value Share\", \"Promo Sales\"]\n",
    "    column_exprs = \",\\n            \".join(f'\"{col}\", COALESCE(CALCULATE([{col}]), 0)' for col in columns)\n",
    "\n",
    "    dax_query = f\"\"\"\n",
    "    EVALUATE\n",
    "    CALCULATETABLE(\n",
    "        ADDCOLUMNS(\n",
    "            CROSSJOIN(\n",
    "                VALUES(Products[{BrandorManuf}]),\n",
    "                VALUES('Promo Description'[Promo Type])\n",
    "            ),\n",
    "            {column_exprs}\n",
    "        ),\n",
    "        Products[Category] = \"{categories[0]}\",\n",
    "        Products[{hierby}] = \"{entity_type}\",\n",
    "        TREATAS({p12m_dax}, 'Calendar'[MonthYear]),\n",
    "        TREATAS({{\"{entity_name}\"}}, 'Market'[{area}]),\n",
    "        FILTER('Scope', 'Scope'[Scope] = \"{hierby}\")\n",
    "    )\n",
    "    \"\"\"\n",
    "    parentdax_query = f\"\"\"\n",
    "    EVALUATE\n",
    "    CALCULATETABLE(\n",
    "        ADDCOLUMNS(\n",
    "            VALUES(Products[{BrandorManuf}]),\n",
    "            {column_exprs}\n",
    "        ),\n",
    "        Products[Category] = \"{categories[0]}\",\n",
    "        Products[{hierby}] = \"{entity_type}\",\n",
    "        TREATAS({p12m_dax}, 'Calendar'[MonthYear]),\n",
    "        TREATAS({{\"{entity_name}\"}}, 'Market'[{area}]),\n",
    "        FILTER('Scope', 'Scope'[Scope] = \"{hierby}\")\n",
    "    )\n",
    "    \"\"\"\n",
    "    grandtotal_query = f\"\"\"\n",
    "    EVALUATE\n",
    "    CALCULATETABLE(\n",
    "        ADDCOLUMNS(\n",
    "            VALUES(Products[Category]),\n",
    "            {column_exprs}\n",
    "        ),\n",
    "        Products[Category] = \"{categories[0]}\",\n",
    "        Products[{hierby}] = \"{entity_type}\",\n",
    "        TREATAS({p12m_dax}, 'Calendar'[MonthYear]),\n",
    "        TREATAS({{\"{entity_name}\"}}, 'Market'[{area}]),\n",
    "        FILTER('Scope', 'Scope'[Scope] = \"{hierby}\")\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(dax_query)\n",
    "            cols = [desc[0] for desc in cursor.description]\n",
    "            data = cursor.fetchall()\n",
    "            \n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(parentdax_query)\n",
    "            pcols = [desc[0] for desc in cursor.description]\n",
    "            pdata = cursor.fetchall()    \n",
    "\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(grandtotal_query)\n",
    "            gcols = [desc[0] for desc in cursor.description]\n",
    "            gdata = cursor.fetchall()\n",
    "\n",
    "        # Format results into DataFrames\n",
    "        df = pd.DataFrame(data, columns=cols)\n",
    "        df.columns = df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        df = df.loc[~(df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "        \n",
    "        pdf = pd.DataFrame(pdata, columns=pcols)\n",
    "        pdf.columns = pdf.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        pdf = pdf.loc[~(pdf.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "        \n",
    "        if pdf.shape[1] > 1:\n",
    "            pdf.iloc[:, 0] = pdf.iloc[:, 0].astype(str) + \" Total\"\n",
    "\n",
    "        if not pdf.empty:\n",
    "            df = pd.concat([df,pdf], ignore_index=True)\n",
    "        dt = pd.DataFrame([gdata[0]], columns=[col.replace(']', '').split('[')[-1] for col in gcols])\n",
    "        dt = dt[[col for col in dt.columns if col in df.columns]]\n",
    "\n",
    "        missing_cols = [col for col in df.columns if col not in dt.columns]\n",
    "        for col in missing_cols:\n",
    "            dt[col] = pd.NA\n",
    "        dt = dt[df.columns]\n",
    "        dt[df.columns[0]] = 'Grand Total'\n",
    "        \n",
    "        df = pd.concat([df, dt], ignore_index=True)\n",
    "\n",
    "        key = f\"{entity_type} | {entity_name}\"\n",
    "        outputdic[key] = df\n",
    "        print(f\"Query executed successfully for {entity_type} {entity_name}.\")\n",
    "\n",
    "    except adodbapi.DatabaseError as db_error:\n",
    "        print(f\"Database error for {entity_type} {entity_name}: {db_error}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error for {entity_type} {entity_name}: {e}\")\n",
    "\n",
    "    return outputdic\n",
    "\n",
    "# List of all entities to process\n",
    "\n",
    "brands_client_dfs = {}\n",
    "# **Threaded Execution**\n",
    "with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "    futures = []\n",
    "    for area, entity_list in entity_hierarchy:\n",
    "        for entity in entity_list:\n",
    "            for hierby, hier_values in hierarchy_levels:\n",
    "                    for value in hier_values:\n",
    "                        futures.append(executor.submit(execute_dax_query, f\"{BrandOrTopB}\",entity, area, hierby, value))\n",
    "               \n",
    "    # Wait for all tasks to complete\n",
    "    for future in futures:\n",
    "        result=future.result()\n",
    "        brands_client_dfs.update(result)\n",
    "\n",
    "pd.to_pickle(brands_client_dfs, os.path.join(path,\"brands_promo_type.pkl\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6edfae3",
   "metadata": {},
   "source": [
    "#### By Brands For P12M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84a4ad94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Disposables Sam's Corp.\n",
      "Query executed successfully for Manual Shave Men Sam's Corp.\n",
      "Query executed successfully for Refills NATIONAL.\n",
      "Query executed successfully for Razors NATIONAL.\n",
      "Query executed successfully for Disposables NATIONAL.\n",
      "Query executed successfully for Disposables NATIONAL.\n",
      "Query executed successfully for System NATIONAL.\n",
      "Query executed successfully for Manual Shave Men NATIONAL.\n",
      "Query executed successfully for Refills Sam's Corp.\n",
      "Query executed successfully for System Sam's Corp.\n",
      "Query executed successfully for Disposables Sam's Corp.\n",
      "Query executed successfully for Razors Sam's Corp.\n",
      "Query executed successfully for Disposables Walmart.\n",
      "Query executed successfully for Manual Shave Men Walmart.\n",
      "Query executed successfully for System Walmart.\n",
      "Query executed successfully for Refills Walmart.\n",
      "Query executed successfully for Disposables Walmart.\n",
      "Query executed successfully for Razors Walmart.\n"
     ]
    }
   ],
   "source": [
    "def execute_dax_query(entity_name, area, hierby, entity_type):\n",
    "    outputdic = {}\n",
    "    if normalized == True:\n",
    "        columns=[\"Promo Value\", 'VSOD', 'VSOD IYA','Value Share','Promo Share','Value Uplift (v. base) Normalized','Value Uplift Normalized IYA','Volume Uplift (v. Base) Normalized', 'Volume Uplift Normalized IYA'] + promo_col\n",
    "\n",
    "    else:        \n",
    "        columns=[\"Promo Value\", 'VSOD', 'VSOD IYA','Value Share','Promo Share','Value Uplift (v. base)','Value Uplift IYA','Volume Uplift (v. Base)', 'Volume Uplift IYA'] + promo_col\n",
    "\n",
    "\n",
    "    column_exprs = \",\\n            \".join(f'\"{col}\", COALESCE(CALCULATE([{col}]), 0)' for col in columns)\n",
    "\n",
    "    dax_query = f\"\"\"\n",
    "    EVALUATE\n",
    "    CALCULATETABLE(\n",
    "        SUMMARIZE(\n",
    "            Products,\n",
    "            Products[{BrandOrTopB}],  \n",
    "            Products[Total Size],  \n",
    "            \n",
    "            {column_exprs}\n",
    "        ),\n",
    "        Products[Category] = \"{categories[0]}\",        \n",
    "        Products[{hierby}] = \"{entity_type}\",\n",
    "        TREATAS(\n",
    "            {p12m_dax},\n",
    "            Calendar[MonthYear]\n",
    "        ),\n",
    "        TREATAS(\n",
    "            {{\"{entity_name}\"}},\n",
    "            Market[{area}]\n",
    "        ),\n",
    "        FILTER(\n",
    "            'Scope', \n",
    "            'Scope'[Scope] = \"{hierby}\"   \n",
    "        )\n",
    "    )   \n",
    "    \"\"\"\n",
    "    parentdax_query = f\"\"\"\n",
    "    EVALUATE\n",
    "    CALCULATETABLE(\n",
    "        SUMMARIZE(\n",
    "            Products,\n",
    "            Products[{BrandOrTopB}],              \n",
    "            {column_exprs}\n",
    "        ),\n",
    "        Products[Category] = \"{categories[0]}\",        \n",
    "        Products[{hierby}] = \"{entity_type}\",\n",
    "        TREATAS(\n",
    "            {p12m_dax},\n",
    "            Calendar[MonthYear]\n",
    "        ),\n",
    "        TREATAS(\n",
    "            {{\"{entity_name}\"}},\n",
    "            Market[{area}]\n",
    "        ),\n",
    "        FILTER(\n",
    "            'Scope', \n",
    "            'Scope'[Scope] = \"{hierby}\"   \n",
    "        )\n",
    "    )   \n",
    "    \"\"\"\n",
    "    grandtotal_query = f\"\"\"\n",
    "    EVALUATE\n",
    "    CALCULATETABLE(\n",
    "        SUMMARIZE(\n",
    "            Products,\n",
    "            Products[{hierby}],\n",
    "            \n",
    "            {column_exprs}\n",
    "        ),\n",
    "        Products[{hierby}] = \"{entity_type}\",\n",
    "        TREATAS(\n",
    "            {p12m_dax},\n",
    "            Calendar[MonthYear]\n",
    "        ),\n",
    "        TREATAS(\n",
    "            {{\"{entity_name}\"}},\n",
    "            Market[{area}]\n",
    "        ),\n",
    "        FILTER(\n",
    "            'Scope', \n",
    "            'Scope'[Scope] = \"{hierby}\"   \n",
    "        )\n",
    "    )   \n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(dax_query)\n",
    "            cols = [desc[0] for desc in cursor.description]\n",
    "            data = cursor.fetchall()\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(parentdax_query)\n",
    "            parentcols = [desc[0] for desc in cursor.description]\n",
    "            parentdata = cursor.fetchall()\n",
    "\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(grandtotal_query)\n",
    "            gcols = [desc[0] for desc in cursor.description]\n",
    "            gdata = cursor.fetchall()\n",
    "\n",
    "        # Format results into DataFrames\n",
    "        df = pd.DataFrame(data, columns=cols)\n",
    "        df.columns = df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        df = df.loc[~(df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "        \n",
    "        parentdf = pd.DataFrame(parentdata, columns=parentcols)\n",
    "        parentdf.columns = parentdf.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        parentdf = parentdf.loc[~(parentdf.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "        if parentdf.shape[1] > 1:\n",
    "            parentdf.iloc[:, 0] = parentdf.iloc[:, 0].astype(str) + \" Total\"\n",
    "\n",
    "\n",
    "            df = pd.concat([df,parentdf], ignore_index=True)\n",
    "       \n",
    "        dt = pd.DataFrame([gdata[0]], columns=[col.replace(']', '').split('[')[-1] for col in gcols])\n",
    "        dt[df.columns[0]] = 'Grand Total'\n",
    "        # Keep only columns that exist in df\n",
    "        dt = dt.loc[:, dt.columns.isin(df.columns)]\n",
    "\n",
    "        # Ensure column order matches df\n",
    "        dt = dt.reindex(columns=df.columns)\n",
    "\n",
    "        df = pd.concat([df, dt], ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "        # You may need this only if a junk column gets appended:\n",
    "        # df = df.loc[:, df.columns.notna()]\n",
    "\n",
    "        key = f\"{entity_type} | {entity_name}\"\n",
    "        outputdic[key] = df\n",
    "        print(f\"Query executed successfully for {entity_type} {entity_name}.\")\n",
    "\n",
    "    except adodbapi.DatabaseError as db_error:\n",
    "        print(f\"Database error for {entity_type} {entity_name}: {db_error}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error for {entity_type} {entity_name}: {e}\")\n",
    "\n",
    "    return outputdic\n",
    "\n",
    "# List of all entities to process\n",
    "\n",
    "promotions_brands_P12M = {}\n",
    "# **Threaded Execution**\n",
    "with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "    futures = []\n",
    "    for area, entity_list in entity_hierarchy:\n",
    "        for entity in entity_list:\n",
    "            for hierby, hier_values in hierarchy_levels:\n",
    "                    for value in hier_values:\n",
    "                        futures.append(executor.submit(execute_dax_query,entity, area, hierby, value))\n",
    "               \n",
    "    # Wait for all tasks to complete\n",
    "    for future in futures:\n",
    "        result=future.result()\n",
    "        promotions_brands_P12M.update(result)\n",
    "\n",
    "pd.to_pickle(promotions_brands_P12M, os.path.join(path,\"promotions_brands_P12M.pkl\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffe1f94",
   "metadata": {},
   "source": [
    "# VSOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2ad8232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Sam's Corp.\n",
      "Query executed successfully for NATIONAL.\n",
      "All DataFrames for Sector saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Promotion\\Promotion Datasets Test\\Sector_VSOD.pkl.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Sam's Corp.\n",
      "Query executed successfully for NATIONAL.\n",
      "All DataFrames for Segment saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Promotion\\Promotion Datasets Test\\Segment_VSOD.pkl.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Sam's Corp.\n",
      "Query executed successfully for NATIONAL.\n",
      "All DataFrames for SubSegment saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Promotion\\Promotion Datasets Test\\SubSegment_VSOD.pkl.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Sam's Corp.\n",
      "All DataFrames for SubCategory saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Promotion\\Promotion Datasets Test\\SubCategory_VSOD.pkl.\n"
     ]
    }
   ],
   "source": [
    "def execute_dax_query(entity_name, area, hierby,direct_parent):\n",
    "    outputdic = {}\n",
    "    key = f\"{categories[0]} | {entity_name}\"\n",
    "    columns = [\n",
    "        \"VSOD\"\n",
    "    ]\n",
    "    column_exprs = \", \".join(f'\"{col}\", COALESCE([{col}], 0)' for col in columns)\n",
    "\n",
    "\n",
    "    # Main query\n",
    "    dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(\n",
    "                    Products,\n",
    "                    Products[{direct_parent[hierby]}],\n",
    "                    Products[{hierby}]\n",
    "                    \n",
    "                ),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            TREATAS({p12m_dax}, Calendar[MonthYear]),\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"Category\")\n",
    "        )\n",
    "    \"\"\"\n",
    "    parenttotal_dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(\n",
    "                    Products,\n",
    "                    Products[{direct_parent[hierby]}]                   \n",
    "                ),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            TREATAS({p12m_dax}, Calendar[MonthYear]),\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"Category\")\n",
    "        )\n",
    "    \"\"\"\n",
    "        # Grand total query\n",
    "    grandtotal_query = f\"\"\"\n",
    "    EVALUATE\n",
    "    CALCULATETABLE(\n",
    "        ADDCOLUMNS(\n",
    "            SELECTCOLUMNS(\n",
    "                FILTER('Scope', 'Scope'[Scope] = \"Category\"),\n",
    "                \"Scope\", 'Scope'[Scope]\n",
    "            ),\n",
    "            \"VSOD\", COALESCE([VSOD], 0)\n",
    "        ),\n",
    "        TREATAS({p12m_dax}, Calendar[MonthYear]),\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}])\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    try:\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(dax_query)\n",
    "            columns_result = [desc[0] for desc in cursor.description]\n",
    "            data = cursor.fetchall()\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(parenttotal_dax_query)\n",
    "            maintotal_columns = [desc[0] for desc in cursor.description]\n",
    "            maintotal_data = cursor.fetchall()\n",
    "            \n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(grandtotal_query)\n",
    "            grandtotal_columns = [desc[0] for desc in cursor.description]\n",
    "            grandtotal_data = cursor.fetchall()\n",
    "\n",
    "        df = pd.DataFrame(data, columns=columns_result)\n",
    "        df.columns = df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        df = df.loc[~(df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "        \n",
    "        maintotal_df = pd.DataFrame(maintotal_data, columns=maintotal_columns)\n",
    "        maintotal_df.columns = maintotal_df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        maintotal_df = maintotal_df.loc[~(maintotal_df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "        \n",
    "        grand_tot = pd.DataFrame(grandtotal_data, columns=grandtotal_columns)\n",
    "        grand_tot.columns = grand_tot.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        grand_tot = grand_tot.loc[~(grand_tot.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "        if maintotal_df.shape[1] > 1:\n",
    "            maintotal_df.iloc[:, 0] = maintotal_df.iloc[:, 0].astype(str) + \" Total\"\n",
    "\n",
    "\n",
    "            df_with_totals = pd.concat([df,maintotal_df], ignore_index=True)\n",
    "    \n",
    "        if not grand_tot.empty:\n",
    "            # Create a dict for the first two columns\n",
    "            grand_tot[df.columns[0]] = 'Grand Total'\n",
    "            grand_tot[df.columns[1]] = np.nan  # or pd.NA\n",
    "\n",
    "            # Ensure all required columns exist, fill missing ones with NaN\n",
    "            for col in df.columns:\n",
    "                if col not in grand_tot.columns:\n",
    "                    grand_tot[col] = np.nan\n",
    "\n",
    "            # Reorder columns exactly as in df\n",
    "            grand_tot = grand_tot[df.columns]\n",
    "            df = pd.concat([df_with_totals, grand_tot], ignore_index=True)\n",
    "\n",
    "        outputdic[key] = df\n",
    "\n",
    "        print(f\"Query executed successfully for {entity_name}.\")\n",
    "    except adodbapi.DatabaseError as db_error:\n",
    "        print(f\"Database error for {entity_name} in {area}: {db_error}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error for {entity_name} in {area}: {e}\")\n",
    "\n",
    "    return outputdic\n",
    "\n",
    "\n",
    "def process_dax_queries(entity_hierarchy, hierarchy_levels,direct_parent):\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        dfs_results = {} \n",
    "        futures = {}\n",
    "        ordered_keys = []\n",
    "        for hierby, hier_values in hierarchy_levels:\n",
    "            if hierby == \"Category\" :\n",
    "                    continue\n",
    "            if isinstance(hier_values, list):\n",
    "                for area, entity_list in entity_hierarchy:\n",
    "                    for entity in entity_list:\n",
    "                        key = f\"{categories[0]} | {entity}\"\n",
    "                        ordered_keys.append(key)\n",
    "                        future = executor.submit(\n",
    "                            execute_dax_query, entity, area, hierby,direct_parent\n",
    "                        )\n",
    "                        futures[future] = key\n",
    "      \n",
    "\n",
    "            temp_results = {}\n",
    "            for future in as_completed(futures):\n",
    "                result = future.result()\n",
    "                temp_results.update(result)\n",
    "\n",
    "            for key in ordered_keys:\n",
    "                if key in temp_results:\n",
    "                    dfs_results[key] = temp_results[key]\n",
    "\n",
    "            filename = f\"{hierby}_VSOD.pkl\"\n",
    "\n",
    "            output_file = f\"{path}\\\\{filename}\"\n",
    "            with open(output_file, \"wb\") as f:\n",
    "                pd.to_pickle(dfs_results, f)\n",
    "            \n",
    "            print(f\"All DataFrames for {hierby} saved to {output_file}.\")\n",
    "\n",
    "process_dax_queries(entity_hierarchy, hierarchy_levels,direct_parent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4df5803",
   "metadata": {},
   "source": [
    "# VSOD Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6eab97cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Sam's Corp.\n",
      "Query executed successfully for Sam's Corp.\n",
      "Query executed successfully for Sam's Corp.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for Sector saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Promotion\\Promotion Datasets Test\\Sector_client_VSOD.pkl.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Sam's Corp.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Sam's Corp.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Sam's Corp.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for Segment saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Promotion\\Promotion Datasets Test\\Segment_client_VSOD.pkl.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Sam's Corp.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Sam's Corp.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Sam's Corp.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for SubSegment saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Promotion\\Promotion Datasets Test\\SubSegment_client_VSOD.pkl.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Sam's Corp.\n",
      "Query executed successfully for Sam's Corp.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Sam's Corp.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for SubCategory saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Promotion\\Promotion Datasets Test\\SubCategory_client_VSOD.pkl.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Sam's Corp.\n",
      "Query executed successfully for Sam's Corp.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Sam's Corp.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for Sector saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Promotion\\Promotion Datasets Test\\Sector_manuf_VSOD.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Sam's Corp.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Sam's Corp.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Sam's Corp.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for Segment saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Promotion\\Promotion Datasets Test\\Segment_manuf_VSOD.\n",
      "Query executed successfully for Sam's Corp.\n",
      "Query executed successfully for Sam's Corp.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Sam's Corp.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for SubSegment saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Promotion\\Promotion Datasets Test\\SubSegment_manuf_VSOD.\n",
      "Query executed successfully for Sam's Corp.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Sam's Corp.\n",
      "Query executed successfully for Sam's Corp.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for SubCategory saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Promotion\\Promotion Datasets Test\\SubCategory_manuf_VSOD.\n"
     ]
    }
   ],
   "source": [
    "def execute_dax_query(ManuforBrand,entity_name, area, hierby,direct_parent,client_list):\n",
    "    outputdic = {}\n",
    "    key = f\"{categories[0]} | {entity_name}\"\n",
    "\n",
    "    columns = [\n",
    "        \"VSOD\"\n",
    "    ]\n",
    "\n",
    "    column_exprs = \", \".join(f'\"{col}\", COALESCE([{col}], 0)' for col in columns)\n",
    "    client_values_dax = \", \".join([f'\"{c.replace(\"\\\"\", \"\\\"\\\"\")}\"' for c in client_list])\n",
    "    cat_filter = \"\"\n",
    "    if hierby !=\"Sector\":\n",
    "        cat_filter = f'''\n",
    "            FILTER(\n",
    "                Products,\n",
    "                Products[Category] = \"{categories[0]}\"\n",
    "            ),\n",
    "        '''    \n",
    "\n",
    "    # Prepare client brand filter conditionally\n",
    "\n",
    "    # Main query\n",
    "    dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(\n",
    "                    Products,\n",
    "                    Products[{direct_parent[hierby]}],\n",
    "                    Products[{hierby}],\n",
    "                    Products[{ManuforBrand}]\n",
    "                      \n",
    "                ),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            TREATAS({p12m_dax}, Calendar[MonthYear]),\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "            {cat_filter}\n",
    "            FILTER(\n",
    "            Products,\n",
    "            Products[{ManuforBrand}] IN {{ {client_values_dax} }}\n",
    "              ),\n",
    "\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"Category\")\n",
    "        )\n",
    "    \"\"\"\n",
    "    parenttotal_dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(\n",
    "                    Products,\n",
    "                    Products[{direct_parent[hierby]}],\n",
    "                    Products[{ManuforBrand}]\n",
    "                                       \n",
    "                ),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            TREATAS({p12m_dax}, Calendar[MonthYear]),\n",
    "            {cat_filter}\n",
    "            FILTER(\n",
    "            Products,\n",
    "            Products[{ManuforBrand}] IN {{ {client_values_dax} }}\n",
    "              ),\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"Category\")\n",
    "        )\n",
    "    \"\"\"\n",
    "    childtotal_dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(\n",
    "                    Products,\n",
    "                    Products[{hierby}]                   \n",
    "                ),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            TREATAS({p12m_dax}, Calendar[MonthYear]),\n",
    "            FILTER(\n",
    "            Products,\n",
    "            Products[{ManuforBrand}] IN {{ {client_values_dax} }}\n",
    "              ),\n",
    "              {cat_filter}\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"Category\")\n",
    "        )\n",
    "    \"\"\"\n",
    "        # Grand total query\n",
    "    grandtotal_query = f\"\"\"\n",
    "    EVALUATE\n",
    "    CALCULATETABLE(\n",
    "        ADDCOLUMNS(\n",
    "            SELECTCOLUMNS(\n",
    "                FILTER('Scope', 'Scope'[Scope] = \"Category\"),\n",
    "                \"Scope\", 'Scope'[Scope]\n",
    "            ),\n",
    "            \"VSOD\", COALESCE([VSOD], 0)\n",
    "        ),\n",
    "        \n",
    "        TREATAS({p12m_dax}, Calendar[MonthYear]),\n",
    "                    FILTER(\n",
    "            Products,\n",
    "            Products[{ManuforBrand}] IN {{ {client_values_dax} }}\n",
    "              ),\n",
    "            {cat_filter}\n",
    "              \n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}])\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(dax_query)\n",
    "            columns_result = [desc[0] for desc in cursor.description]\n",
    "            data = cursor.fetchall()\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(parenttotal_dax_query)\n",
    "            maintotal_columns = [desc[0] for desc in cursor.description]\n",
    "            maintotal_data = cursor.fetchall()\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(childtotal_dax_query)\n",
    "            childtotal_columns = [desc[0] for desc in cursor.description]\n",
    "            childtotal_data = cursor.fetchall()         \n",
    "              \n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(grandtotal_query)\n",
    "            grandtotal_columns = [desc[0] for desc in cursor.description]\n",
    "            grandtotal_data = cursor.fetchall()\n",
    "\n",
    "        df = pd.DataFrame(data, columns=columns_result)\n",
    "        df.columns = df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        df = df.loc[~(df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "        \n",
    "        maintotal_df = pd.DataFrame(maintotal_data, columns=maintotal_columns)\n",
    "        maintotal_df.columns = maintotal_df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        maintotal_df = maintotal_df.loc[~(maintotal_df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "        \n",
    "        childtotal_df = pd.DataFrame(childtotal_data, columns=childtotal_columns)\n",
    "        childtotal_df.columns = childtotal_df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        childtotal_df = childtotal_df.loc[~(childtotal_df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "        \n",
    "        grand_tot = pd.DataFrame(grandtotal_data, columns=grandtotal_columns)\n",
    "        grand_tot.columns = grand_tot.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        grand_tot = grand_tot.loc[~(grand_tot.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "        \n",
    "        if maintotal_df.shape[1] > 1:\n",
    "            maintotal_df.iloc[:, 0] = maintotal_df.iloc[:, 0].astype(str) + \" Total\"\n",
    "\n",
    "        if maintotal_df.empty:\n",
    "            outputdic[key] = maintotal_df\n",
    "            return outputdic\n",
    "        if not maintotal_df.empty:\n",
    "            df_with_totals = pd.concat([df,maintotal_df], ignore_index=True)\n",
    "    \n",
    "        if not grand_tot.empty:\n",
    "            # Create a dict for the first two columns\n",
    "            grand_tot[df.columns[0]] = 'Grand Total'\n",
    "            grand_tot[df.columns[1]] = np.nan  # or pd.NA\n",
    "\n",
    "            # Ensure all required columns exist, fill missing ones with NaN\n",
    "            for col in df.columns:\n",
    "                if col not in grand_tot.columns:\n",
    "                    grand_tot[col] = np.nan\n",
    "                    \n",
    "        if childtotal_df.shape[1] > 1:\n",
    "            childtotal_df.iloc[:, 0] =  childtotal_df.iloc[:, 0].astype(str) + \" Total\"\n",
    "\n",
    "\n",
    "            # Reorder columns exactly as in df\n",
    "            grand_tot = grand_tot[df.columns]\n",
    "            df = pd.concat([df_with_totals, grand_tot], ignore_index=True)\n",
    "            df = pd.concat([df, childtotal_df], ignore_index=True)\n",
    "            \n",
    "\n",
    "        outputdic[key] = df\n",
    "\n",
    "        print(f\"Query executed successfully for {entity_name}.\")\n",
    "    except adodbapi.DatabaseError as db_error:\n",
    "        print(f\"Database error for {entity_name} in {area}: {db_error}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error for {entity_name} in {area}: {e}\")\n",
    "\n",
    "    return outputdic\n",
    "\n",
    "\n",
    "def process_dax_queries(ManuforBrand,entity_hierarchy, hierarchy_levels,direct_parent,client_list):\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        dfs_results = {} \n",
    "        futures = {}\n",
    "        ordered_keys = []\n",
    "        for hierby, hier_values in hierarchy_levels:\n",
    "            if hierby == \"Category\" :\n",
    "                    continue\n",
    "            if isinstance(hier_values, list):\n",
    "                for area, entity_list in entity_hierarchy:\n",
    "                    for entity in entity_list:\n",
    "                        for client in client_brands:\n",
    "                            key = f\"{categories[0]} | {entity}\"\n",
    "                            ordered_keys.append(key)\n",
    "                            future = executor.submit(\n",
    "                                execute_dax_query,ManuforBrand, entity, area, hierby,direct_parent,client_list\n",
    "                            )\n",
    "                            futures[future] = key\n",
    "\n",
    "\n",
    "            temp_results = {}\n",
    "            for future in as_completed(futures):\n",
    "                result = future.result()\n",
    "                temp_results.update(result)\n",
    "\n",
    "            for key in ordered_keys:\n",
    "                if key in temp_results:\n",
    "                    dfs_results[key] = temp_results[key]\n",
    "\n",
    "            if ManuforBrand==f'{BrandOrTopB}':\n",
    "                filename = f\"{hierby}_client_VSOD.pkl\"\n",
    "            else:\n",
    "                filename = f\"{hierby}_manuf_VSOD\"\n",
    "                 \n",
    "            output_file = f\"{path}\\\\{filename}\"\n",
    "            with open(output_file, \"wb\") as f:\n",
    "                pd.to_pickle(dfs_results, f)\n",
    "            \n",
    "            print(f\"All DataFrames for {hierby} saved to {output_file}.\")\n",
    "\n",
    "process_dax_queries(f'{BrandOrTopB}',entity_hierarchy, hierarchy_levels,direct_parent,client_brands)\n",
    "process_dax_queries(f'{ManufOrTopC}',entity_hierarchy, hierarchy_levels,direct_parent,client_manuf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0de6d8",
   "metadata": {},
   "source": [
    "# By Products/Item P12M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41918a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Disposables Sam's Corp.\n",
      "Query executed successfully for Manual Shave Men Sam's Corp.\n",
      "Query executed successfully for Refills NATIONAL.\n",
      "Query executed successfully for Razors NATIONAL.\n",
      "Query executed successfully for Disposables NATIONAL.\n",
      "Query executed successfully for Disposables NATIONAL.\n",
      "Query executed successfully for System NATIONAL.\n",
      "Query executed successfully for Manual Shave Men NATIONAL.\n",
      "Query executed successfully for System Sam's Corp.\n",
      "Query executed successfully for Refills Sam's Corp.\n",
      "Query executed successfully for Disposables Sam's Corp.\n",
      "Query executed successfully for Razors Sam's Corp.\n",
      "Query executed successfully for Disposables Walmart.\n",
      "Query executed successfully for Manual Shave Men Walmart.\n",
      "Query executed successfully for System Walmart.\n",
      "Query executed successfully for Refills Walmart.\n",
      "Query executed successfully for Razors Walmart.\n",
      "Query executed successfully for Disposables Walmart.\n"
     ]
    }
   ],
   "source": [
    "def execute_dax_query(entity_name, area, hierby, entity_type):\n",
    "    outputdic = {}\n",
    "    if normalized == True:\n",
    "        columns = [\"Promo Sales\",\"Promo Value\",\"Discount Depth (%)\",\"Promo Share\",\"VSOD\",\"Base Price/Unit\",\"Promo Price/Unit\",\"Gross Margin %\",\"Value Uplift (v. base) Normalized\",'Volume Uplift (v. Base) Normalized',\"Trade Effectiveness\",\"Incr Value\"]\n",
    "    else:        \n",
    "        columns = [\"Promo Sales\",\"Promo Value\",\"Discount Depth (%)\",\"Promo Share\",\"VSOD\",\"Base Price/Unit\",\"Promo Price/Unit\",\"Gross Margin %\",\"Value Uplift (v. base)\",'Volume Uplift (v. Base)',\"Trade Effectiveness\",\"Incr Value\"]\n",
    "\n",
    "    column_exprs = \",\\n            \".join(f'\"{col}\", COALESCE(CALCULATE([{col}]), 0)' for col in columns)\n",
    "\n",
    "    dax_query = f\"\"\"\n",
    "    EVALUATE\n",
    "    CALCULATETABLE(\n",
    "        SUMMARIZE(\n",
    "            Products,\n",
    "            Products[{BrandOrTopB}],  \n",
    "            Products[{prodORitem}],  \n",
    "            \n",
    "            {column_exprs}\n",
    "        ),\n",
    "        Products[Category] = \"{categories[0]}\",        \n",
    "        Products[{hierby}] = \"{entity_type}\",\n",
    "        TREATAS(\n",
    "            {p12m_dax},\n",
    "            Calendar[MonthYear]\n",
    "        ),\n",
    "        TREATAS(\n",
    "            {{\"{entity_name}\"}},\n",
    "            Market[{area}]\n",
    "        ),\n",
    "        FILTER(\n",
    "            'Scope', \n",
    "            'Scope'[Scope] = \"{hierby}\"   \n",
    "        )\n",
    "    )   \n",
    "    \"\"\"\n",
    "    grandtotal_query = f\"\"\"\n",
    "    EVALUATE\n",
    "    CALCULATETABLE(\n",
    "        ADDCOLUMNS(\n",
    "            VALUES(Products[Category]),\n",
    "            {column_exprs}\n",
    "        ),\n",
    "        Products[Category] = \"{categories[0]}\",\n",
    "        Products[{hierby}] = \"{entity_type}\",\n",
    "        TREATAS({p12m_dax}, 'Calendar'[MonthYear]),\n",
    "        TREATAS({{\"{entity_name}\"}}, 'Market'[{area}]),\n",
    "        FILTER('Scope', 'Scope'[Scope] = \"Category\")\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(dax_query)\n",
    "            cols = [desc[0] for desc in cursor.description]\n",
    "            data = cursor.fetchall()\n",
    "\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(grandtotal_query)\n",
    "            gcols = [desc[0] for desc in cursor.description]\n",
    "            gdata = cursor.fetchall()\n",
    "\n",
    "        # Format results into DataFrames\n",
    "        df = pd.DataFrame(data, columns=cols)\n",
    "        df.columns = df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        df = df.loc[~(df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "\n",
    "        dt = pd.DataFrame([gdata[0]], columns=[col.replace(']', '').split('[')[-1] for col in gcols])\n",
    "        \n",
    "        dt[df.columns[0]] = 'Grand Total'\n",
    "        for col in df.columns:\n",
    "            if col not in dt.columns:\n",
    "                dt[col] = np.nan\n",
    "\n",
    "        # Reorder columns exactly as in df\n",
    "        dt = dt[df.columns]\n",
    "        df = pd.concat([df, dt], ignore_index=True)\n",
    "\n",
    "        # You may need this only if a junk column gets appended:\n",
    "        # df = df.loc[:, df.columns.notna()]\n",
    "\n",
    "        key = f\"{entity_type} | {entity_name}\"\n",
    "        outputdic[key] = df\n",
    "        print(f\"Query executed successfully for {entity_type} {entity_name}.\")\n",
    "\n",
    "    except adodbapi.DatabaseError as db_error:\n",
    "        print(f\"Database error for {entity_type} {entity_name}: {db_error}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error for {entity_type} {entity_name}: {e}\")\n",
    "\n",
    "    return outputdic\n",
    "\n",
    "# List of all entities to process\n",
    "\n",
    "promotions_products_P12M = {}\n",
    "# **Threaded Execution**\n",
    "with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "    futures = []\n",
    "    for area, entity_list in entity_hierarchy:\n",
    "        for entity in entity_list:\n",
    "            for hierby, hier_values in hierarchy_levels:\n",
    "                    for value in hier_values:\n",
    "                        futures.append(executor.submit(execute_dax_query,entity, area, hierby, value))\n",
    "               \n",
    "    # Wait for all tasks to complete\n",
    "    for future in futures:\n",
    "        result=future.result()\n",
    "        promotions_products_P12M.update(result)\n",
    "\n",
    "pd.to_pickle(promotions_products_P12M, os.path.join(path,\"promotions_products_P12M.pkl\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae1e35b",
   "metadata": {},
   "source": [
    "# By End of The Week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04ee71e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Sam's Corp.\n",
      "Query executed successfully for Sam's Corp.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Sam's Corp.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Sam's Corp.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Sam's Corp.\n",
      "Query executed successfully for Sam's Corp.\n",
      "Query executed successfully for Sam's Corp.\n",
      "Query executed successfully for Sam's Corp.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Sam's Corp.\n",
      "Query executed successfully for Sam's Corp.\n",
      "Query executed successfully for Sam's Corp.\n",
      "Query executed successfully for Sam's Corp.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Sam's Corp.\n",
      "Query executed successfully for Sam's Corp.\n",
      "Query executed successfully for Sam's Corp.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Sam's Corp.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Sam's Corp.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Sam's Corp.\n",
      "Query executed successfully for Sam's Corp.\n",
      "Query executed successfully for Sam's Corp.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Sam's Corp.\n",
      "Query executed successfully for Sam's Corp.\n",
      "Query executed successfully for Sam's Corp.\n",
      "Query executed successfully for Sam's Corp.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for SubCategory saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Promotion\\Promotion Datasets Test\\promotions_EndOfWeek.pkl.\n"
     ]
    }
   ],
   "source": [
    "def execute_dax_query(entity_name,entity_type,hierby, area, client,manuf):\n",
    "    outputdic = {}\n",
    "    key = f\"{entity_type} | {client} | {entity_name}\" if client else (f\"{entity_type} | {manuf} | {entity_name}\" if manuf else f\"{entity_name}\")\n",
    "    if normalized == True:\n",
    "        columns = ['Value Uplift (v. base) Normalized','Promo Value','Non Promo Value','Value Sales','Base Sales','VSOD','Promo Volume','Non Promo Volume']\n",
    "    else:\n",
    "        columns = ['Value Uplift (v. base)','Promo Value','Non Promo Value','Value Sales','Base Sales','VSOD','Promo Volume','Non Promo Volume']\n",
    "\n",
    "\n",
    "    column_exprs = \", \".join(f'\"{col}\", COALESCE([{col}], 0)' for col in columns)\n",
    "    # Prepare client brand filter conditionally\n",
    "    client_filter = \"\"\n",
    "    if client:\n",
    "        client_filter = f'''\n",
    "            FILTER(\n",
    "                Products,\n",
    "                Products[Top Brands] = \"{client}\"\n",
    "            ),\n",
    "        '''    \n",
    "    manuf_filter=''\n",
    "    if  manuf:\n",
    "        manuf_filter = f'''\n",
    "            FILTER(\n",
    "                Products,\n",
    "                Products[Top Companies] = \"{manuf}\"\n",
    "            ),\n",
    "        '''\n",
    "\n",
    "    # Main query\n",
    "    dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(\n",
    "                    Calendar,\n",
    "                    Calendar[End of Week]\n",
    "                    \n",
    "                ),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            {client_filter}\n",
    "            {manuf_filter}\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "            Products[{hierby}] = \"{entity_type}\",\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"Category\")\n",
    "        )\n",
    "    \"\"\"\n",
    " \n",
    "        # Grand total query\n",
    "    grandtotal_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                VALUES(Products[Category]),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            Products[{hierby}] = \"{entity_type}\",\n",
    "            {client_filter}\n",
    "            {manuf_filter}\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"Category\")\n",
    "            \n",
    "        )\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(dax_query)\n",
    "            columns_result = [desc[0] for desc in cursor.description]\n",
    "            data = cursor.fetchall()\n",
    "\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(grandtotal_query)\n",
    "            grandtotal_columns = [desc[0] for desc in cursor.description]\n",
    "            grandtotal_data = cursor.fetchall()\n",
    "        df = pd.DataFrame(data, columns=columns_result)\n",
    "        df.columns = df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        # Rename the column if necessary (optional)\n",
    "        df.columns = df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "\n",
    "        # Find the column name for the date column (usually \"End of Week\")\n",
    "        date_col = df.columns[0]  # Assuming the first column is the date\n",
    "        if not df.empty:\n",
    "            df[date_col] = pd.to_datetime(df[date_col]).dt.strftime('%Y-%m-%d')\n",
    "        if not df.empty:\n",
    "            df.iloc[:, 0] = pd.to_datetime(df.iloc[:, 0]).dt.date\n",
    "        df = df.loc[~(df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "\n",
    "\n",
    "\n",
    "        grand_tot = pd.DataFrame(grandtotal_data, columns=grandtotal_columns)\n",
    "        grand_tot.columns = grand_tot.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        grand_tot = grand_tot.loc[~(grand_tot.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "\n",
    "        if not grand_tot.empty:\n",
    "            # Create a dict for the first two columns\n",
    "            grand_tot[df.columns[0]] = 'Grand Total'\n",
    "            # grand_tot[df.columns[1]] = np.nan  # or pd.NA\n",
    "\n",
    "            # Ensure all required columns exist, fill missing ones with NaN\n",
    "            for col in df.columns:\n",
    "                if col not in grand_tot.columns:\n",
    "                    grand_tot[col] = np.nan\n",
    "\n",
    "            # Reorder columns exactly as in df\n",
    "            grand_tot = grand_tot[df.columns]\n",
    "            df = pd.concat([df, grand_tot], ignore_index=True)\n",
    "        outputdic[key] = df\n",
    "\n",
    "        print(f\"Query executed successfully for {entity_name}.\")\n",
    "    except adodbapi.DatabaseError as db_error:\n",
    "        print(f\"Database error for {entity_name} in {area}: {db_error}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error for {entity_name} in {area}: {e}\")\n",
    "\n",
    "    return outputdic\n",
    "\n",
    "\n",
    "def process_dax_queries(entity_hierarchy, hierarchy_levels, client_brands=None, client_manuf=None):\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        dfs_results = {} \n",
    "        futures = {}\n",
    "        ordered_keys = []\n",
    "        for hierby, hier_values in hierarchy_levels:\n",
    "            if isinstance(hier_values, list):\n",
    "                for value in hier_values:\n",
    "                    for area, entity_list in entity_hierarchy:\n",
    "                        for entity in entity_list:\n",
    "                            if client_manuf:\n",
    "                                for manuf in client_manuf:\n",
    "                                    key = f\"{value} | {manuf} | {entity}\"\n",
    "                                    ordered_keys.append(key)\n",
    "                                    future = executor.submit(\n",
    "                                        execute_dax_query, entity, value,hierby, area, '', manuf\n",
    "                                    )\n",
    "                                    futures[future] = key\n",
    "\n",
    "                            if client_brands:\n",
    "                                for client in client_brands:\n",
    "                                    key = f\"{value} | {client} | {entity}\"\n",
    "                                    ordered_keys.append(key)\n",
    "                                    future = executor.submit(\n",
    "                                        execute_dax_query, entity, value,hierby, area, client, ''\n",
    "                                    )\n",
    "                                    futures[future] = key\n",
    "\n",
    "        temp_results = {}\n",
    "        for future in as_completed(futures):\n",
    "            try:\n",
    "                result = future.result()\n",
    "                temp_results.update(result)\n",
    "            except Exception as e:\n",
    "                print(f\"Error in future: {e}\")\n",
    "\n",
    "        for key in ordered_keys:\n",
    "            if key in temp_results:\n",
    "                dfs_results[key] = temp_results[key]\n",
    "\n",
    "        filename = f\"promotions_EndOfWeek\"\n",
    "        output_file = f\"{path}\\\\{filename}.pkl\"\n",
    "\n",
    "        with open(output_file, \"wb\") as f:\n",
    "            pd.to_pickle(dfs_results, f)\n",
    "\n",
    "        print(f\"All DataFrames for {hierby} saved to {output_file}.\")\n",
    "\n",
    "\n",
    "if client_brands:\n",
    "    process_dax_queries(entity_hierarchy, hierarchy_levels, client_manuf=client_manuf,client_brands=client_brands)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72454168",
   "metadata": {},
   "source": [
    "# Value Uplift vs Discount depth By  Client Brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "827dd3ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ali Salem\\AppData\\Local\\Temp\\ipykernel_13492\\3288857394.py:125: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Sam's Corp.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ali Salem\\AppData\\Local\\Temp\\ipykernel_13492\\3288857394.py:125: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for NATIONAL.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ali Salem\\AppData\\Local\\Temp\\ipykernel_13492\\3288857394.py:125: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n",
      "C:\\Users\\Ali Salem\\AppData\\Local\\Temp\\ipykernel_13492\\3288857394.py:125: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ali Salem\\AppData\\Local\\Temp\\ipykernel_13492\\3288857394.py:125: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Sam's Corp.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ali Salem\\AppData\\Local\\Temp\\ipykernel_13492\\3288857394.py:125: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Walmart.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ali Salem\\AppData\\Local\\Temp\\ipykernel_13492\\3288857394.py:125: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for NATIONAL.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ali Salem\\AppData\\Local\\Temp\\ipykernel_13492\\3288857394.py:125: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Walmart.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ali Salem\\AppData\\Local\\Temp\\ipykernel_13492\\3288857394.py:125: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for NATIONAL.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ali Salem\\AppData\\Local\\Temp\\ipykernel_13492\\3288857394.py:125: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n",
      "C:\\Users\\Ali Salem\\AppData\\Local\\Temp\\ipykernel_13492\\3288857394.py:125: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Sam's Corp.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ali Salem\\AppData\\Local\\Temp\\ipykernel_13492\\3288857394.py:125: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Walmart.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ali Salem\\AppData\\Local\\Temp\\ipykernel_13492\\3288857394.py:125: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Walmart.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ali Salem\\AppData\\Local\\Temp\\ipykernel_13492\\3288857394.py:125: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for NATIONAL.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ali Salem\\AppData\\Local\\Temp\\ipykernel_13492\\3288857394.py:125: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for NATIONAL.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ali Salem\\AppData\\Local\\Temp\\ipykernel_13492\\3288857394.py:125: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for NATIONAL.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ali Salem\\AppData\\Local\\Temp\\ipykernel_13492\\3288857394.py:125: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Sam's Corp.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ali Salem\\AppData\\Local\\Temp\\ipykernel_13492\\3288857394.py:125: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Walmart.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ali Salem\\AppData\\Local\\Temp\\ipykernel_13492\\3288857394.py:125: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Walmart.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ali Salem\\AppData\\Local\\Temp\\ipykernel_13492\\3288857394.py:125: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n",
      "C:\\Users\\Ali Salem\\AppData\\Local\\Temp\\ipykernel_13492\\3288857394.py:125: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ali Salem\\AppData\\Local\\Temp\\ipykernel_13492\\3288857394.py:125: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n",
      "C:\\Users\\Ali Salem\\AppData\\Local\\Temp\\ipykernel_13492\\3288857394.py:125: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ali Salem\\AppData\\Local\\Temp\\ipykernel_13492\\3288857394.py:125: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n",
      "C:\\Users\\Ali Salem\\AppData\\Local\\Temp\\ipykernel_13492\\3288857394.py:125: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n",
      "C:\\Users\\Ali Salem\\AppData\\Local\\Temp\\ipykernel_13492\\3288857394.py:125: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ali Salem\\AppData\\Local\\Temp\\ipykernel_13492\\3288857394.py:125: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for NATIONAL.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ali Salem\\AppData\\Local\\Temp\\ipykernel_13492\\3288857394.py:125: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for NATIONAL.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ali Salem\\AppData\\Local\\Temp\\ipykernel_13492\\3288857394.py:125: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Sam's Corp.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ali Salem\\AppData\\Local\\Temp\\ipykernel_13492\\3288857394.py:125: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Walmart.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ali Salem\\AppData\\Local\\Temp\\ipykernel_13492\\3288857394.py:125: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Walmart.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ali Salem\\AppData\\Local\\Temp\\ipykernel_13492\\3288857394.py:125: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for NATIONAL.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ali Salem\\AppData\\Local\\Temp\\ipykernel_13492\\3288857394.py:125: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for NATIONAL.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ali Salem\\AppData\\Local\\Temp\\ipykernel_13492\\3288857394.py:125: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for NATIONAL.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ali Salem\\AppData\\Local\\Temp\\ipykernel_13492\\3288857394.py:125: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Sam's Corp.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ali Salem\\AppData\\Local\\Temp\\ipykernel_13492\\3288857394.py:125: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n",
      "C:\\Users\\Ali Salem\\AppData\\Local\\Temp\\ipykernel_13492\\3288857394.py:125: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ali Salem\\AppData\\Local\\Temp\\ipykernel_13492\\3288857394.py:125: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Walmart.\n",
      "All DataFrames for saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Promotion\\Promotion Datasets Test\\value_uplift.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ali Salem\\AppData\\Local\\Temp\\ipykernel_13492\\3288857394.py:125: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Sam's Corp.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ali Salem\\AppData\\Local\\Temp\\ipykernel_13492\\3288857394.py:125: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Sam's Corp.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ali Salem\\AppData\\Local\\Temp\\ipykernel_13492\\3288857394.py:125: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for NATIONAL.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ali Salem\\AppData\\Local\\Temp\\ipykernel_13492\\3288857394.py:125: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n",
      "C:\\Users\\Ali Salem\\AppData\\Local\\Temp\\ipykernel_13492\\3288857394.py:125: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Sam's Corp.\n",
      "Query executed successfully for NATIONAL.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ali Salem\\AppData\\Local\\Temp\\ipykernel_13492\\3288857394.py:125: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n",
      "C:\\Users\\Ali Salem\\AppData\\Local\\Temp\\ipykernel_13492\\3288857394.py:125: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ali Salem\\AppData\\Local\\Temp\\ipykernel_13492\\3288857394.py:125: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for NATIONAL.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ali Salem\\AppData\\Local\\Temp\\ipykernel_13492\\3288857394.py:125: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for NATIONAL.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ali Salem\\AppData\\Local\\Temp\\ipykernel_13492\\3288857394.py:125: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Walmart.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ali Salem\\AppData\\Local\\Temp\\ipykernel_13492\\3288857394.py:125: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Walmart.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ali Salem\\AppData\\Local\\Temp\\ipykernel_13492\\3288857394.py:125: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for NATIONAL.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ali Salem\\AppData\\Local\\Temp\\ipykernel_13492\\3288857394.py:125: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n",
      "C:\\Users\\Ali Salem\\AppData\\Local\\Temp\\ipykernel_13492\\3288857394.py:125: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Sam's Corp.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ali Salem\\AppData\\Local\\Temp\\ipykernel_13492\\3288857394.py:125: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for NATIONAL.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ali Salem\\AppData\\Local\\Temp\\ipykernel_13492\\3288857394.py:125: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Sam's Corp.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Promotion\\Promotion Datasets Test\\value_uplift_manuf.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ali Salem\\AppData\\Local\\Temp\\ipykernel_13492\\3288857394.py:125: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, grand_tot], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "def execute_dax_query(entity_name,entity_type,hierby, area, client,manuf):\n",
    "    outputdic = {}\n",
    "    key = f\"{entity_type} | {client} | {entity_name}\" if client else (f\"{entity_type} | {manuf} | {entity_name}\" if manuf else f\"{entity_name}\")\n",
    "    if normalized == True:\n",
    "        columns = ['Discount Depth (%)','Promo Price/Unit','Value Uplift (v. base) Normalized','Promo Sales']\n",
    "    else:\n",
    "        columns = ['Discount Depth (%)','Promo Price/Unit','Value Uplift (v. base)','Promo Sales']\n",
    "\n",
    "\n",
    "    column_exprs = \", \".join(f'\"{col}\", COALESCE([{col}], 0)' for col in columns)\n",
    "\n",
    "    # Prepare client brand filter conditionally\n",
    "    client_filter = \"\"\n",
    "    if client:\n",
    "        client_filter = f'''\n",
    "            FILTER(\n",
    "                Products,\n",
    "                Products[Top Brands] = \"{client}\"\n",
    "            ),\n",
    "        '''    \n",
    "    manuf_filter=''\n",
    "    if manuf:\n",
    "        manuf_filter = f'''\n",
    "            FILTER(\n",
    "                Products,\n",
    "                Products[Top Companies] = \"{manuf}\"\n",
    "            ),\n",
    "        '''\n",
    "\n",
    "    # Main query\n",
    "    dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "            CROSSJOIN(\n",
    "                VALUES(Products[{prodORitem}]),\n",
    "                VALUES('Calendar'[End of Week])\n",
    "    \n",
    "                ),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            {client_filter}\n",
    "            {manuf_filter}\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "            Products[{hierby}] = \"{entity_type}\",\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"{hierby}\")\n",
    "        )\n",
    "    \"\"\"\n",
    "    parenttotal_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                VALUES(Products[{prodORitem}]),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            Products[{hierby}] = \"{entity_type}\",\n",
    "            {client_filter}\n",
    "            {manuf_filter}\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}])\n",
    "        )\n",
    "    \"\"\"\n",
    "        # Grand total query\n",
    "    grandtotal_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                VALUES(Products[Category]),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            Products[{hierby}] = \"{entity_type}\",\n",
    "            \n",
    "            {client_filter}\n",
    "            {manuf_filter}\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}])\n",
    "        )\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(dax_query)\n",
    "            columns_result = [desc[0] for desc in cursor.description]\n",
    "            data = cursor.fetchall()\n",
    "\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(parenttotal_query)\n",
    "            parcol = [desc[0] for desc in cursor.description]\n",
    "            pardata = cursor.fetchall()\n",
    "\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(grandtotal_query)\n",
    "            grandtotal_columns = [desc[0] for desc in cursor.description]\n",
    "            grandtotal_data = cursor.fetchall()\n",
    "\n",
    "        df = pd.DataFrame(data, columns=columns_result)\n",
    "        df.columns = df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        df = df.loc[~(df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "\n",
    "        dp = pd.DataFrame(pardata, columns=parcol)\n",
    "        dp.columns = dp.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        dp = dp.loc[~(dp.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "\n",
    "        grand_tot = pd.DataFrame(grandtotal_data, columns=grandtotal_columns)\n",
    "        grand_tot.columns = grand_tot.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        grand_tot = grand_tot.loc[~(grand_tot.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "\n",
    "        if dp.shape[1] > 1:\n",
    "            dp.iloc[:, 0] = dp.iloc[:, 0].astype(str) + \" Total\"\n",
    "\n",
    "        if dp.empty:\n",
    "            outputdic[key] = dp\n",
    "            return outputdic\n",
    "        if not df.empty: \n",
    "            df = pd.concat([df, dp], ignore_index=True)\n",
    "\n",
    "            if not grand_tot.empty and grand_tot.dropna(axis=1, how='all').shape[1] > 0:\n",
    "                grand_tot[df.columns[0]] = 'Grand Total'\n",
    "                for col in df.columns:\n",
    "                    if col not in grand_tot.columns:\n",
    "                        grand_tot[col] = np.nan\n",
    "                grand_tot = grand_tot[df.columns]\n",
    "                if not  df.empty:\n",
    "                    df = pd.concat([df, grand_tot], ignore_index=True)\n",
    "\n",
    "            outputdic[key] = df\n",
    "            print(f\"Query executed successfully for {entity_name}.\")\n",
    "\n",
    "    except adodbapi.DatabaseError as db_error:\n",
    "        print(f\"Database error for {entity_name} in {area}: {db_error}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error for {entity_name} in {area}: {e}\")\n",
    "\n",
    "    return outputdic\n",
    "\n",
    "\n",
    "\n",
    "def process_dax_queries(entity_hierarchy, hierarchy_levels, client_brands=None, client_manuf=None):\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        dfs_results = {} \n",
    "        futures = {}\n",
    "        ordered_keys = []\n",
    "        for hierby, hier_values in hierarchy_levels:\n",
    "            if isinstance(hier_values, list):\n",
    "                for value in hier_values:\n",
    "                    for area, entity_list in entity_hierarchy:\n",
    "                        for entity in entity_list:\n",
    "                            if client_manuf:\n",
    "                                for manuf in client_manuf:\n",
    "                                    key = f\"{value} | {manuf} | {entity}\"\n",
    "                                    ordered_keys.append(key)\n",
    "                                    future = executor.submit(\n",
    "                                        execute_dax_query, entity, value,hierby, area, '', manuf\n",
    "                                    )\n",
    "                                    futures[future] = key\n",
    "\n",
    "                            if client_brands:\n",
    "                                for client in client_brands:\n",
    "                                    key = f\"{value} | {client} | {entity}\"\n",
    "                                    ordered_keys.append(key)\n",
    "                                    future = executor.submit(\n",
    "                                        execute_dax_query, entity, value,hierby, area, client, ''\n",
    "                                    )\n",
    "                                    futures[future] = key\n",
    "    \n",
    "\n",
    "\n",
    "        temp_results = {}\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            temp_results.update(result)\n",
    "\n",
    "        for key in ordered_keys:\n",
    "            if key in temp_results:\n",
    "                dfs_results[key] = temp_results[key]\n",
    "        if client_brands:\n",
    "            filename = f\"value_uplift\"\n",
    "        else:\n",
    "            filename = f\"value_uplift_manuf\"\n",
    "\n",
    "        output_file = f\"{path}\\\\{filename}\"\n",
    "        with open(output_file, \"wb\") as f:\n",
    "            pd.to_pickle(dfs_results, f)\n",
    "        \n",
    "        print(f\"All DataFrames for saved to {output_file}.\")\n",
    "\n",
    "if client_brands:\n",
    "    process_dax_queries(entity_hierarchy, hierarchy_levels, client_brands=client_brands)\n",
    "if client_manuf:\n",
    "    process_dax_queries(entity_hierarchy, hierarchy_levels, client_manuf=client_manuf)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f22135",
   "metadata": {},
   "source": [
    "# Seasonality index dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61bf3d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Sam's Corp.\n",
      "All DataFrames for Sector saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Promotion\\Promotion Datasets Test\\Sector_MonthYear.pkl.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Sam's Corp.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Sam's Corp.\n",
      "All DataFrames for Segment saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Promotion\\Promotion Datasets Test\\Segment_MonthYear.pkl.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Sam's Corp.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Sam's Corp.\n",
      "Query executed successfully for Sam's Corp.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for SubSegment saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Promotion\\Promotion Datasets Test\\SubSegment_MonthYear.pkl.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Sam's Corp.\n",
      "Query executed successfully for Sam's Corp.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for Walmart.\n",
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Sam's Corp.\n",
      "Query executed successfully for Walmart.\n",
      "All DataFrames for SubCategory saved to c:\\Users\\Ali Salem\\Desktop\\App_Update\\Promotion\\Promotion Datasets Test\\SubCategory_MonthYear.pkl.\n"
     ]
    }
   ],
   "source": [
    "def execute_dax_query(entity_name,entity_type, area, hierby):\n",
    "    outputdic = {}\n",
    "    key = f\"{entity_type} | {entity_name}\"\n",
    "\n",
    "    columns = [\"Value Sales\"]\n",
    "    column_exprs = \", \".join(f'\"{col}\", COALESCE([{col}], 0)' for col in columns)\n",
    "\n",
    "    # Prepare client brand filter conditionally\n",
    "\n",
    "    # Main query\n",
    "    dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "            CROSSJOIN(\n",
    "                VALUES(Products[{hierby}]),\n",
    "                VALUES('Calendar'[MonthYear])\n",
    "    \n",
    "                ),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "            Products[{direct_parent[hierby]}] = \"{entity_type}\",\n",
    "\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"Category\")\n",
    "        )\n",
    "    \"\"\"\n",
    "    parenttotal_dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                SUMMARIZE(\n",
    "                    Products,\n",
    "                    Products[{hierby}]                   \n",
    "                ),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "            Products[{direct_parent[hierby]}] = \"{entity_type}\",\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"Category\")\n",
    "        )\n",
    "    \"\"\"\n",
    "        # Grand total query\n",
    "    grandtotal_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                VALUES(Products[Category]),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            Products[Category] = \"{categories[0]}\",\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"Category\")\n",
    "            \n",
    "        )\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(dax_query)\n",
    "            columns_result = [desc[0] for desc in cursor.description]\n",
    "            data = cursor.fetchall()\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(parenttotal_dax_query)\n",
    "            maintotal_columns = [desc[0] for desc in cursor.description]\n",
    "            maintotal_data = cursor.fetchall()\n",
    "            \n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(grandtotal_query)\n",
    "            grandtotal_columns = [desc[0] for desc in cursor.description]\n",
    "            grandtotal_data = cursor.fetchall()\n",
    "\n",
    "        df = pd.DataFrame(data, columns=columns_result)\n",
    "        df.columns = df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        df = df.loc[~(df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "        maintotal_df = pd.DataFrame(maintotal_data, columns=maintotal_columns)\n",
    "        maintotal_df.columns = maintotal_df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        maintotal_df = maintotal_df.loc[~(maintotal_df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "        \n",
    "        grand_tot = pd.DataFrame(grandtotal_data, columns=grandtotal_columns)\n",
    "        grand_tot.columns = grand_tot.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        grand_tot = grand_tot.loc[~(grand_tot.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "        \n",
    "        if maintotal_df.shape[1] > 1:\n",
    "            maintotal_df.iloc[:, 0] = maintotal_df.iloc[:, 0].astype(str) + \" Total\"\n",
    "\n",
    "        if maintotal_df.empty:\n",
    "            outputdic[key] = maintotal_df\n",
    "            return outputdic\n",
    "        if not maintotal_df.empty:\n",
    "            df_with_totals = pd.concat([df,maintotal_df], ignore_index=True)\n",
    "    \n",
    "        if not grand_tot.empty:\n",
    "            # Create a dict for the first two columns\n",
    "            grand_tot[df.columns[0]] = 'Grand Total'\n",
    "            grand_tot[df.columns[1]] = np.nan  # or pd.NA\n",
    "\n",
    "            # Ensure all required columns exist, fill missing ones with NaN\n",
    "            for col in df.columns:\n",
    "                if col not in grand_tot.columns:\n",
    "                    grand_tot[col] = np.nan\n",
    "            grand_tot = grand_tot[df.columns]\n",
    "            df = pd.concat([df_with_totals, grand_tot], ignore_index=True)\n",
    "\n",
    "        outputdic[key] = df\n",
    "\n",
    "        print(f\"Query executed successfully for {entity_name}.\")\n",
    "    except adodbapi.DatabaseError as db_error:\n",
    "        print(f\"Database error for {entity_name} in {area}: {db_error}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error for {entity_name} in {area}: {e}\")\n",
    "\n",
    "    return outputdic\n",
    "\n",
    "\n",
    "def process_dax_queries(entity_hierarchy, hierarchy_levels):\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "\n",
    "        hierarchy_dict = dict(hierarchy_levels)\n",
    "        for hierby, _ in hierarchy_levels:\n",
    "            if hierby == \"Category\":\n",
    "                continue\n",
    "            dfs_results = {} \n",
    "            futures = {}\n",
    "            ordered_keys = []\n",
    "            # Get the parent level name (e.g., 'Segment')\n",
    "            parent_level = direct_parent[hierby]\n",
    "\n",
    "            # Get the list of values associated with that parent level\n",
    "            parent_values = hierarchy_dict.get(parent_level, [])\n",
    "            if isinstance(parent_values, list):\n",
    "                for value in parent_values:\n",
    "                    for area, entity_list in entity_hierarchy:\n",
    "                        for entity in entity_list:\n",
    "                            key = f\"{value} | {entity}\"\n",
    "                            ordered_keys.append(key)\n",
    "                            future = executor.submit(\n",
    "                                execute_dax_query, entity, value, area, hierby\n",
    "                            )\n",
    "                            futures[future] = key\n",
    "            \n",
    "            temp_results = {}\n",
    "            for future in as_completed(futures):\n",
    "                result = future.result()\n",
    "                temp_results.update(result)\n",
    "\n",
    "            for key in ordered_keys:\n",
    "                if key in temp_results:\n",
    "                    dfs_results[key] = temp_results[key]\n",
    "\n",
    "            filename = f\"{hierby}_MonthYear.pkl\"\n",
    "\n",
    "                 \n",
    "            output_file = f\"{path}\\\\{filename}\"\n",
    "            with open(output_file, \"wb\") as f:\n",
    "                pd.to_pickle(dfs_results, f)\n",
    "            \n",
    "            print(f\"All DataFrames for {hierby} saved to {output_file}.\")\n",
    "\n",
    "process_dax_queries(entity_hierarchy, hierarchy_levels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9d09a0",
   "metadata": {},
   "source": [
    "# Seasonality index dic Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "292c1346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query executed successfully for NATIONAL.\n",
      "Query executed successfully for Sam's Corp.\n",
      "Query executed successfully for Walmart.\n"
     ]
    }
   ],
   "source": [
    "def execute_dax_query(entity_name, area,manuf):\n",
    "    outputdic = {}\n",
    "    key =f\"{manuf} | {entity_name}\"\n",
    "    columns = ['Value Sales']\n",
    "\n",
    "    column_exprs = \", \".join(f'\"{col}\", COALESCE([{col}], 0)' for col in columns)\n",
    "\n",
    "    # Prepare client brand filter conditionally\n",
    "    manuf_filter=''\n",
    "    if manuf:\n",
    "        manuf_filter = f'''\n",
    "            FILTER(\n",
    "                Products,\n",
    "                Products[Top Companies] = \"{manuf}\"\n",
    "            ),\n",
    "        '''\n",
    "\n",
    "    # Main query\n",
    "    dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "            CROSSJOIN(\n",
    "                VALUES(Products[Category]),\n",
    "                VALUES('Calendar'[MonthYear])\n",
    "    \n",
    "                ),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            {manuf_filter}\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"Category\")\n",
    "        )\n",
    "    \"\"\"\n",
    "    parenttotal_dax_query = f\"\"\"\n",
    "        EVALUATE\n",
    "        CALCULATETABLE(\n",
    "            ADDCOLUMNS(\n",
    "                VALUES(Products[Category]),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            {manuf_filter}\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}]),\n",
    "            FILTER('Scope', 'Scope'[Scope] = \"Category\")\n",
    "        )\n",
    "    \"\"\"\n",
    "        # Grand total query\n",
    "    grandtotal_query = f\"\"\"\n",
    "    EVALUATE\n",
    "    CALCULATETABLE(\n",
    "        ADDCOLUMNS(\n",
    "            SELECTCOLUMNS(\n",
    "                FILTER('Scope', 'Scope'[Scope] = \"Category\"),\n",
    "                \"Scope\", 'Scope'[Scope]\n",
    "            ),\n",
    "                {column_exprs}\n",
    "            ),\n",
    "            {manuf_filter}\n",
    "            TREATAS({{ \"{entity_name}\" }}, Market[{area}])\n",
    "        )\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(dax_query)\n",
    "            columns_result = [desc[0] for desc in cursor.description]\n",
    "            data = cursor.fetchall()\n",
    "            \n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(parenttotal_dax_query)\n",
    "            maintotal_columns = [desc[0] for desc in cursor.description]\n",
    "            maintotal_data = cursor.fetchall()\n",
    "\n",
    "        with adodbapi.connect(conn_str) as conn, conn.cursor() as cursor:\n",
    "            cursor.execute(grandtotal_query)\n",
    "            grandtotal_columns = [desc[0] for desc in cursor.description]\n",
    "            grandtotal_data = cursor.fetchall()\n",
    "        \n",
    "        df = pd.DataFrame(data, columns=columns_result)\n",
    "        df.columns = df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        df = df.loc[~(df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "            \n",
    "        maintotal_df = pd.DataFrame(maintotal_data, columns=maintotal_columns)\n",
    "        maintotal_df.columns = maintotal_df.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        maintotal_df = maintotal_df.loc[~(maintotal_df.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "        \n",
    "        grand_tot = pd.DataFrame(grandtotal_data, columns=grandtotal_columns)\n",
    "        grand_tot.columns = grand_tot.columns.str.replace(r'.*\\[|\\]', '', regex=True)\n",
    "        grand_tot = grand_tot.loc[~(grand_tot.select_dtypes(include='number') == 0).all(axis=1)]\n",
    "        \n",
    "        if maintotal_df.shape[1] > 1:\n",
    "            maintotal_df.iloc[:, 0] = maintotal_df.iloc[:, 0].astype(str) + \" Total\"\n",
    "\n",
    "        if not maintotal_df.empty:\n",
    "            df_with_totals = pd.concat([df,maintotal_df], ignore_index=True)\n",
    "    \n",
    "        if not grand_tot.empty:\n",
    "            # Create a dict for the first two columns\n",
    "            grand_tot[df.columns[0]] = 'Grand Total'\n",
    "            grand_tot[df.columns[1]] = np.nan  # or pd.NA\n",
    "\n",
    "            # Ensure all required columns exist, fill missing ones with NaN\n",
    "            for col in df.columns:\n",
    "                if col not in grand_tot.columns:\n",
    "                    grand_tot[col] = np.nan\n",
    "            grand_tot = grand_tot[df.columns]\n",
    "            df = pd.concat([df_with_totals, grand_tot], ignore_index=True)\n",
    "\n",
    "        outputdic[key] = df\n",
    "\n",
    "\n",
    "        print(f\"Query executed successfully for {entity_name}.\")\n",
    "    except adodbapi.DatabaseError as db_error:\n",
    "        print(f\"Database error for {entity_name} in {area}: {db_error}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error for {entity_name} in {area}: {e}\")\n",
    "\n",
    "    return outputdic\n",
    "\n",
    "\n",
    "def process_dax_queries(entity_hierarchy,client_manuf=None):\n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        dfs_results = {} \n",
    "        futures = {}\n",
    "        ordered_keys = []\n",
    "        \n",
    "        if client_manuf:\n",
    "            for area, entity_list in entity_hierarchy:\n",
    "                for entity in entity_list:\n",
    "                    for manuf in client_manuf:\n",
    "                        key = f\"{manuf} | {entity}\" \n",
    "                        ordered_keys.append(key)\n",
    "                        future = executor.submit(\n",
    "                            execute_dax_query, entity, area,manuf,\n",
    "                        )\n",
    "                        futures[future] = key               \n",
    "\n",
    "\n",
    "            temp_results = {}\n",
    "            for future in as_completed(futures):\n",
    "                result = future.result()\n",
    "                temp_results.update(result)\n",
    "\n",
    "            for key in ordered_keys:\n",
    "                if key in temp_results:\n",
    "                    dfs_results[key] = temp_results[key]\n",
    "\n",
    "            filename = f\"Category_MonthYear\"\n",
    "\n",
    "            output_file = f\"{path}\\\\{filename}\"\n",
    "            with open(output_file, \"wb\") as f:\n",
    "                pd.to_pickle(dfs_results, f)\n",
    "            \n",
    "\n",
    "\n",
    "if client_manuf:\n",
    "    process_dax_queries(entity_hierarchy, client_manuf=client_manuf)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ecca619a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script started at: Mon Sep  1 14:45:43 2025\n",
      "Script ended at: Mon Sep  1 14:53:51 2025\n",
      "Elapsed time: 488.37 seconds\n"
     ]
    }
   ],
   "source": [
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Script started at: {time.ctime(start_time)}\")\n",
    "print(f\"Script ended at: {time.ctime(end_time)}\")\n",
    "print(f\"Elapsed time: {elapsed_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31399ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
